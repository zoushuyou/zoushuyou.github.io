[{"author":null,"categories":["Code"],"content":" 本文介绍下从hexo迁移到hugo的过程\n背景：\n从深圳回武汉后，都没怎么写博客了，一方面是公司开发主要都是业务方面涉及的多，技术方面没那么高的要求。\n最近想着重新刷刷面试题，背背八股文，看看有没有更好的机会，就开始重新鼓捣起博客了。\n其实之前在深圳的时候就知道hugo这个框架搭建博客，一直不太习惯用npm，所以这几天干脆把博客从hexo迁移到hugo了。\n一开始是想沿用之前的hux主题，网上搜了几天都没找到，后面搭建的时候挑了NEXT主题，搭建的时候了解github action，无意间又找到了puppet主题（hux主题的hugo版），想着还是换个主题换个心情，还是用NEXT主题了。\n迁移：\n大多数步骤网上都能搜到，这里不做详细介绍了，简略记录下过程。\n1.需要安装hugo、配置hugo环境变量。\n2.安装NEXT主题，调整配置。\n3.使用github action自动部署。\n后续：\n找个好用的图床工具 博客细节调整 找些golang项目学学 ","date":1717977600,"description":"","dir":"post\\Blog\\","excerpt_html":null,"excerpt_text":null,"expirydate":-62135596800,"fuzzywordcount":500,"html":null,"keywords":null,"kind":"page","lang":"en","lastmod":1717977600,"objectID":"fc4797a7a5f13c9b187b72f50f116c92","permalink":"http://localhost:1313/post/blog/%E4%BB%8Ehexo%E8%BF%81%E7%A7%BB%E5%88%B0hugo/","publishdate":"2024-06-10T00:00:00Z","readingtime":1,"relpermalink":"/post/blog/%E4%BB%8Ehexo%E8%BF%81%E7%A7%BB%E5%88%B0hugo/","section":"post","summary":"本文介绍下从hexo迁移到hugo的过程 背景： 从深圳回武汉后，都没怎么写博客了，一方面是公司开发主要都是业务方面涉及的多，技术方面没那么高的","tags":["Blog"],"title":"从hexo迁移到hugo","type":"post","url":"/post/blog/%E4%BB%8Ehexo%E8%BF%81%E7%A7%BB%E5%88%B0hugo/","weight":0,"wordcount":413},{"author":null,"categories":["Life"],"content":"2021过完啦，1月中旬就会回去了，2022年2月中旬开始在武汉找工作。\n刚入社会的第一年，已经受到了社会的毒打了，呜呜呜呜。\n回看2021，新冠扰乱社会的一年，而我也戴了一年的口罩，打了三针疫苗。\n首先是工作，勤勤恳恳，任劳任怨地工作了一年，还是值得夸赞的一年。虽然还是底层打工仔，但是要相信自己能有so much money 的呀。\n其次就是生活，也算是去了其他地方，广州、北京两个大都市，不得不感叹真是乡巴佬进城呀，有钱人是真的多。\n奶奶在去年走了。\n一年又一年，心里的目标呢？期待的生活呢？\n好像始终是为了美好的生活而四处奔波，明明别人触手可及的，却是自己羡慕不已的。\n今年也是一样呀，回武汉找份好工作，努力赚钱。\n虽然生活很苦，但是自己心里要觉得甜甜的呀，做一个有能量的人。\n后记\n回武汉3天了，前天把电脑装好了，这两天找女朋友玩了，也倒腾了下新电脑。家里的宽带快到期了，本来想换电信的宽带的，今天上门师傅来安装，弄了半天，后面发现我家的光纤埋线坏了，装不了电信的了，等退钱吧，我订个联通的宽带看看。\n武汉的冬天好冷，天总是灰蒙蒙的，全是雾霾呀。\n明天开始要准备简历，刷刷面试题之类的了。\n希望2022生活更美好，自己也变得更好。\n","date":1641427200,"description":"","dir":"post\\Shuyou\\","excerpt_html":null,"excerpt_text":null,"expirydate":-62135596800,"fuzzywordcount":500,"html":null,"keywords":null,"kind":"page","lang":"en","lastmod":1641427200,"objectID":"33aa6d51a28ff902235ea3d2be2371c7","permalink":"http://localhost:1313/post/shuyou/2021/","publishdate":"2022-01-06T00:00:00Z","readingtime":1,"relpermalink":"/post/shuyou/2021/","section":"post","summary":"2021过完啦，1月中旬就会回去了，2022年2月中旬开始在武汉找工作。 刚入社会的第一年，已经受到了社会的毒打了，呜呜呜呜。 回看2021，新","tags":["记录"],"title":"2021","type":"post","url":"/post/shuyou/2021/","weight":0,"wordcount":496},{"author":null,"categories":["Code"],"content":" 本文介绍Spring的循环依赖问题，以及Spring是如何解决的。\n循环依赖是指多个对象实例之间存在直接或间接的依赖关系，如A对象中引用了B对象，B对象中引用了A对象，有时在项目中遇到这种情况会出现StackOverflow异常，可以通过属性注入的方式解决这个问题。\nSpring循环依赖是指容器中的bean对象存在的循环依赖问题，Spring通过使用三级缓存解决的该问题。\n第一层缓存（singletonObjects）：单例对象缓存池，已经实例化并且属性赋值，这里的对象是成熟对象； 第二层缓存（earlySingletonObjects）：单例对象缓存池，已经实例化还未属性赋值，这里的对象是半成品对象； 第三层缓存（singletonFactories）: 单例工厂的缓存 AbstractBeanFactory类中：\n/** Cache of singleton objects: bean name --\u0026gt; bean instance */ private final Map\u0026lt;String, Object\u0026gt; singletonObjects = new ConcurrentHashMap\u0026lt;String, Object\u0026gt;(256); /** Cache of early singleton objects: bean name --\u0026gt; bean instance */ private final Map\u0026lt;String, Object\u0026gt; earlySingletonObjects = new HashMap\u0026lt;String, Object\u0026gt;(16); /** Cache of singleton factories: bean name --\u0026gt; ObjectFactory */ private final Map\u0026lt;String, ObjectFactory\u0026lt;?\u0026gt;\u0026gt; singletonFactories = new HashMap\u0026lt;String, ObjectFactory\u0026lt;?\u0026gt;\u0026gt;(16); 获取单例bean\nprotected Object getSingleton(String beanName, boolean allowEarlyReference) { //从一级缓存中获取 Object singletonObject = this.singletonObjects.get(beanName); if (singletonObject == null \u0026amp;\u0026amp; this.isSingletonCurrentlyInCreation(beanName)) { synchronized(this.singletonObjects) { //一级缓存获取不到，则从二级缓存中获取 singletonObject = this.earlySingletonObjects.get(beanName); if (singletonObject == null \u0026amp;\u0026amp; allowEarlyReference) { //从三级缓存中获取 ObjectFactory ObjectFactory\u0026lt;?\u0026gt; singletonFactory = (ObjectFactory)this.singletonFactories.get(beanName); if (singletonFactory != null) { singletonObject = singletonFactory.getObject(); //将beanName存入二级缓存 this.earlySingletonObjects.put(beanName, singletonObject); // 把当前这个 beanName 从三级缓存中删除 this.singletonFactories.remove(beanName); } } } } return singletonObject != NULL_OBJECT ? singletonObject : null; } bean的获取过程：先从一级缓存获取，获取不到再从二级缓存获取，获取不到再从三级缓存获取并提高到二级缓存中\n检测循环依赖的过程：\nA创建过程中需要B，于是A将自己放入三级缓存，再实例化B B实例化时发现需要A，于是 B 先查一级缓存，没有，再查二级缓存，还是没有，再查三级缓存，找到了！ 然后把三级缓存里面的这个 A 放到二级缓存里面，并删除三级缓存里面的 A B 顺利初始化完毕，将自己放到一级缓存里面（此时 B 里面的 A 依然是创建中状态） 然后回来接着创建 A，此时 B 已经创建结束，直接从一级缓存里面拿到 B ，然后完成创建，并将自己放到一级缓存里面 相关问题 Spring为什么不能解决构造器的循环依赖？\n构造器注入形成的循环依赖： 也就是beanB需要在beanA的构造函数中完成初始化，beanA也需要在beanB的构造函数中完成舒适化，这种情况的结果就是两个bean都不能完成初始化，循环依赖难以解决。\nSpring解决循环依赖主要依赖三级缓存，但在调用构造方法之前也就是实例化之前还未将其放入三级缓存，因此后续依赖调用构造方法的时候并不能从三级缓存中获取到依赖的Bean。\nSpring为什么不能解决prototype作用域循环依赖？\n这种循环依赖同样无法解决，因为spring不会缓存‘prototype’作用域的bean，而spring中循环依赖的解决正是通过缓存来实现的。\nSpring为什么不能解决多例的循环依赖？\n多实例Bean是每次调用一次getBean都会执行一次构造方法并且未属性赋值，根本没有三级缓存，因此无法解决循环依赖。\n参考\nSpring 循环依赖问题（三级缓存） Spring进阶- Spring IOC实现原理详解之Bean实例化(生命周期,循环依赖等) ","date":1640649600,"description":"","dir":"post\\Spring\\","excerpt_html":null,"excerpt_text":null,"expirydate":-62135596800,"fuzzywordcount":1300,"html":null,"keywords":null,"kind":"page","lang":"en","lastmod":1640649600,"objectID":"f4c3ae75ac44b1ffe8d4cb9a1e3f9aba","permalink":"http://localhost:1313/post/spring/spring%E5%BE%AA%E7%8E%AF%E4%BE%9D%E8%B5%96/","publishdate":"2021-12-28T00:00:00Z","readingtime":3,"relpermalink":"/post/spring/spring%E5%BE%AA%E7%8E%AF%E4%BE%9D%E8%B5%96/","section":"post","summary":"本文介绍Spring的循环依赖问题，以及Spring是如何解决的。 循环依赖是指多个对象实例之间存在直接或间接的依赖关系，如A对象中引用了B对","tags":["Spring"],"title":"Spring 循环依赖","type":"post","url":"/post/spring/spring%E5%BE%AA%E7%8E%AF%E4%BE%9D%E8%B5%96/","weight":0,"wordcount":1261},{"author":null,"categories":["Code"],"content":" 本文介绍在win10下使用docker和wsl2\n之前一直是在虚拟机或者远程服务器上使用的docker，后面发现了win10下有wsl2之后就习惯了在win10的wsl2上使用docker\n使用wsl2也很方便\nwsl2 安装 WSL\n可从win10自带的商店下载安装发行版linux，我使用的是ubuntu\ndocker 在wsl2中允许docker，需要下载docker官方提供的 Docker Desktop\nwindows terminal 在win10商店下载 windows terminal，一款美化的命令行工具。\ndocker-compose 使用docker-compose管理docker相关的内容会更加的方便。\nutools 这里再推荐一款工具:Utools，它是一款win10插件平台，提供了很多插件。\n","date":1639008000,"description":"","dir":"post\\Docker\\","excerpt_html":null,"excerpt_text":null,"expirydate":-62135596800,"fuzzywordcount":300,"html":null,"keywords":null,"kind":"page","lang":"en","lastmod":1639008000,"objectID":"9fe8adb78ef2ae3efe12a04b7913bb37","permalink":"http://localhost:1313/post/docker/win10%E4%B8%8B%E4%BD%BF%E7%94%A8docker/","publishdate":"2021-12-09T00:00:00Z","readingtime":1,"relpermalink":"/post/docker/win10%E4%B8%8B%E4%BD%BF%E7%94%A8docker/","section":"post","summary":"本文介绍在win10下使用docker和wsl2 之前一直是在虚拟机或者远程服务器上使用的docker，后面发现了win10下有wsl2之后就","tags":["Docker"],"title":"Win10下使用docker","type":"post","url":"/post/docker/win10%E4%B8%8B%E4%BD%BF%E7%94%A8docker/","weight":0,"wordcount":286},{"author":null,"categories":["Code"],"content":" 本文简单分析Mybatis的执行过程\n重要类 SqlSession Configuration Excutor MappedStatement StatementHandler 从注入开始 spring boot使用mybatis，需要导入mybatis-spring-boot-starter包，该包会导入mybatis-spring-boot-autoconfigure包。\nMybatisAutoConfiguration类会自动注入 SqlSessionFactory ，注入MapperScannerConfigurer且会扫描dao包，注入其中标注@Mapper注解的接口。 如果是使用XML配置的，则会在 SqlSessionFactory（其实是SqlSessionFactoryBean类） 注入时，扫描xml配置，并添加到 Configuration 中。 会注入 SqlSessionTemplate ，该类包含一个SqlSession代理类。\nMapperRegistry、MapperProxyFactory、MapperProxy Configuration中包含 MapperRegistry 属性。\nmapperRegistry有getMapper和addMapper方法，有一个map，用来保存mapper类和MapperProxyFactory。当需要的mapper在map中找不到时，会通过MapperProxyFactory生成一个代理mapper类。\nMapperProxyFactory是MapperProxy的工程类，可以生成MapperProxy代理类\nMapperProxy是Mapper代理类，相当于mapper接口的注入到容器的bean类型，使用mapper接口的方法时，会调到MapperProxy的invoke方法上，这里使用的是JDK代理。\nSqlSessionTemplate、SqlSession MapperProxy的invoke方法会执行MapperMethod的invoke方法，之后根据sql语句类型继续向下执行，接着会调用SqlSessionTemplate上的方法，\nSqlSessionTemplate会调用代理的SqlSession类上的方法，这里是DefaultSqlSession。\nExcutor MappedStatement包含很多属性，sql语句、返回结果等 DefaultSqlSession会调用Excutor继续执行相应的方法。\nStatementHandler Excutor执行时，会调用相应的StatementHandler继续执行。\n小结 大致的流程就是上述过程，还是挺复杂的。\n参考\nSpringBoot中，Mybatis的执行流程源码解析 ","date":1637020800,"description":"","dir":"post\\Mybatis\\","excerpt_html":null,"excerpt_text":null,"expirydate":-62135596800,"fuzzywordcount":1000,"html":null,"keywords":null,"kind":"page","lang":"en","lastmod":1637020800,"objectID":"c97c2522ffbfd914685a67f38337aaba","permalink":"http://localhost:1313/post/mybatis/mybatis%E6%89%A7%E8%A1%8C%E8%BF%87%E7%A8%8B/","publishdate":"2021-11-16T00:00:00Z","readingtime":2,"relpermalink":"/post/mybatis/mybatis%E6%89%A7%E8%A1%8C%E8%BF%87%E7%A8%8B/","section":"post","summary":"本文简单分析Mybatis的执行过程 重要类 SqlSession Configuration Excutor MappedStatement StatementHandler 从注入开始 spring boot使用mybatis，需要导入mybatis-spring-boot-","tags":["Mybatis"],"title":"Mybatis执行过程","type":"post","url":"/post/mybatis/mybatis%E6%89%A7%E8%A1%8C%E8%BF%87%E7%A8%8B/","weight":0,"wordcount":996},{"author":null,"categories":["Life"],"content":"11.3\n和她吵架了，这次比较严重，她想分手，可能这次吵架只是一个导火索而已。\n她觉得我对她不够好，说我不是有意识的，只是我性格和脾气的一部分。想想确实觉得自己对她不是很好。\n唉，昨晚失眠很久，发信息挽回。\n今天发短信挽回，她还是很生气，我知道她只是希望有时候我能哄她，包容她，照顾她。但是有时候我自己的情绪都很难照顾，所以顾及不上她。\n11.4\n道歉了很久，哄了她很久，她也不是那么生气了。\n我觉得我还是很喜欢她的，只是偶尔两个人有争论，有矛盾。\n11.7\n和好了，呜呜呜呜。\n这几天感觉开心了很多。\n","date":1635984000,"description":"","dir":"post\\Shuyou\\","excerpt_html":null,"excerpt_text":null,"expirydate":-62135596800,"fuzzywordcount":300,"html":null,"keywords":null,"kind":"page","lang":"en","lastmod":1635984000,"objectID":"75919c6b21c31a57f221cccac7ae036a","permalink":"http://localhost:1313/post/shuyou/%E5%90%B5%E6%9E%B6%E5%90%8E%E8%AE%B0/","publishdate":"2021-11-04T00:00:00Z","readingtime":1,"relpermalink":"/post/shuyou/%E5%90%B5%E6%9E%B6%E5%90%8E%E8%AE%B0/","section":"post","summary":"11.3 和她吵架了，这次比较严重，她想分手，可能这次吵架只是一个导火索而已。 她觉得我对她不够好，说我不是有意识的，只是我性格和脾气的一部分。想想确","tags":["记录"],"title":"吵架后记","type":"post","url":"/post/shuyou/%E5%90%B5%E6%9E%B6%E5%90%8E%E8%AE%B0/","weight":0,"wordcount":232},{"author":null,"categories":["Code"],"content":" 本文介绍怎么把SSM项目改造为Spring Boot项目\n0.改造步骤 原ssm项目打成jar包 将相关配置文件放到resources目录下 将前端界面文件放到web目录下 servlet 、 listener 和 Filter 要注册到容器中 1.Servlet SSM里使用的Servlet，要注册到容器中\n使用ServletRegistrationBean类或者使用注解@WebServlet\n@Bean public ServletRegistrationBean myServlet() { return new ServletRegistrationBean(new MyServlet, new String[]{\u0026#34;/kjdp_cache\u0026#34;}); } 2.Filter SSM里使用的Filter，要注册到容器中\n使用FilterRegistrationBean类或者使用注解@WebFilter\n@Bean public FilterRegistrationBean encodingFilter() { FilterRegistrationBean registration = new FilterRegistrationBean(); registration.setName(\u0026#34;encodingFilter\u0026#34;); registration.setOrder(1); registration.addUrlPatterns(new String[]{\u0026#34;/*\u0026#34;}); registration.setFilter(new CharacterEncodingFilter()); registration.addInitParameter(\u0026#34;encoding\u0026#34;, \u0026#34;UTF-8\u0026#34;); return registration; } 3.Listener SSM里使用的Filter，要注册到容器中\n使用ListenerRegistrationBean类或者使用注解@WebListener\n4.自定义启动类注解 可以通过@SpringBootApplication再注解一个自定义启动类注解\n@Target({ElementType.TYPE}) @Retention(RetentionPolicy.RUNTIME) @Inherited @SpringBootApplication @ImportResource public @interface MyBootApplication { @AliasFor( annotation = ImportResource.class ) String[] locations() default {\u0026#34;classpath:conf/spring-application.xml\u0026#34;}; @AliasFor( annotation = SpringBootApplication.class ) Class\u0026lt;?\u0026gt;[] exclude() default {DataSourceAutoConfiguration.class}; @AliasFor( annotation = SpringBootApplication.class ) String[] excludeName() default {}; @AliasFor( annotation = SpringBootApplication.class ) String[] scanBasePackages() default {\u0026#34;com.my.boot\u0026#34;}; @AliasFor( annotation = SpringBootApplication.class ) Class\u0026lt;?\u0026gt;[] scanBasePackageClasses() default {}; @AliasFor( annotation = SpringBootApplication.class ) boolean proxyBeanMethods() default true; } 使用上面三个注解的需要在启动类上加一个@ServletComponentScan注解来扫描。\n","date":1635379200,"description":"","dir":"post\\Spring Boot\\","excerpt_html":null,"excerpt_text":null,"expirydate":-62135596800,"fuzzywordcount":500,"html":null,"keywords":null,"kind":"page","lang":"en","lastmod":1635379200,"objectID":"2609d49c1502b52296c8371f5c1dac6e","permalink":"http://localhost:1313/post/spring-boot/ssm%E9%A1%B9%E7%9B%AE%E6%94%B9%E9%80%A0%E4%B8%BAspring-boot%E9%A1%B9%E7%9B%AE/","publishdate":"2021-10-28T00:00:00Z","readingtime":1,"relpermalink":"/post/spring-boot/ssm%E9%A1%B9%E7%9B%AE%E6%94%B9%E9%80%A0%E4%B8%BAspring-boot%E9%A1%B9%E7%9B%AE/","section":"post","summary":"本文介绍怎么把SSM项目改造为Spring Boot项目 0.改造步骤 原ssm项目打成jar包 将相关配置文件放到resources目录下 将前端界","tags":["Spring Boot"],"title":"SSM项目改造为Spring Boot项目","type":"post","url":"/post/spring-boot/ssm%E9%A1%B9%E7%9B%AE%E6%94%B9%E9%80%A0%E4%B8%BAspring-boot%E9%A1%B9%E7%9B%AE/","weight":0,"wordcount":481},{"author":null,"categories":["Code"],"content":" 本文介绍Spring Cloud Gateway网关服务相关知识\n简介 Gateway 是Spring提供的API网关服务框架，具有强大的智能路由和过滤功能。\n依赖 \u0026lt;properties\u0026gt; \u0026lt;java.version\u0026gt;1.8\u0026lt;/java.version\u0026gt; \u0026lt;!-- 核心依赖 --\u0026gt; \u0026lt;spring-cloud.version\u0026gt;2020.0.2\u0026lt;/spring-cloud.version\u0026gt; \u0026lt;spring-cloud-alibaba.version\u0026gt;2021.1\u0026lt;/spring-cloud-alibaba.version\u0026gt; \u0026lt;nacos.version\u0026gt;2.0.1\u0026lt;/nacos.version\u0026gt; \u0026lt;/properties\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-starter-bootstrap\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.alibaba.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-starter-alibaba-nacos-discovery\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-starter-gateway\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-starter-loadbalancer\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;dependencyManagement\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;!-- 核心依赖 --\u0026gt; \u0026lt;!--spring cloud--\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-dependencies\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${spring-cloud.version}\u0026lt;/version\u0026gt; \u0026lt;type\u0026gt;pom\u0026lt;/type\u0026gt; \u0026lt;scope\u0026gt;import\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!--spring cloud alibaba--\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.alibaba.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-alibaba-dependencies\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${spring-cloud-alibaba.version}\u0026lt;/version\u0026gt; \u0026lt;type\u0026gt;pom\u0026lt;/type\u0026gt; \u0026lt;scope\u0026gt;import\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-starter-gateway\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.0.2\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.alibaba.nacos\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;nacos-client\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${nacos.version}\u0026lt;/version\u0026gt; \u0026lt;exclusions\u0026gt; \u0026lt;exclusion\u0026gt; \u0026lt;groupId\u0026gt;org.yaml\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;snakeyaml\u0026lt;/artifactId\u0026gt; \u0026lt;/exclusion\u0026gt; \u0026lt;/exclusions\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;/dependencyManagement\u0026gt; 这里使用 nacos 做服务注册中心，还须引入 loadbalancer 和 gateway 依赖包。\n配置 server: port: 8446 spring: application: name: gateway cloud: nacos: discovery: server-addr: http://10.60.80.115:8848 gateway: discovery: locator: enabled: true #开启从注册中心动态创建路由的功能 routes: - id: feign uri: lb://feign #lb是指从注册中心获取id为feign的路由地址 predicates: - Path=/** - Method=GET logging: level: com.zsy.gateway: debug 使用 参考代码\n一些配置功能 Route Predicate Spring Cloud Gateway包括许多内置的Route Predicate工厂。 所有这些Predicate都与HTTP请求的不同属性匹配。 多个Route Predicate工厂可以进行组合。可以参考官网，这里只列举一些，比如：\nAfter Route Predicate 在指定时间之后的请求会匹配该路由 Before Route Predicate 在指定时间之前的请求会匹配该路由 Route Filter 路由过滤器可用于修改进入的HTTP请求和返回的HTTP响应，路由过滤器只能指定路由进行使用。Spring Cloud Gateway 内置了多种路由过滤器，他们都由GatewayFilter的工厂类来产生。可以参考官网，这里只列举一些，比如：\nAddRequestHeader 给请求头添加参数的过滤器 AddRequestParameter 给请求添加参数的过滤器 How It Works ","date":1635292800,"description":"","dir":"post\\Spring Cloud\\","excerpt_html":null,"excerpt_text":null,"expirydate":-62135596800,"fuzzywordcount":600,"html":null,"keywords":null,"kind":"page","lang":"en","lastmod":1635292800,"objectID":"96cf59c7f004ac0cebc7d74bf47ff47f","permalink":"http://localhost:1313/post/spring-cloud/spring-cloud-gateway%E7%BD%91%E5%85%B3%E6%9C%8D%E5%8A%A1/","publishdate":"2021-10-27T00:00:00Z","readingtime":2,"relpermalink":"/post/spring-cloud/spring-cloud-gateway%E7%BD%91%E5%85%B3%E6%9C%8D%E5%8A%A1/","section":"post","summary":"本文介绍Spring Cloud Gateway网关服务相关知识 简介 Gateway 是Spring提供的API网关服务框架，具有强大的智能路由和过滤功能。 依赖 \u0026lt;properties\u0026gt; \u0026lt;java.version\u0026gt;1.8\u0026lt;/java.version\u0026gt; \u0026lt;!-- 核","tags":["Spring Cloud"],"title":"Spring Cloud Gateway网关服务","type":"post","url":"/post/spring-cloud/spring-cloud-gateway%E7%BD%91%E5%85%B3%E6%9C%8D%E5%8A%A1/","weight":0,"wordcount":536},{"author":null,"categories":["Code"],"content":"最近试着在wsl2中使用docker跑公司的web项目，但是tomcat容器会报错\n报错 java.lang.illegalargumentexception the main resource set specified [...] is not valid in Tomcat 解决方案 将 %CATALINA_HOME%/conf/server.xml 中的\n\u0026lt;Context docBase=\u0026#34;\u0026#34; path=\u0026#34;/web\u0026#34; reloadable=\u0026#34;false\u0026#34; /\u0026gt; 删除掉\n并在 %CATALINA_HOME%/conf/Catalina/localhost 中创建 ROOT.xml 文件 ，写入以下内容\n\u0026lt;Context docBase=\u0026#34;\u0026lt;yourApp\u0026gt;\u0026#34; path=\u0026#34;/web\u0026#34; reloadable=\u0026#34;false\u0026#34; /\u0026gt; 我本机是成功解决了报上述错的问题。\nJDK版本问题 java.lang.ClassNotFoundException: javax.xml.bind.JAXBException 使用的镜像是 tomcat8.5 ，但是它是JDK11，会报上述错误\n原因：JAXB API是java EE 的API，因此在java SE 9.0 中不再包含这个 Jar 包。 java 9 中引入了模块的概念，默认情况下，Java SE中将不再包含java EE 的Jar包 而在 java 6/7 / 8 时关于这个API 都是捆绑在一起的\n解决方案 降低 JDK 版本\n参考： How to solve common problems when using Tomcat 真正解决方案：java.lang.ClassNotFoundException: javax.xml.bind.JAXBException\n","date":1635120000,"description":"","dir":"post\\Tomcat\\","excerpt_html":null,"excerpt_text":null,"expirydate":-62135596800,"fuzzywordcount":400,"html":null,"keywords":null,"kind":"page","lang":"en","lastmod":1635120000,"objectID":"5d9893b1ce7516bd05e02327fdc382c9","permalink":"http://localhost:1313/post/tomcat/tomcat%E6%8A%A5%E9%94%99%E5%8F%8A%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/","publishdate":"2021-10-25T00:00:00Z","readingtime":1,"relpermalink":"/post/tomcat/tomcat%E6%8A%A5%E9%94%99%E5%8F%8A%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/","section":"post","summary":"最近试着在wsl2中使用docker跑公司的web项目，但是tomcat容器会报错 报错 java.lang.illegalargumentexception the main resource set specified [...] is not valid in Tomcat 解决方案 将 %CATALINA_HOME%/conf/server.xml 中的 \u0026lt;Context docBase=\u0026#34;\u0026#34; path=\u0026#34;/web\u0026#34; reloadable=\u0026#34;false\u0026#34; /\u0026gt; 删除","tags":["Tomcat"],"title":"Tomcat报错及解决方案","type":"post","url":"/post/tomcat/tomcat%E6%8A%A5%E9%94%99%E5%8F%8A%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/","weight":0,"wordcount":305},{"author":null,"categories":["Code"],"content":" 本文简单分析下Disruptor的原理\n简介 Disruptor是一个高性能队列，它是系统内部的内存队列，而不是Kafka这样的分布式队列。\n由于Java内置的队列，会出现加锁和伪共享等影响性能的问题，所以公司项目里使用了Disruptor框架。\nDisruptor采用生产者-消费者模式，并使用环形数组结构，无锁设计，拥有很高的性能。\n环形数组队列 先介绍下 Disruptor 的环形数组 RingBuffer\npublic static final long INITIAL_CURSOR_VALUE = -1L; private final int indexMask; private final Object[] entries; private final int bufferSize; private final Sequencer sequencer; 这是 RingBuffer 类中的变量，比较重要的变量：\nentries 环形数组 生产者生产的类就放在这个数组中 bufferSize 环形数组的大小 sequencer 序列号 用于事件发布者和事件处理者在ringbuffer上相互追逐，标记它们的相对位置 next()\npublic long next() { return this.sequencer.next(); } 返回下一个可用的序列号\nget()\npublic E get(long sequence) { return this.entries[(int)sequence \u0026amp; this.indexMask]; } 返回生产者生产的消息，消息对象里面的内容是空的，需要指定值\npublish()\npublic void publish(long sequence) { this.sequencer.publish(sequence); } 发布消费者可用序列，只有发布了，消费者才能看见。\n序列 Sequence 是 Disruptor 中的序列类，主要用于生成序列号\n在 Sequence 类中，可以看到避免伪共享的相关代码，主要就是 long 类型的，使用了长度为16的 long 类型的数组进行填充，这样可以有效的避免伪共享。\n也采用了CAS相关操作，可以提高性能。\npublic class Sequence { static final long INITIAL_VALUE = -1L; private static final Unsafe UNSAFE = Util.getUnsafe(); private static final long VALUE_OFFSET; private final long[] paddedValue; public Sequence() { this(-1L); } public Sequence(long initialValue) { this.paddedValue = new long[15]; UNSAFE.putOrderedLong(this.paddedValue, VALUE_OFFSET, initialValue); } public long get() { return UNSAFE.getLongVolatile(this.paddedValue, VALUE_OFFSET); } public void set(long value) { UNSAFE.putOrderedLong(this.paddedValue, VALUE_OFFSET, value); } public void setVolatile(long value) { UNSAFE.putLongVolatile(this.paddedValue, VALUE_OFFSET, value); } public boolean compareAndSet(long expectedValue, long newValue) { return UNSAFE.compareAndSwapLong(this.paddedValue, VALUE_OFFSET, expectedValue, newValue); } public long incrementAndGet() { return this.addAndGet(1L); } public long addAndGet(long increment) { long currentValue; long newValue; do { currentValue = this.get(); newValue = currentValue + increment; } while(!this.compareAndSet(currentValue, newValue)); return newValue; } public String toString() { return Long.toString(this.get()); } static { int base = UNSAFE.arrayBaseOffset(long[].class); int scale = UNSAFE.arrayIndexScale(long[].class); VALUE_OFFSET = (long)(base + scale * 7); } } Sequencer 接口，它的很多功能是提供给事件发布者使用的。SequenceBarrier 是给事件处理者使用的。\nnext()\n采用自旋CAS的方式，获取下一个序列。\npublic long next(int n) { if (n \u0026lt; 1) { throw new IllegalArgumentException(\u0026#34;n must be \u0026gt; 0\u0026#34;); } else { long current; long next; do { while(true) { current = this.cursor.get(); next = current + (long)n; long wrapPoint = next - (long)this.bufferSize; long cachedGatingSequence = this.gatingSequenceCache.get(); if (wrapPoint \u0026lt;= cachedGatingSequence \u0026amp;\u0026amp; cachedGatingSequence \u0026lt;= current) { break; } long gatingSequence = Util.getMinimumSequence(this.gatingSequences, current); if (wrapPoint \u0026gt; gatingSequence) { LockSupport.parkNanos(1L); } else { this.gatingSequenceCache.set(gatingSequence); } } } while(!this.cursor.compareAndSet(current, next)); return next; } } 处理事件 几个重要的类：\nWorkProcessor：此类是事件处理类，实现了 Runnable 接口 WorkPool：对处理序列 Sequence 和处理类 WorkProcessor 的封装类 WorkerPoolInfo：实现了 ConsumerInfo 类，对 WorkerPool 和SequenceBarrier 的封装类 ConsumerRepository：消费仓库 处理流程：\n调用 disruptor.handleEventsWithWorkerPool 初始化 Disruptor 时，会向 consumerRepository 消费仓库中添加 WorkerPoolInfo （包装了WorkerPool （将 ringBuffer 对象传入）和 SequenceBarrier） 调用 disruptor.start 时，会从consumerRepository 消费仓库中取出 ConsumerInfo 即 WorkerPoolInfo,并调用其 start 方法 WorkerPoolInfo 的 start 方法 会调用当前 workerPool 的 start 方法，WorkerPool 的 start 方法会使用传进来的线程池去执行 WorkProcessor 在 WorkProcessor 的 run 方法中会调用 WorkHandler 的 onEvent 方法，即自己定义的消费者类 参考：\ndisruptor-3.3.2源码解析汇总 高性能队列——Disruptor Disruptor使用代码案例 ","date":1634688000,"description":"","dir":"post\\Disruptor\\","excerpt_html":null,"excerpt_text":null,"expirydate":-62135596800,"fuzzywordcount":1200,"html":null,"keywords":null,"kind":"page","lang":"en","lastmod":1634688000,"objectID":"b0d56ca608a9a7321fe25557849e5097","permalink":"http://localhost:1313/post/disruptor/disruptor%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/","publishdate":"2021-10-20T00:00:00Z","readingtime":3,"relpermalink":"/post/disruptor/disruptor%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/","section":"post","summary":"本文简单分析下Disruptor的原理 简介 Disruptor是一个高性能队列，它是系统内部的内存队列，而不是Kafka这样的分布式队列。 由于","tags":["Disruptor"],"title":"Disruptor源码分析","type":"post","url":"/post/disruptor/disruptor%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/","weight":0,"wordcount":1149},{"author":null,"categories":["Code"],"content":" 本文介绍 OpenFeign 相关知识\n简介 Spring Cloud OpenFeign 是声明式的服务调用工具，它整合了Ribbon和Hystrix，拥有负载均衡和服务容错功能。\n依赖 \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-web\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-starter-bootstrap\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.alibaba.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-starter-alibaba-nacos-discovery\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-starter-openfeign\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-loadbalancer\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-starter-netflix-hystrix\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; 这里使用 nacos 做服务注册中心，要想拥有负载均衡和服务容错还须引入 loadbalancer 和 hystrix 依赖包。\n配置 server: port: 8445 spring: application: name: feign cloud: nacos: discovery: server-addr: http://10.60.80.115:8848 feign: circuitbreaker: enabled: true //开启服务容错功能 这里由于使用的版本较高 低版本应该是 feign.hystrix.enabled client: config: default: connectTimeout: 5000 readTimeout: 5000 loggerLevel: FULL //请求的日志级别为记录全部 logging: level: com.zsy.feign: debug //还需指定本项目的日志级别 需要在启动类上加上注解 @EnableFeignClients\n使用 创建一个接口，并加上注解 @FeignClient，例如： @FeignClient(value = \u0026ldquo;provider\u0026rdquo;, fallback = FallBackService.class)\n这里的 value 表示要调用的服务的名称， fallback 表示服务降级时调用的类。\n参考代码\n","date":1634515200,"description":"","dir":"post\\Spring Cloud\\","excerpt_html":null,"excerpt_text":null,"expirydate":-62135596800,"fuzzywordcount":300,"html":null,"keywords":null,"kind":"page","lang":"en","lastmod":1634515200,"objectID":"c90a04ad9b87bebecb14fa728d994616","permalink":"http://localhost:1313/post/spring-cloud/openfeign%E6%9C%8D%E5%8A%A1%E8%B0%83%E7%94%A8/","publishdate":"2021-10-18T00:00:00Z","readingtime":1,"relpermalink":"/post/spring-cloud/openfeign%E6%9C%8D%E5%8A%A1%E8%B0%83%E7%94%A8/","section":"post","summary":"本文介绍 OpenFeign 相关知识 简介 Spring Cloud OpenFeign 是声明式的服务调用工具，它整合了Ribbon和Hystrix，拥有负载均衡和服务容错功能。 依赖 \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-web\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-starter-bootstrap\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt;","tags":["Spring Cloud"],"title":"OpenFeign服务调用","type":"post","url":"/post/spring-cloud/openfeign%E6%9C%8D%E5%8A%A1%E8%B0%83%E7%94%A8/","weight":0,"wordcount":292},{"author":null,"categories":["Code"],"content":" 本文介绍 Nacos 相关知识\n简介 Nacos 可以用来实现分布式环境下的配置管理和服务注册与发现。\n通过 Nacos Server 和 spring-cloud-starter-alibaba-nacos-config 实现配置的动态变更。 通过 Nacos Server 和 spring-cloud-starter-alibaba-nacos-discovery 实现服务的注册与发现。 安装 Nacos下载地址：nacos\n下载并解压之后，还需要作一些配置\nconf 目录下的 application.properties 需要配置数据源\n### If use MySQL as datasource: spring.datasource.platform=mysql ### Count of DB: db.num=1 ### Connect URL of DB: db.url.0=jdbc:mysql://127.0.0.1:3306/nacos?characterEncoding=utf8\u0026amp;connectTimeout=1000\u0026amp;socketTimeout=3000\u0026amp;autoReconnect=true\u0026amp;useUnicode=true\u0026amp;useSSL=false\u0026amp;serverTimezone=UTC db.user.0=root db.password.0=123456 然后在MySQL数据库中新建nacos数据库，并导入Nacos解压包conf目录下的nacos-mysql.sql脚本\n更改 Nacos 启动方式为单机模式 配置 pom.xml文件\n需要注意的是 spring-cloud-starter-alibaba-nacos-discovery 的依赖，我看网上其他例子配的是spring-cloud-alibaba-nacos-discovery，这个好像是低版本的依赖\n\u0026lt;properties\u0026gt; \u0026lt;java.version\u0026gt;1.8\u0026lt;/java.version\u0026gt; \u0026lt;!-- 核心依赖 --\u0026gt; \u0026lt;spring-boot.version\u0026gt;2.4.3\u0026lt;/spring-boot.version\u0026gt; \u0026lt;spring-cloud.version\u0026gt;2020.0.2\u0026lt;/spring-cloud.version\u0026gt; \u0026lt;spring-cloud-alibaba.version\u0026gt;2021.1\u0026lt;/spring-cloud-alibaba.version\u0026gt; \u0026lt;/properties\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-web\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-starter-bootstrap\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.alibaba.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-starter-alibaba-nacos-discovery\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;dependencyManagement\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;!-- 核心依赖 --\u0026gt; \u0026lt;!--spring boot--\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-dependencies\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${spring-boot.version}\u0026lt;/version\u0026gt; \u0026lt;type\u0026gt;pom\u0026lt;/type\u0026gt; \u0026lt;scope\u0026gt;import\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!--spring cloud--\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-dependencies\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${spring-cloud.version}\u0026lt;/version\u0026gt; \u0026lt;type\u0026gt;pom\u0026lt;/type\u0026gt; \u0026lt;scope\u0026gt;import\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!--spring cloud alibaba--\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.alibaba.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-alibaba-dependencies\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${spring-cloud-alibaba.version}\u0026lt;/version\u0026gt; \u0026lt;type\u0026gt;pom\u0026lt;/type\u0026gt; \u0026lt;scope\u0026gt;import\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;/dependencyManagement\u0026gt; 启动类上需要加注解 @EnableDiscoveryClient\napplication.yml 中需要指明 nacos 的地址\nserver: port: 8443 spring: application: name: provider cloud: nacos: discovery: server-addr: http://10.60.80.115:8848 入门代码案例 参考：spring-cloud-samples\n","date":1634083200,"description":"","dir":"post\\Spring Cloud\\","excerpt_html":null,"excerpt_text":null,"expirydate":-62135596800,"fuzzywordcount":500,"html":null,"keywords":null,"kind":"page","lang":"en","lastmod":1634083200,"objectID":"6b0671d93ca587e5ea9cd101fe85301e","permalink":"http://localhost:1313/post/spring-cloud/nacos%E6%9C%8D%E5%8A%A1%E6%B3%A8%E5%86%8C%E4%B8%8E%E5%8F%91%E7%8E%B0/","publishdate":"2021-10-13T00:00:00Z","readingtime":1,"relpermalink":"/post/spring-cloud/nacos%E6%9C%8D%E5%8A%A1%E6%B3%A8%E5%86%8C%E4%B8%8E%E5%8F%91%E7%8E%B0/","section":"post","summary":"本文介绍 Nacos 相关知识 简介 Nacos 可以用来实现分布式环境下的配置管理和服务注册与发现。 通过 Nacos Server 和 spring-cloud-starter-alibaba-nacos-config 实现配置的动态变更。 通过 Nacos Server 和 spring-cloud-starter-alibaba-nacos-discovery 实现服务的注册与发现","tags":["Spring Cloud"],"title":"Nacos服务注册与发现","type":"post","url":"/post/spring-cloud/nacos%E6%9C%8D%E5%8A%A1%E6%B3%A8%E5%86%8C%E4%B8%8E%E5%8F%91%E7%8E%B0/","weight":0,"wordcount":414},{"author":null,"categories":["Code"],"content":" 本文介绍Eureka相关知识\n简介 Eureka是由netflix开发的一款服务治理的框架，Sping Cloud对其进行了集成。\nEureka既包括客户端也包括服务端。Eureka客户端是服务提供者，它将自己注册到Eureka服务端，并周期性地发送心跳包来更新它的服务租约，同时也能从服务端查询当前注册的服务信息并把它们缓存到本地并周期性地刷新服务状态；Eureka服务端是一个服务注册中心，提供服务的注册和发现，即当前有哪些服务注册进来可供使用。\n服务注册中心 1.在pom.xml文件中引入 eureka-server\n\u0026lt;!--eureka-server--\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-starter-netflix-eureka-server\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 2.在启动类中加上注解 @EnableEurekaServer 3.在 application.yml 添加以下配置，作为服务注册中心时禁止默认的自我注册：\neureka: instance: hostname: eureka7001.com #eureka服务端实例名称 client: register-with-eureka: false #表示不向注册中心注册自己 fetch-registry: false #false表示自己就是注册中心 service-url: defaultZone: http://eureka7001.com:7001/eureka/ 服务提供者 1.在pom.xml文件中引入 eureka-client\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-starter-netflix-eureka-client\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 2.在启动类中加上注解 @EnableDiscoveryClient 3.在 application.yml 添加以下配置：\neureka: client: register-with-eureka: true #表示向注册中心注册自己 fetch-registry: true #是否从EurekaServer抓取已有的注册信息，默认为true，单节点无所谓,集群必须设置为true才能配合ribbon使用负载均衡 service-url: defaultZone: http://eureka7001.com:7001/eureka/,http://eureka7002.com:7002/eureka/ Eureka-Server集群 Eureka服务端充当了重要的角色，所有Eureka客户端都将自己提供的服务注册到Eureka服务端，然后供所有服务消费者使用。如果单节点的Eureka服务端宕机了，那么所有服务都无法正常的访问，这必将是灾难性的。为了提高Eureka服务端的可用性，我们一般会对其集群部署，即同时部署多个Eureka服务端，并且可以相互间同步服务。\neureka-server1:\nserver: port: 7001 eureka: instance: hostname: eureka7001.com #eureka服务端实例名称 client: register-with-eureka: false #表示不向注册中心注册自己 fetch-registry: false #false表示自己就是组测中心 service-url: defaultZone: http://eureka7002.com:7002/eureka/ eureka-server2:\nserver: port: 7002 eureka: instance: hostname: eureka7001.com #eureka服务端实例名称 client: register-with-eureka: false #表示不向注册中心注册自己 fetch-registry: false #false表示自己就是组测中心 service-url: defaultZone: http://eureka7001.com:7001/eureka/ 通过指定 defaultZone 为其他 server 地址进行集群。\n参考\nSpring Cloud Eureka服务治理 Eureka ","date":1633910400,"description":"","dir":"post\\Spring Cloud\\","excerpt_html":null,"excerpt_text":null,"expirydate":-62135596800,"fuzzywordcount":900,"html":null,"keywords":null,"kind":"page","lang":"en","lastmod":1633910400,"objectID":"a707992954f382491b897f47e5bf403d","permalink":"http://localhost:1313/post/spring-cloud/eureka%E6%9C%8D%E5%8A%A1%E6%B3%A8%E5%86%8C%E4%B8%8E%E5%8F%91%E7%8E%B0/","publishdate":"2021-10-11T00:00:00Z","readingtime":2,"relpermalink":"/post/spring-cloud/eureka%E6%9C%8D%E5%8A%A1%E6%B3%A8%E5%86%8C%E4%B8%8E%E5%8F%91%E7%8E%B0/","section":"post","summary":"本文介绍Eureka相关知识 简介 Eureka是由netflix开发的一款服务治理的框架，Sping Cloud对其进行了集成。 Eureka既包","tags":["Spring Cloud"],"title":"Eureka服务注册与发现","type":"post","url":"/post/spring-cloud/eureka%E6%9C%8D%E5%8A%A1%E6%B3%A8%E5%86%8C%E4%B8%8E%E5%8F%91%E7%8E%B0/","weight":0,"wordcount":812},{"author":null,"categories":["Life"],"content":"黄金假期 十一假期已经过完啦，太快咯，要是天天都放假就好了。\n29号回去的，10号又回深圳了，除去两天车程，相当于有十天假期。\n愉快的日子一去不复返，又是努力奋斗、拼命打工的时候了。\n十月规划 十月底可能又得去北京，要做二期项目，希望12月底能做完回深圳。\n十月希望能熟悉微服务相关知识，重点是spring cloud alibaba的解决方案，公司采用的netflix方案也可以了解一下。\n明年要回武汉，希望可以在武汉找一份好工作。\n路上感想 脑子好像没有以前那么好使了，就像是得了一场大病之后的后遗症，也可能是这几年晚上睡太晚的原因。\n这几年学习和工作老是心不在焉，自己也能感受的到，但是却无力改变，集中不了注意力。\n要整理好自己，慢慢变好。\n如梦复原 工作之前的过去20多年像是虚度一样，自我感觉是没起到太大的作用。\n如果说，读书是人生序章的话，那么我人生的起点就是很低的了。\n乐观的往前，相信有一个好的未来。\n","date":1633910400,"description":"","dir":"post\\Shuyou\\","excerpt_html":null,"excerpt_text":null,"expirydate":-62135596800,"fuzzywordcount":400,"html":null,"keywords":null,"kind":"page","lang":"en","lastmod":1633910400,"objectID":"9b046795ca0cf0ebd963cf92e5404c2d","permalink":"http://localhost:1313/post/shuyou/%E5%A4%8D%E5%8E%9F%E7%9A%84%E5%8D%81%E6%9C%88/","publishdate":"2021-10-11T00:00:00Z","readingtime":1,"relpermalink":"/post/shuyou/%E5%A4%8D%E5%8E%9F%E7%9A%84%E5%8D%81%E6%9C%88/","section":"post","summary":"黄金假期 十一假期已经过完啦，太快咯，要是天天都放假就好了。 29号回去的，10号又回深圳了，除去两天车程，相当于有十天假期。 愉快的日子一去不复","tags":["记录"],"title":"十月杂感","type":"post","url":"/post/shuyou/%E5%A4%8D%E5%8E%9F%E7%9A%84%E5%8D%81%E6%9C%88/","weight":0,"wordcount":387},{"author":null,"categories":["Code"],"content":" 本文介绍微服务相关概念，了解微服务的相关架构和发展。\n微服务架构 传统的软件架构，往往是一个单体应用糅合了各种业务模块。随着业务的发展，单体应用变得庞大之后产生难以维护的问题，微服务架构便出现了。\n微服务是一种架构风格，微服务将一个独立的系统拆分成多个微小的服务，这些小型服务都在各自独立的进程中运行，服务之间通过约定的通讯方式进行通信协作。\n微服务架构中的一些核心概念：\n服务注册与发现 熔断、限流、降级、隔离 服务网关 链路追踪 负载均衡 服务调用 集群与分布式 集群：多台计算机完成同一个工作。 分布式：多台计算机做不同的工作，但是它们之间彼此进行交互以实现一个共同的目标。 分布式集群：一个业务使用集群部署，多个业务之间采用分布式部署，从而形成分布式集群。\n服务注册与发现 对每个微服务模块进行统一的服务注册，方便管理。\n常用的解决方案有：\nSpring Cloud Eureka Nacos Zookeeper 服务调用 服务之间互相调用的解决方案: rpc 还是 http\nDubbo Spring Cloud Ribbon Spring Cloud Feign 配置中心 解决服务太多导致配置文件太多的问题\nNacos Spring Cloud Config API网关 对外提供统一的API接口，方便管理，采用API网关\nSpring Cloud Gateway Spring Cloud Zuul 链路跟踪 当服务之间调用出现错误时，方便排除哪一服务出现问题。\nSpring Cloud Sleuth 服务流控、熔断等 当访问量过大时对流量进行控制\nSpring Cloud Alibaba Sentinel Spring Cloud Hystrix Spring Cloud对微服务架构提供了很多的解决方案，其中一部分是Netflix的解决方案，Spring Cloud对其进行了整合，还有一部分是alibaba提供的解决方案。\n总的来说，Spring Cloud Alibaba的解决方案更加完整，且社区还在进行更新，所以后续可能会入门和学习Spring Cloud Alibaba的解决方案。\n","date":1632700800,"description":"","dir":"post\\Spring Cloud\\","excerpt_html":null,"excerpt_text":null,"expirydate":-62135596800,"fuzzywordcount":700,"html":null,"keywords":null,"kind":"page","lang":"en","lastmod":1632700800,"objectID":"0b65566c2654c6c8c75d74b1bdaf39e5","permalink":"http://localhost:1313/post/spring-cloud/%E5%BE%AE%E6%9C%8D%E5%8A%A1%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/","publishdate":"2021-09-27T00:00:00Z","readingtime":2,"relpermalink":"/post/spring-cloud/%E5%BE%AE%E6%9C%8D%E5%8A%A1%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/","section":"post","summary":"本文介绍微服务相关概念，了解微服务的相关架构和发展。 微服务架构 传统的软件架构，往往是一个单体应用糅合了各种业务模块。随着业务的发展，单体应用","tags":["Spring Cloud"],"title":"微服务相关概念","type":"post","url":"/post/spring-cloud/%E5%BE%AE%E6%9C%8D%E5%8A%A1%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/","weight":0,"wordcount":667},{"author":null,"categories":["Life"],"content":"事 一件事是具有多面性的，不同的人看到的面是不一样的，当然可能和观察者的阅历、角度等有很大的关系。\n看到哪一面，除了事情流露出来的信息外，很大程度上取决于观察者相信什么。\n互联网时代的今天，每天都有很多新闻，各种瓜。\n作为吃瓜群众，往往会跟着舆论或者大众趋势去看待某件事。\n但是信息往往不是那么公开的，看到的可能只是冰山一角。\n这样可能会对别人带来误解，同时使我们自己尴尬。\n保持理性很重要。\n人 人也是有多面性的，是复杂的动物。\n现在的爽文、影视剧中的人物都太单一了，但其实人是很复杂的，具有多面性的。\n身份、角色对人的面有较大的影响。\n人在成长中，经历多面性，看到社会的一角，会慢慢地成熟。\n八面玲珑的人，可谓是人精了。\n少有年轻轻轻的八面玲珑之人，但也不是没有。\n父辈的传道有很关键的作用。\n自我 个体与社会利益总会存在冲突的情况。\n比如现在的教育，是流水线的教育形式，毕业也就是出厂，工作就是到各行各业当螺丝钉。\n这于国家发展而言，是有利的，但对个人来说，是弊大于利的。\n当自我与它物存在冲突面时，如何抉择，往往是个问题。\n很多时候是没有很大选择的，只能被社会、时代裹挟着往前。\n上层决定下层，如何跳出上层画的圈，跳出下层面，很难也很重要。\n所谓富贵险中求，道理人人都懂，但不冒险自然是求不得的。\n","date":1632355200,"description":"","dir":"post\\Shuyou\\","excerpt_html":null,"excerpt_text":null,"expirydate":-62135596800,"fuzzywordcount":600,"html":null,"keywords":null,"kind":"page","lang":"en","lastmod":1632355200,"objectID":"989067799631c6413d79c582d765a2cd","permalink":"http://localhost:1313/post/shuyou/%E9%9D%A2/","publishdate":"2021-09-23T00:00:00Z","readingtime":2,"relpermalink":"/post/shuyou/%E9%9D%A2/","section":"post","summary":"事 一件事是具有多面性的，不同的人看到的面是不一样的，当然可能和观察者的阅历、角度等有很大的关系。 看到哪一面，除了事情流露出来的信息外，很大程","tags":["记录"],"title":"面","type":"post","url":"/post/shuyou/%E9%9D%A2/","weight":0,"wordcount":529},{"author":null,"categories":["Code"],"content":" 本文介绍内容协商相关知识\n内容协商（Content Negotiation） 内容协商：客户端和服务器就响应的资源内容进行交涉，然后服务器提供给客户端最为合适的资源。\n内容协商会以响应资源的语言、字符集、编码方式等作为判断的基准。HTTP请求头中Content-Type，Accept等内容就是内容协商判断的标准。\nHttpMessageConverter HttpMessageConverter为HTTP消息转换接口，Spring Boot根据不同的媒体类型进行了相应的实现。\nMappingJackson2XmlHttpMessageConverter MappingJackson2HttpMessageConverter 当使用@ResponseBody注解时，默认是返回JSON格式的数据。\n当在pom.xml中加入如下两个包：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.fasterxml.jackson.core\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;jackson-databind\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.9.8\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- jackson默认只会支持的json。若要xml的支持，需要额外导入如下包 --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.fasterxml.jackson.dataformat\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;jackson-dataformat-xml\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.9.8\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 指定请求头Accpt: application/xml时，就会返回XML格式的数据。\n这是因为最终都由AbstractMessageConverterMethodProcessor.writeWithMessageConverters()处理\nSpring Boot会根据请求头进行内容协商，返回相应格式的数据。\n通过实现AbstractGenericHttpMessageConverter类，可以自定义HttpMessageConverter。\n除了请求头Accept，还可以根据扩展名、请求参数等方法进行内容协商。\n除了HttpMessageConverter外，还能通过实现HandlerMethodArgumentResolver接口对方法入参进行内容协商。 通过实现HandlerMethodReturnValueHandler接口对返回值进行内容协商。\n值得注意的是：不能在配置类WebMvcConfigurer中通过重写addArgumentResolvers的方式来添加到Spring Boot自带的HandlerMethodArgumentResolver实现类集合，而是通过修改RequestMappingHandlerAdapter来实现。\n参考：\n自定义Spring Boot 内容协商 Spring MVC内容协商 ","date":1631491200,"description":"","dir":"post\\Spring Boot\\","excerpt_html":null,"excerpt_text":null,"expirydate":-62135596800,"fuzzywordcount":900,"html":null,"keywords":null,"kind":"page","lang":"en","lastmod":1631491200,"objectID":"cc30a9e21bb554f02ee942f7364d2944","permalink":"http://localhost:1313/post/spring-boot/spring-boot%E5%86%85%E5%AE%B9%E5%8D%8F%E5%95%86/","publishdate":"2021-09-13T00:00:00Z","readingtime":2,"relpermalink":"/post/spring-boot/spring-boot%E5%86%85%E5%AE%B9%E5%8D%8F%E5%95%86/","section":"post","summary":"本文介绍内容协商相关知识 内容协商（Content Negotiation） 内容协商：客户端和服务器就响应的资源内容进行交涉，然后服务器提供给客","tags":["Spring Boot"],"title":"Spring Boot内容协商","type":"post","url":"/post/spring-boot/spring-boot%E5%86%85%E5%AE%B9%E5%8D%8F%E5%95%86/","weight":0,"wordcount":860},{"author":null,"categories":["Life"],"content":"这一年发生了很多事情，尤其是社会和国家，当然我个人也经历了很多事。\n小时候以为电视里的新闻，政府的政策与我没有关系，影响不到我，越长大越明白权力的力量，那是直接或间接影响一代又一代人的。\n有时候会想是不是在之前的时光里荒废了许多时间，比如读书的时候没用功学习，当然这都是事后复盘。\n有时候往日的种种会在脑海中浮现，然后就会产生不一样的情绪。\n现在还算是比较年轻，但是又老了一岁。\n新的一岁，希望自己更加成熟，工作顺利，赚更多的money，生活越来越美好。\n对过去的事说再见，向着未来出发。\n","date":1631404800,"description":"","dir":"post\\Shuyou\\","excerpt_html":null,"excerpt_text":null,"expirydate":-62135596800,"fuzzywordcount":300,"html":null,"keywords":null,"kind":"page","lang":"en","lastmod":1631404800,"objectID":"291eefe612e16867c71c024dc2c30f95","permalink":"http://localhost:1313/post/shuyou/%E5%8F%88%E4%B8%80%E5%B2%81/","publishdate":"2021-09-12T00:00:00Z","readingtime":1,"relpermalink":"/post/shuyou/%E5%8F%88%E4%B8%80%E5%B2%81/","section":"post","summary":"这一年发生了很多事情，尤其是社会和国家，当然我个人也经历了很多事。 小时候以为电视里的新闻，政府的政策与我没有关系，影响不到我，越长大越明白权","tags":["记录"],"title":"又一岁","type":"post","url":"/post/shuyou/%E5%8F%88%E4%B8%80%E5%B2%81/","weight":0,"wordcount":238},{"author":null,"categories":["Code"],"content":" 过滤器（Filter）和拦截器（Interceptor）是Web项目中常用的两个功能，本文将简单介绍在Spring Boot中使用过滤器和拦截器。\nFilter 过滤器可以用于过滤一些非法字符、权限检查等操作\n在Spring Boot中可以通过注解@WebFilter或者使用配置类来实现过滤器\n1.通过注解\n@WebFilter(\u0026#34;/*\u0026#34;) public class MyFilter implements Filter { @Override public void init(FilterConfig filterConfig) throws ServletException { System.out.println(\u0026#34;过滤器初始化\u0026#34;); } @Override public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException { System.out.println(\u0026#34;开始执行过滤器\u0026#34;); Long start = System.currentTimeMillis(); HttpServletRequest httpServletRequest = (HttpServletRequest) request; String s = httpServletRequest.getRequestURL().toString(); if (s.contains(\u0026#34;/user\u0026#34;)){ response.setCharacterEncoding(\u0026#34;utf-8\u0026#34;); response.setContentType(\u0026#34;text/json;charset=UTF-8\u0026#34;); PrintWriter writer = response.getWriter(); writer.write(\u0026#34;不允许访问此url\u0026#34;); writer.flush(); writer.close(); } chain.doFilter(request, response); System.out.println(\u0026#34;【过滤器】耗时 \u0026#34; + (System.currentTimeMillis() - start)); System.out.println(\u0026#34;结束执行过滤器\u0026#34;); } @Override public void destroy() { System.out.println(\u0026#34;过滤器销毁\u0026#34;); } } 这样当访问\u0026quot;/user\u0026quot;时，会在浏览器显示\u0026quot;不允许访问此url\u0026quot;。 2.通过配置类\n@Configuration public class WebConfig{ @Bean public FilterRegistrationBean timeFilter(){ FilterRegistrationBean filterRegistrationBean = new FilterRegistrationBean(); MyFilter myFilter = new MyFilter(); filterRegistrationBean.setFilter(myFilter); List\u0026lt;String\u0026gt; urlList = new ArrayList\u0026lt;\u0026gt;(); urlList.add(\u0026#34;/*\u0026#34;); filterRegistrationBean.setUrlPatterns(urlList); return filterRegistrationBean; } } 上述配置类也会对所有url进行过滤。\n2.拦截器 拦截器需要实现HandlerInterceptor接口\n@Component public class MyInterceptor implements HandlerInterceptor { @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception { System.out.println(\u0026#34;处理拦截之前\u0026#34;); request.setAttribute(\u0026#34;startTime\u0026#34;, System.currentTimeMillis()); if (null == request.getSession().getAttribute(\u0026#34;SESSION\u0026#34;)){ response.sendRedirect(request.getContextPath() + \u0026#34;/login\u0026#34;); return false; } System.out.println(((HandlerMethod) handler).getBean().getClass().getName()); System.out.println(((HandlerMethod) handler).getMethod().getName()); return true; } @Override public void postHandle(HttpServletRequest request, HttpServletResponse response, Object handler, ModelAndView modelAndView) throws Exception { System.out.println(\u0026#34;开始处理拦截\u0026#34;); Long start = (Long) request.getAttribute(\u0026#34;startTime\u0026#34;); System.out.println(\u0026#34;【拦截器】耗时 \u0026#34; + (System.currentTimeMillis() - start)); } @Override public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) throws Exception { System.out.println(\u0026#34;处理拦截之后\u0026#34;); Long start = (Long) request.getAttribute(\u0026#34;startTime\u0026#34;); System.out.println(\u0026#34;【拦截器】耗时 \u0026#34; + (System.currentTimeMillis() - start)); System.out.println(\u0026#34;异常信息 \u0026#34; + ex); } } 上述拦截器会判断是否登录，对未登录的进行拦截。 还需进行注册才能生效。\n@Configuration public class WebConfig implements WebMvcConfigurer { @Autowired MyInterceptor myInterceptor; @Override public void addInterceptors(InterceptorRegistry registry) { registry.addInterceptor(myInterceptor) .addPathPatterns(\u0026#34;/user\u0026#34;) .excludePathPatterns(\u0026#34;/jacksonSerialization\u0026#34;); } } 过滤器要先于拦截器执行，晚于拦截器结束。 上图很好的描述了他们的执行时间。\n参考：\nSpring Boot中使用过滤器和拦截器 ","date":1631232000,"description":"","dir":"post\\Spring Boot\\","excerpt_html":null,"excerpt_text":null,"expirydate":-62135596800,"fuzzywordcount":1000,"html":null,"keywords":null,"kind":"page","lang":"en","lastmod":1631232000,"objectID":"1eb97afa1611276086a284a190e19f8d","permalink":"http://localhost:1313/post/spring-boot/spring-boot%E4%B8%AD%E4%BD%BF%E7%94%A8%E8%BF%87%E6%BB%A4%E5%99%A8%E5%92%8C%E6%8B%A6%E6%88%AA%E5%99%A8/","publishdate":"2021-09-10T00:00:00Z","readingtime":2,"relpermalink":"/post/spring-boot/spring-boot%E4%B8%AD%E4%BD%BF%E7%94%A8%E8%BF%87%E6%BB%A4%E5%99%A8%E5%92%8C%E6%8B%A6%E6%88%AA%E5%99%A8/","section":"post","summary":"过滤器（Filter）和拦截器（Interceptor）是Web项目中常用的两个功能，本文将简单介绍在Spring Boot中使用过滤器和拦截","tags":["Spring Boot"],"title":"Spring Boot中使用过滤器和拦截器","type":"post","url":"/post/spring-boot/spring-boot%E4%B8%AD%E4%BD%BF%E7%94%A8%E8%BF%87%E6%BB%A4%E5%99%A8%E5%92%8C%E6%8B%A6%E6%88%AA%E5%99%A8/","weight":0,"wordcount":967},{"author":null,"categories":["Code"],"content":" 本文介绍常用的JSON相关操作，Spring Boot内置Jackson包可以进行JSON相关操作\n序列化 Fastjson User user1 = new User(); String s1 = JSON.toJSONString(user1); System.out.println(s1); Jackson ObjectMapper objectMapper = new ObjectMapper(); String s2 = objectMapper.writeValueAsString(user); System.out.println(s2); 反序列化 Fastjson String s =\u0026#34;{\\n\u0026#34; + \u0026#34; \\\u0026#34;id\\\u0026#34;: 1,\\n\u0026#34; + \u0026#34; \\\u0026#34;userName\\\u0026#34;: \\\u0026#34;zhangsan\\\u0026#34;,\\n\u0026#34; + \u0026#34; \\\u0026#34;password\\\u0026#34;: \\\u0026#34;123456\\\u0026#34;,\\n\u0026#34; + \u0026#34; \\\u0026#34;userSex\\\u0026#34;: \\\u0026#34;man\\\u0026#34;,\\n\u0026#34; + \u0026#34; \\\u0026#34;nickName\\\u0026#34;: \\\u0026#34;asdf\\\u0026#34;,\\n\u0026#34; + \u0026#34; \\\u0026#34;birthday\\\u0026#34;: \\\u0026#34;2000-09-11 00:00:00\\\u0026#34;\\n\u0026#34; + \u0026#34;}\u0026#34;; User user = JSON.parseObject(s, User.class); System.out.println(user.toString()); Jackson User user = (User) objectMapper.readValue(s, User.class); System.out.println(user.toString()); 自定义ObjectMapper Spring Boot内置了Jackson来完成JSON的序列化和反序列化。\n在Spring中使用@ResponseBody注解可以将方法返回的对象序列化成json串\n在Spring Boot中可以自定义一个ObjectMapper来序列化我们想要返回的格式，比如序列化时间\npackage com.springboot.demos.config; import com.fasterxml.jackson.databind.ObjectMapper; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; import java.text.SimpleDateFormat; /** * @author zousy * @version v1.0 * @Description * @date 2021-09-07 15:22 */ @Configuration public class JacksonConfig { @Bean public ObjectMapper getObjectMapper(){ ObjectMapper mapper = new ObjectMapper(); mapper.setDateFormat(new SimpleDateFormat(\u0026#34;yyyy-MM-dd HH:mm:ss\u0026#34;)); return mapper; } } 这样就会返回如下json串：\n{ \u0026#34;id\u0026#34;: 1, \u0026#34;userName\u0026#34;: \u0026#34;zhangsan\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;123456\u0026#34;, \u0026#34;nickName\u0026#34;: \u0026#34;asdf\u0026#34;, \u0026#34;birthday\u0026#34;: \u0026#34;2000-09-11 00:00:00\u0026#34;, \u0026#34;sex\u0026#34;: \u0026#34;man\u0026#34; } Jackson注解 1.@JsonProperty @JsonProperty，作用在属性上，用来为JSON Key指定一个别名。\n2.@Jsonlgnore @Jsonlgnore，作用在属性上，用来忽略此属性。\n3.@JsonIgnoreProperties @JsonIgnoreProperties，忽略一组属性，作用于类上，比如JsonIgnoreProperties({ \u0026ldquo;password\u0026rdquo;, \u0026ldquo;birthday\u0026rdquo; })。\n4.@JsonFormat @JsonFormat，用于日期格式化，如：@JsonFormat(pattern = \u0026ldquo;yyyy-MM-dd HH:mm:ss\u0026rdquo;)\n还有其他的一些注解，这里不再介绍可参考官方文档。\nJSON相关操作 JSON数组字符串\u0026ndash;\u0026gt;List Jackson\nString jsonArray = \u0026#34;[{\\\u0026#34;brand\\\u0026#34;:\\\u0026#34;ford\\\u0026#34;}, {\\\u0026#34;brand\\\u0026#34;:\\\u0026#34;Fiat\\\u0026#34;}]\u0026#34;; ObjectMapper objectMapper = new ObjectMapper(); List\u0026lt;Car\u0026gt; cars1 = objectMapper.readValue(jsonArray, new TypeReference\u0026lt;List\u0026lt;Car\u0026gt;\u0026gt;(){}); Fastjson\nString jsonArray = \u0026#34;[{\\\u0026#34;brand\\\u0026#34;:\\\u0026#34;ford\\\u0026#34;}, {\\\u0026#34;brand\\\u0026#34;:\\\u0026#34;Fiat\\\u0026#34;}]\u0026#34;; List\u0026lt;Car\u0026gt; cars = JSON.parseArray(jsonArray, Car.class); Jackson 主要操作JSON的类是 ObjectMapper FastJson 主要操作JSON的类是 JSON、JSONObject、JSONArray\n参考：\nSpring Boot中的JSON技术 Jackson使用详解 FastJson使用详解 ","date":1630972800,"description":"","dir":"post\\Spring Boot\\","excerpt_html":null,"excerpt_text":null,"expirydate":-62135596800,"fuzzywordcount":700,"html":null,"keywords":null,"kind":"page","lang":"en","lastmod":1630972800,"objectID":"d3ddcedde38e66d8c486e0310046f57b","permalink":"http://localhost:1313/post/spring-boot/spring-boot%E4%B8%AD%E7%9A%84json/","publishdate":"2021-09-07T00:00:00Z","readingtime":2,"relpermalink":"/post/spring-boot/spring-boot%E4%B8%AD%E7%9A%84json/","section":"post","summary":"本文介绍常用的JSON相关操作，Spring Boot内置Jackson包可以进行JSON相关操作 序列化 Fastjson User user1 = new User(); String s1 = JSON.toJSONString(user1); System.out.println(s1); Jackson ObjectMapper objectMapper = new ObjectMapper(); String s2","tags":["Spring Boot"],"title":"Spring Boot中操作JSON","type":"post","url":"/post/spring-boot/spring-boot%E4%B8%AD%E7%9A%84json/","weight":0,"wordcount":698},{"author":null,"categories":["Code"],"content":" 本文介绍Spring Boot开发web相关注解知识\n基础web注解 Bean处理\n@Component：通用的注解，可标注任意类为 Spring 组件。如果一个 Bean 不知道属于哪个层，可以使用@Component 注解标注； @Repository：对应持久层即 Dao 层，主要用于数据库相关操作； @Service：对应服务层，主要涉及一些复杂的逻辑，需要用到 Dao 层； @Controller：对应 Spring MVC 控制层，一般需要注入 Service 类返回结果数据； @RestController：继承于 @Controller，区别在于标注后整个类所有方法将直接返回 JSON 数据，不再需要视图解析处理，目前前后端分离的项目后端都是直接用这个注解的； @Configuration：标注是 Java 代码的配置类， Spring Boot 中推荐这种做法不再使用 xml 配置了； @Scope：声明 Spring Bean 的作用域,作用于一共有以下几种： singleton：唯一 bean 实例，Spring 中的 bean 默认都是单例的。 prototype：每次请求都会创建一个新的 bean 实例。 request：每一次 HTTP 请求都会产生一个新的 bean，该 bean 仅在当前 HTTP request 内有效。 session：每一次 HTTP 请求都会产生一个新的 bean，该 bean 仅在当前 HTTP session 内有效。 HTTP请求\n@RequestMapping：@RequestMapping(value=\u0026quot;/test\u0026quot;,method=RequestMethod.GET)可以指定路径和请求方法 @GetMapping：get请求注解 @PostMapping：post请求注解 @PutMapping @DeleteMapping 前后端参数传递\n@RequestParam：用在方法的参数前面，获取请求中表单类型的key=value格式的数据。 @PathVariable：用于获取请求路径中的参数。 @RequestBody：获取请求 body 中的数据，常用于搭配 @PostMapping 请求来提交对象数据. 请求体 的Content-Type 必须为 application/json 格式的数据，接收到数据之后会自动将数据绑定到 Java 对象上去。系统会使用 HttpMessageConverter 或者自定义的 HttpMessageConverter将请求的 body 中的 json 字符串转换为 java 对象。 @ResponseBody：表示该方法的返回结果直接写入 HTTP response body 中，格式为 json。上面我们提到的 @RestController 其实就是 @Controller 和 @ResponseBody 两个结合起来的。 读取配置\n@value：可以在任意 Spring 管理的 Bean 中通过这个注解获取任何来源配置的属性值。 @ConfigurationProperties：指定该类是配置类，且可以指定配置前缀 @PropertySource：定读取我们自定义的配置文件的。 @Component @ConfigurationProperties(prefix= \u0026#34;my\u0026#34; ) @PropertySource(value = {\u0026#34;classpath:my.properties\u0026#34;}) @Data public class MyProperties { private int maxValue= 0; } 参数校验\n@NotEmpty 被注释的字符串的不能为 null 也不能为空 @NotBlank 被注释的字符串非 null，并且必须包含一个非空白字符 @Null 被注释的元素必须为 null @NotNull 被注释的元素必须不为 null @AssertTrue 被注释的元素必须为 true @AssertFalse 被注释的元素必须为 false @Pattern(regex=,flag=)被注释的元素必须符合指定的正则表达式 @Email 被注释的元素必须是 Email 格式。 @Min(value)被注释的元素必须是一个数字，其值必须大于等于指定的最小值 @Max(value)被注释的元素必须是一个数字，其值必须小于等于指定的最大值 @DecimalMin(value)被注释的元素必须是一个数字，其值必须大于等于指定的最小值 @DecimalMax(value) 被注释的元素必须是一个数字，其值必须小于等于指定的最大值 @Size(max=, min=)被注释的元素的大小必须在指定的范围内 @Digits (integer, fraction)被注释的元素必须是一个数字，其值必须在可接受的范围内 @Past被注释的元素必须是一个过去的日期 @Future 被注释的元素必须是一个将来的日期 只需要在请求处理方法中需要验证的参数前加上 @Valid 注解就会开启校验了，如果验证失败将抛出异常：MethodArgumentNotValidException。\n@Validated：如果你的入参不是用一个 Java 对象来接收的话，比如用 @PathVariables 和 @RequestParam 注解来获取入参，这种情况下要校验参数不要忘记在类的头上加 @Validated 注解，这个参数可以告诉 Spring 去校验方法参数。 @RestController @RequestMapping(\u0026#34;/user\u0026#34;) @Validated public class UserController { @GetMapping(\u0026#34;/{id}\u0026#34;) public ResponseEntity\u0026lt;List\u0026lt;User\u0026gt;\u0026gt; findById( @PathVariable @Max(value = 5,message = \u0026#34;超过 id 的范围了\u0026#34;) long id) { return new ResponseEntity(userService.findById(id),HttpStatus.OK); } } 统一异常处理\n@ControllerAdvice：定义全局异常处理类，包含 @Component 所以可以被 Spring 扫描到。 @ExceptionHandler : 声明异常处理方法，表示遇到这个异常，就执行标注的方法。 配置启动\n@SpringBootApplication：等价于使用 @Configuration、@EnableAutoConfiguration、@ComponentScan 三个注解。 @Configuration：声明是是一个 Java 形式的配置类，Spring Boot 提倡基于 Java 的配置，相当于你之前在 xml 中配置 bean； @EnableConfigurationProperties： 指定配置类，并使配置类生效 @ComponentScan：标注哪些路径下的类需要被Spring扫描。 @Conditional：Spring4 新提供的注解，通过 @Conditional 注解可以根据代码中设置的条件装载不同的 bean，也是SpringBoot实现自动配置的基石。 ","date":1630454400,"description":"","dir":"post\\Spring Boot\\","excerpt_html":null,"excerpt_text":null,"expirydate":-62135596800,"fuzzywordcount":2100,"html":null,"keywords":null,"kind":"page","lang":"en","lastmod":1630454400,"objectID":"686302f5ae1f484fabfe548d362314d1","permalink":"http://localhost:1313/post/spring-boot/spring-boot%E7%9B%B8%E5%85%B3%E6%B3%A8%E8%A7%A3/","publishdate":"2021-09-01T00:00:00Z","readingtime":5,"relpermalink":"/post/spring-boot/spring-boot%E7%9B%B8%E5%85%B3%E6%B3%A8%E8%A7%A3/","section":"post","summary":"本文介绍Spring Boot开发web相关注解知识 基础web注解 Bean处理 @Component：通用的注解，可标注任意类为 Spring 组件。如果一个","tags":["Spring Boot"],"title":"Spring Boot相关注解","type":"post","url":"/post/spring-boot/spring-boot%E7%9B%B8%E5%85%B3%E6%B3%A8%E8%A7%A3/","weight":0,"wordcount":2041},{"author":null,"categories":["Code"],"content":" 本文介绍Spring Boot自动配置相关知识\nSpring 3.0开始，Spring提供了Java Config的方式，进行Spring bean的创建\n@Configuration public class DemoConfiguration { @Bean public void object() { return new Obejct(); } } 通过在类上添加 @Configuration 注解，声明这是一个 Spring 配置类。 通过在方法上添加 @Bean 注解，声明该方法创建一个 Spring Bean。 在Spring Boot中也可以使用上述注解进行Bean的配置。\n配置类：在类上添加了 @Configuration 注解，声明这是一个配置类 条件注解：在类上添加了 @ConditionalOnWebApplication 条件注解 配置属性：使用@ConfigurationProperties 注解声明配置属性类和 @EnableConfigurationProperties 注解让配置属性类生效 自动配置 上面的介绍仅仅是解决了配置的问题，Spring Boot是如何实现自动配置的呢？\n@SpringBootApplication 注解中有 @EnableAutoConfiguration 这样一个注解。而@EnableAutoConfiguration 这个注解看名字就知道是启用自动配置注解\n@EnableAutoConfiguration\n@EnableAutoConfiguration使用@Import添加了一个AutoConfigurationImportSelector类，Spring自动注入配置的核心功能就依赖于这个对象。\n在这个类中，提供了一个getCandidateConfigurations()方法用来加载配置文件。借助Spring提供的工具类SpringFactories的loadFactoryNames()方法加载配置文件。扫描的默认路径位于META-INF/spring.factories中。\n原先 @Configuration 注解的配置类，就升级成类自动配置类。这样，Spring Boot 在获取到需要自动配置的配置类后，就可以自动创建相应的 Bean，完成自动配置的功能。\nAutoConfigurationImportSelector类中的getAutoConfigurationEntry()方法 ConfigurationClassPostProcessor类实现了BeanDefinitionRegistryPostProcessor接口。 而在Spring刷新容器时，会实例化BeanDefinitionRegistryPostProcessor接口的实现类，并调用它的postProcessBeanDefinitionRegistry方法\rConfigurationClassPostProcessor类的postProcessBeanDefinitionRegistry调用了ConfigurationClassParser类的getImports方法，该方法会调用AutoConfigurationImportSelector的process方法进行自动配置类的导入和过滤。 ConfigurationClassParser类起到解析配置类的作用 ConfigurationClassPostProcessor类中会将解析的配置类注入到Spring IOC容器 ","date":1630368000,"description":"","dir":"post\\Spring Boot\\","excerpt_html":null,"excerpt_text":null,"expirydate":-62135596800,"fuzzywordcount":1200,"html":null,"keywords":null,"kind":"page","lang":"en","lastmod":1630368000,"objectID":"096c6487348ec110039177e9672cd096","permalink":"http://localhost:1313/post/spring-boot/spring-boot%E8%87%AA%E5%8A%A8%E9%85%8D%E7%BD%AE/","publishdate":"2021-08-31T00:00:00Z","readingtime":3,"relpermalink":"/post/spring-boot/spring-boot%E8%87%AA%E5%8A%A8%E9%85%8D%E7%BD%AE/","section":"post","summary":"本文介绍Spring Boot自动配置相关知识 Spring 3.0开始，Spring提供了Java Config的方式，进行Spring bean的创建 @Configuration public class","tags":["Spring Boot"],"title":"Spring Boot自动配置相关知识","type":"post","url":"/post/spring-boot/spring-boot%E8%87%AA%E5%8A%A8%E9%85%8D%E7%BD%AE/","weight":0,"wordcount":1161},{"author":null,"categories":["Life"],"content":"最近看了电视剧天道，根据豆豆的小说遥远的救世主改编的。\n有比较大的触动。\n书中有这样一句话：神就是道，道就是规律。规律如来，容不得你思议，按规律办事的人就是神。\n从来没有救世主，能救自己的只有自己呀。\n古语说：有道无术，术尚可求，有术无道，止于术\n以前我是没有道的，做一件事情，也没有很大的兴趣探究所以然，像是被推着往前，学习和工作都是如此。\n但是我想今后我的道应该是努力去追寻事务的规律,了解人的交流和社会的运作。\n","date":1629676800,"description":"","dir":"post\\Shuyou\\","excerpt_html":null,"excerpt_text":null,"expirydate":-62135596800,"fuzzywordcount":300,"html":null,"keywords":null,"kind":"page","lang":"en","lastmod":1629676800,"objectID":"ebd2dc2e820f2e101ba802583419a515","permalink":"http://localhost:1313/post/shuyou/%E9%81%A5%E8%BF%9C%E7%9A%84%E6%95%91%E4%B8%96%E4%B8%BB/","publishdate":"2021-08-23T00:00:00Z","readingtime":1,"relpermalink":"/post/shuyou/%E9%81%A5%E8%BF%9C%E7%9A%84%E6%95%91%E4%B8%96%E4%B8%BB/","section":"post","summary":"最近看了电视剧天道，根据豆豆的小说遥远的救世主改编的。 有比较大的触动。 书中有这样一句话：神就是道，道就是规律。规律如来，容不得你思议，按规律","tags":["记录"],"title":"天道","type":"post","url":"/post/shuyou/%E9%81%A5%E8%BF%9C%E7%9A%84%E6%95%91%E4%B8%96%E4%B8%BB/","weight":0,"wordcount":202},{"author":null,"categories":["Life"],"content":"时间过得真快呀，就到八月了。这个月回到深圳了，可能下个月还得去北京，流下不争气的泪水。\n想以后每个月都可以写一篇杂文，可以回顾这个月发生的事儿，或者吃瓜下网上的热门话题。\n最近沉迷永结无间，想双十一入手一台主机电脑，但是最近显卡太贵了，还没想好倒底买不买。马上要国庆了，应该要回武汉。\n最近在学习Golang语言，准备通过一些小项目来熟悉这门语言。打算慢慢补上自己因为出差而欠下的相关八股文。\n突然发现自己做啥事都没有基调，就像现在写的东西，挺乱的，但是自我感觉最近成长了许多，自己将事业和发展放到了更重要的位置。\n世界一日千里啊，发展太快了。要跟得上，与时俱进是必须的，确定主基调的同时，要顺应时代的发展。\n","date":1629244800,"description":"","dir":"post\\Shuyou\\","excerpt_html":null,"excerpt_text":null,"expirydate":-62135596800,"fuzzywordcount":300,"html":null,"keywords":null,"kind":"page","lang":"en","lastmod":1629244800,"objectID":"ce087d7cfe8f13c157de9c003ed7740f","permalink":"http://localhost:1313/post/shuyou/%E5%BA%94%E6%98%AF%E5%A6%82%E6%AD%A4/","publishdate":"2021-08-18T00:00:00Z","readingtime":1,"relpermalink":"/post/shuyou/%E5%BA%94%E6%98%AF%E5%A6%82%E6%AD%A4/","section":"post","summary":"时间过得真快呀，就到八月了。这个月回到深圳了，可能下个月还得去北京，流下不争气的泪水。 想以后每个月都可以写一篇杂文，可以回顾这个月发生的事儿","tags":["记录"],"title":"无题","type":"post","url":"/post/shuyou/%E5%BA%94%E6%98%AF%E5%A6%82%E6%AD%A4/","weight":0,"wordcount":299},{"author":null,"categories":["Code"],"content":" 总结下synchronized和volatile关键字相关八股文\n在多线程环境下，线程是交替执行的，多线程竞争共享资源容易产生线程不安全的问题，甚至产生死锁的问题。\n使用多线程，一定要保证程序是线程安全的。通常会采用以下一些方法来保证线程安全：\n多线程之间没有共享变量，即考虑使用线程私有变量 加锁 CAS synchronized\nSynchronized是Java的关键字，它可以将代码块锁起来。\npublic void synchronized lock(){ //do something } 可以修饰普通方法、代码块、静态方法等。\nSynchronized是可重入锁、互斥锁、内置锁（监视器锁）\n可重入锁：是指允许同一个线程多次获取同一把锁 互斥锁：是指同一时刻只允许一个线程进入锁 内置锁：是值使用对象的内置锁（监视器）来实现的锁 Synchronized是由相关字节码指令实现的：monitorenter和monitorexit，而这两个指令依赖于底层操作系统Mutex Lock来实现的。但是使用Mutex Lock需要将当前线程挂起并从用户态切换到内核态来执行，这种切换的代价是非常昂贵的。在jdk1.6后，对synchronized关键字进行了大量的优化，如锁升级、偏向锁、轻量级锁、重量级锁等。\n在Java SE 1.6里Synchronied同步锁，一共有四种状态：无锁、偏向锁、轻量级锁、重量级锁，它会随着竞争情况逐渐升级。锁可以升级但是不可以降级，目的是为了提供获取锁和释放锁的效率。\n锁膨胀方向： 无锁 → 偏向锁 → 轻量级锁 → 重量级锁 (此过程是不可逆的)\n偏向锁\n当一个线程访问同步块并获取锁时，会在对象头和栈帧中的锁记录里存储锁偏向的线程ID，以后该线程在进入和退出同步块时不需要进行CAS操作来加锁和解锁。只需要简单地测试一下对象头的Mark Word里是否存储着指向当前线程的偏向锁。如果成功，表示线程已经获取到了锁。\n偏向锁使用了一种等待竞争出现才会释放锁的机制。所以当其他线程尝试获取偏向锁时，持有偏向锁的线程才会释放锁。\n但是偏向锁的撤销需要等到全局安全点(就是当前线程没有正在执行的字节码)。它会首先暂停拥有偏向锁的线程，让你后检查持有偏向锁的线程是否活着。如果线程不处于活动状态，直接将对象头设置为无锁状态。如果线程活着，JVM会遍历栈帧中的锁记录，栈帧中的锁记录和对象头要么偏向于其他线程，要么恢复到无锁状态或者标记对象不适合作为偏向锁。\n轻量级锁\n轻量级加锁：线程在执行同步块之前，JVM会先在当前线程的栈桢中创建用于存储锁记录的空间，并 将对象头中的Mark Word复制到锁记录中，官方称为Displaced Mark Word。然后线程尝试使用 CAS将对象头中的Mark Word替换为指向锁记录的指针。如果成功，当前线程获得锁，如果失 败，表示其他线程竞争锁，当前线程便尝试使用自旋来获取锁。\n轻量级解锁：会使用原子的CAS操作将Displaced Mark Word替换回到对象头，如果成 功，则表示没有竞争发生。如果失败，表示当前锁存在竞争，锁就会膨胀成重量级锁。\nvolatile\n在多线程并发编程中synchronized和volatile都扮演着重要的角色，volatile是轻量级的 synchronized，它在多处理器开发中保证了共享变量的“可见性”。可见性的意思是当一个线程 修改一个共享变量时，另外一个线程能读到这个修改的值。如果volatile变量修饰符使用恰当 的话，它比synchronized的使用和执行成本更低，因为它不会引起线程上下文的切换和调度。\n为了性能优化，JMM 在不改变正确语义的前提下，会允许编译器和处理器对指令序列进行重排序。JMM 提供了内存屏障阻止这种重排序。\nJava 编译器会在生成指令系列时在适当的位置会插入内存屏障指令来禁止特定类型的处理器重排序。\nvolatile保证\u0026quot;可见性\u0026quot;，但不保证\u0026quot;原子性\u0026quot;。\n","date":1627257600,"description":"","dir":"post\\Java Concurrent\\","excerpt_html":null,"excerpt_text":null,"expirydate":-62135596800,"fuzzywordcount":1600,"html":null,"keywords":null,"kind":"page","lang":"en","lastmod":1627257600,"objectID":"297d89c850d84c069dc7e77bbc653463","permalink":"http://localhost:1313/post/java-concurrent/synchronized%E5%92%8Cvolatile%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86/","publishdate":"2021-07-26T00:00:00Z","readingtime":4,"relpermalink":"/post/java-concurrent/synchronized%E5%92%8Cvolatile%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86/","section":"post","summary":"总结下synchronized和volatile关键字相关八股文 在多线程环境下，线程是交替执行的，多线程竞争共享资源容易产生线程不安全的问题","tags":["Java并发"],"title":"synchronized和volatile相关知识","type":"post","url":"/post/java-concurrent/synchronized%E5%92%8Cvolatile%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86/","weight":0,"wordcount":1556},{"author":null,"categories":["Code"],"content":" 本文介绍Cookie、Session和Token相关知识\n什么是Cookie\n由于Http是无状态的协议，这意味着一旦数据提交后，客户端与服务器的连接就会关闭，再次交互的时候需要重新建立新的连接。\n服务器无法确认用户信息，于是W3C就提出给每个用户发一个通行证，无论谁访问都携带通行证，这样服务器就能从通行证上确认用户信息，这个通行证就是cookie。\ncookie存储在客户端：cookie 是服务器发送到用户浏览器并保存在本地的一小块数据，它会在浏览器下次向同一服务器再发起请求时被携带并发送到服务器上。 cookie是不可跨域的： 每个 cookie 都会绑定单一的域名，无法在别的域名下获取使用，一级域名和二级域名之间是允许共享使用的（靠的是 domain）。 在servlet中使用cookie\nresponse.setContentType(\u0026#34;text/html;charset=UTF-8\u0026#34;); Cookie cookie = new Cookie(\u0026#34;user\u0026#34;,\u0026#34;zsy\u0026#34;); cookie.setMaxAge(30000); response.addCookie(cookie); response.getWriter().write(\u0026#34;返回Cookie\u0026#34;); 什么是Session\nSession 是另⼀种记录浏览器状态的机制。Cookie保存在浏览器中， 而Session保存在服务器中。⽤户使⽤浏览器访问服务器的时候，服务器把⽤户的信息以某种的形式记录在服务器，这就是Session。\nSession可以存储任何类型的数据，而Cookie只能存字符串 Session存在服务器，Cookie存在客户端浏览器上 Session是基于Cookie实现的，依赖于名为JSESSIONID的Cookie Cookie 可设置为长时间保持，比如我们经常使用的默认登录功能，Session 一般失效时间较短，客户端关闭（默认情况下）或者 Session 超时都会失效 在servlet中使用session\n//得到Session对象 HttpSession httpSession = request.getSession(); //设置Session属性 httpSession.setAttribute(\u0026#34;name\u0026#34;, \u0026#34;看完博客就要点赞！！ \u0026#34;); //得到Session对象 HttpSession httpSession = request.getSession(); //设置Session属性 httpSession.setAttribute(\u0026#34;name\u0026#34;, \u0026#34;看完博客就要点赞！！ \u0026#34;); 什么是Token\ntoken是访问资源接口（API）时所需要的资源凭证\n每一次请求都需要携带 token，需要把 token 放到 HTTP 的 Header 里 基于 token 的用户认证是一种服务端无状态的认证方式，服务端不用存放 token 数据。用解析 token 的计算时间换取 session 的存储空间，从而减轻服务器的压力，减少频繁的查询数据库 使用JWT（Json Web Token）可以解决跨域相关问题 ","date":1626393600,"description":"","dir":"post\\Tomcat\\","excerpt_html":null,"excerpt_text":null,"expirydate":-62135596800,"fuzzywordcount":1000,"html":null,"keywords":null,"kind":"page","lang":"en","lastmod":1626393600,"objectID":"0b35939cbd076a2f6d46f1f8d2e52ee9","permalink":"http://localhost:1313/post/tomcat/cookiesession%E5%92%8Ctoken/","publishdate":"2021-07-16T00:00:00Z","readingtime":2,"relpermalink":"/post/tomcat/cookiesession%E5%92%8Ctoken/","section":"post","summary":"本文介绍Cookie、Session和Token相关知识 什么是Cookie 由于Http是无状态的协议，这意味着一旦数据提交后，客户端与服务器","tags":["Tomcat"],"title":"Cookie、Session和Token相关知识","type":"post","url":"/post/tomcat/cookiesession%E5%92%8Ctoken/","weight":0,"wordcount":934},{"author":null,"categories":["Life"],"content":"最近一直都没写博客，是因为来北京出差了，在中信建投这边做项目，很累，没什么时间写。\n都连续4个周末全加班了，很累，估计年底要辞职回家了。希望下半年不用来这边做二期的项目。\n很累但是又感觉啥都没做，这种状态是很可怕的。很多时候还是很烦躁，怕生，焦虑，但是能感觉到比以前的状态好多了。\n记录下最近的想法：\n要保持乐观的心态，积极向上的人生态度 多与人交流，别怕生 不要为过去的事情烦恼，多思考未来 做事情要专注，努力去做想做的事 要学会找到正确的方法，多观察，多思考 了解更多的事，多听，多看，停止事不关己的懒惰 多经营自己的人生，少羡慕别人 ","date":1625270400,"description":"","dir":"post\\Shuyou\\","excerpt_html":null,"excerpt_text":null,"expirydate":-62135596800,"fuzzywordcount":300,"html":null,"keywords":null,"kind":"page","lang":"en","lastmod":1625270400,"objectID":"8c567f913294e502bcc84a35f988f570","permalink":"http://localhost:1313/post/shuyou/%E5%A6%82%E9%A2%98/","publishdate":"2021-07-03T00:00:00Z","readingtime":1,"relpermalink":"/post/shuyou/%E5%A6%82%E9%A2%98/","section":"post","summary":"最近一直都没写博客，是因为来北京出差了，在中信建投这边做项目，很累，没什么时间写。 都连续4个周末全加班了，很累，估计年底要辞职回家了。希望下","tags":["记录"],"title":"道阻且长 行则将至","type":"post","url":"/post/shuyou/%E5%A6%82%E9%A2%98/","weight":0,"wordcount":259},{"author":null,"categories":["Code"],"content":" 本文介绍使用docker操作Redis主从复制相关内容\n这里使用 docker-compose 来测试 Redis 的主从复制功能。\n首先需要安装 docker 和 docker-compose 以及 Redis 镜像，这里就不再做这方面的安装介绍了。\n安装好后，需要编写 docker-compose.yml 文件\nversion: \u0026#39;3\u0026#39; services: master: image: redis container_name: redis-master restart: always command: redis-server --port 6379 --requirepass master123 --appendonly yes ports: - 6379:6379 volumes: - ./data/master:/data slave1: image: redis container_name: redis-slave-1 restart: always command: redis-server --slaveof master 6379 --port 6380 --requirepass slave123 --masterauth master123 --appendonly yes ports: - 6380:6380 volumes: - ./data/slave1:/data slave2: image: redis container_name: redis-slave-2 restart: always command: redis-server --slaveof master 6379 --port 6381 --requirepass slave456 --masterauth master123 --appendonly yes ports: - 6381:6381 volumes: - ./data/slave2:/data 然后在当前文件夹下，输入命令: docker-compose up -d\n运行结果：\nzsy@zsy:~/redis/testSlave$ ll total 16 drwxr-xr-x 3 zsy zsy 4096 Oct 21 11:17 ./ drwxr-xr-x 4 zsy zsy 4096 Oct 21 11:07 ../ drwxr-xr-x 5 root root 4096 Oct 21 11:17 data/ -rw-r--r-- 1 zsy zsy 1112 Oct 21 11:17 docker-compose.yml zsy@zsy:~/redis/testSlave$ docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 20adf82b1afd redis \u0026#34;docker-entrypoint.s…\u0026#34; 3 hours ago Up 3 hours 0.0.0.0:6379-\u0026gt;6379/tcp redis-master 6ba1c3c93d2e redis \u0026#34;docker-entrypoint.s…\u0026#34; 3 hours ago Up 3 hours 6379/tcp, 0.0.0.0:6381-\u0026gt;6381/tcp redis-slave-2 96b023c4bc57 redis \u0026#34;docker-entrypoint.s…\u0026#34; 3 hours ago Up 15 minutes 6379/tcp, 0.0.0.0:6380-\u0026gt;6380/tcp redis-slave-1 zsy@zsy:~/redis/testSlave$ 测试主从复制：\n测试是否能向从库写入数据？ 测试写入数据到主库，从库有没有数据？ 测试一个从库节点挂掉，主库写入数据，从库节点重启后，是否能同步主库数据？ 测试主库挂点，从库是否能读取数据？ 答案：\n不能 有 能同步 能 参考： docker-compose搭建redis哨兵集群 使用docker 搭建redis的主从复制\n","date":1621555200,"description":"","dir":"post\\Redis\\","excerpt_html":null,"excerpt_text":null,"expirydate":-62135596800,"fuzzywordcount":600,"html":null,"keywords":null,"kind":"page","lang":"en","lastmod":1621555200,"objectID":"f5253d855248c6ffd63a790930210e2d","permalink":"http://localhost:1313/post/redis/redis%E5%AE%9E%E6%88%98----%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6/","publishdate":"2021-05-21T00:00:00Z","readingtime":2,"relpermalink":"/post/redis/redis%E5%AE%9E%E6%88%98----%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6/","section":"post","summary":"本文介绍使用docker操作Redis主从复制相关内容 这里使用 docker-compose 来测试 Redis 的主从复制功能。 首先需要安装 docker 和 docker-compose 以及 Redis 镜像，这里就不再做这方面的安","tags":["Redis"],"title":"Redis实战----主从复制","type":"post","url":"/post/redis/redis%E5%AE%9E%E6%88%98----%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6/","weight":0,"wordcount":510},{"author":null,"categories":["Code"],"content":" 本文介绍使用docker操作Redis哨兵机制相关内容\n还是使用 docker-compose 来测试 Redis 的哨兵机制。\n这里建议使用一个 docker-compose 来测试，我一开始是使用的两个 yaml 文件，会出现主节点挂掉之后，从节点无法切换为主节点的情况，搞了很久，后面看了下网上的说法，感觉应该是网络共享的问题。\n先建立一个文件夹测试哨兵机制，这边是我的目录结构\nzsy@zsy:~/redis/testSentinel$ ll total 32 drwxr-xr-x 3 zsy zsy 4096 Oct 21 17:23 ./ drwxr-xr-x 4 zsy zsy 4096 Oct 21 15:41 ../ drwxr-xr-x 5 root root 4096 Oct 21 17:23 data/ -rw-r--r-- 1 zsy zsy 1751 Oct 21 17:23 docker-compose.yml -rw-r--r-- 1 zsy zsy 264 Oct 21 17:21 sentinel.conf -rw-r--r-- 1 zsy zsy 264 Oct 21 17:21 sentinel1.conf -rw-r--r-- 1 zsy zsy 264 Oct 21 17:21 sentinel2.conf -rw-r--r-- 1 zsy zsy 264 Oct 21 17:21 sentinel3.conf 在文件夹里新建一个 docker-compose.yml\nversion: \u0026#39;3\u0026#39; services: master: image: redis container_name: redis-master restart: always command: redis-server --port 6379 --requirepass 123456 --masterauth 123456 --appendonly yes ports: - 6379:6379 volumes: - ./data/master:/data slave1: image: redis container_name: redis-slave-1 restart: always command: redis-server --slaveof master 6379 --port 6380 --requirepass 123456 --masterauth 123456 --appendonly yes ports: - 6380:6380 volumes: - ./data/slave1:/data depends_on: - master slave2: image: redis container_name: redis-slave-2 restart: always command: redis-server --slaveof master 6379 --port 6381 --requirepass 123456 --masterauth 123456 --appendonly yes ports: - 6381:6381 volumes: - ./data/slave2:/data depends_on: - slave1 - master sentinel1: image: redis container_name: redis-sentinel-1 command: redis-sentinel /usr/local/etc/redis/sentinel.conf restart: always ports: - 26379:26379 volumes: - ./sentinel1.conf:/usr/local/etc/redis/sentinel.conf depends_on: - slave2 sentinel2: image: redis container_name: redis-sentinel-2 command: redis-sentinel /usr/local/etc/redis/sentinel.conf restart: always ports: - 26380:26379 volumes: - ./sentinel2.conf:/usr/local/etc/redis/sentinel.conf depends_on: - slave2 sentinel3: image: redis container_name: redis-sentinel-3 command: redis-sentinel /usr/local/etc/redis/sentinel.conf restart: always ports: - 26381:26379 volumes: - ./sentinel3.conf:/usr/local/etc/redis/sentinel.conf depends_on: - slave2 再新建 sentinel.conf 配置文件,并拷贝为3个文件供 sentinel 节点使用。\nport 26379\rdir /tmp\rsentinel monitor mymaster 172.28.225.116 6379 2\rsentinel auth-pass mymaster 123456\rsentinel down-after-milliseconds mymaster 30000\rsentinel parallel-syncs mymaster 1\rsentinel failover-timeout mymaster 180000\rsentinel deny-scripts-reconfig yes 然后执行命令： docker-compose up -d\n测试： 杀掉 master 节点后，使用命令 docker logs redis-sentinel-1 查看日志：\nzsy@zsy:~/redis/testSentinel$ docker logs redis-sentinel-1 1:X 21 Oct 2021 09:24:04.292 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo 1:X 21 Oct 2021 09:24:04.292 # Redis version=6.2.6, bits=64, commit=00000000, modified=0, pid=1, just started 1:X 21 Oct 2021 09:24:04.292 # Configuration loaded 1:X 21 Oct 2021 09:24:04.293 * monotonic clock: POSIX clock_gettime 1:X 21 Oct 2021 09:24:04.293 * Running mode=sentinel, port=26379. 1:X 21 Oct 2021 09:24:04.299 # Could not rename tmp config file (Device or resource busy) 1:X 21 Oct 2021 09:24:04.299 # WARNING: Sentinel was not able to save the new configuration on disk!!!: Device or resource busy 1:X 21 Oct 2021 09:24:04.299 # Sentinel ID is 44a0f4c7217b06a20a66efd4987bb54228cf0fc1 1:X 21 Oct 2021 09:24:04.299 # +monitor master mymaster 172.28.225.116 6379 quorum 2 1:X 21 Oct 2021 09:24:04.304 * +slave slave 172.20.0.3:6380 172.20.0.3 6380 @ mymaster 172.28.225.116 6379 1:X 21 Oct 2021 09:24:04.306 # Could not rename tmp config file (Device or resource busy) 1:X 21 Oct 2021 09:24:04.306 # WARNING: Sentinel was not able to save the new configuration on disk!!!: Device or resource busy 1:X 21 Oct 2021 09:24:04.306 * +slave slave 172.20.0.4:6381 172.20.0.4 6381 @ mymaster 172.28.225.116 6379 1:X 21 Oct 2021 09:24:04.309 # Could not rename tmp config file (Device or resource busy) 1:X 21 Oct 2021 09:24:04.309 # WARNING: Sentinel was not able to save the new configuration on disk!!!: Device or resource busy 1:X 21 Oct 2021 09:24:05.668 * +sentinel sentinel e83e00583428b608fe07105eb458d69817175fb3 172.20.0.5 26379 @ mymaster 172.28.225.116 6379 1:X 21 Oct 2021 09:24:05.678 # Could not rename tmp config file (Device or resource busy) 1:X 21 Oct 2021 09:24:05.678 # WARNING: Sentinel was not able to save the new configuration on disk!!!: Device or resource busy 1:X 21 Oct 2021 09:24:06.079 * +sentinel sentinel 79bf36c262e20b0544b99bd0ac0ac43fe5164789 172.20.0.6 26379 @ mymaster 172.28.225.116 6379 1:X 21 Oct 2021 09:24:06.122 # Could not rename tmp config file (Device or resource busy) 1:X 21 Oct 2021 09:24:06.122 # WARNING: Sentinel was not able to save the new configuration on disk!!!: Device or resource busy 1:X 21 Oct 2021 09:25:19.047 # +sdown master mymaster 172.28.225.116 6379 1:X 21 Oct 2021 09:25:19.111 # Could not rename tmp config file (Device or resource busy) 1:X 21 Oct 2021 09:25:19.111 # WARNING: Sentinel was not able to save the new configuration on disk!!!: Device or resource busy 1:X 21 Oct 2021 09:25:19.111 # +new-epoch 1 1:X 21 Oct 2021 09:25:19.113 # Could not rename tmp config file (Device or resource busy) 1:X 21 Oct 2021 09:25:19.113 # WARNING: Sentinel was not able to save the new configuration on disk!!!: Device or resource busy 1:X 21 Oct 2021 09:25:19.113 # +vote-for-leader 79bf36c262e20b0544b99bd0ac0ac43fe5164789 1 1:X 21 Oct 2021 09:25:19.113 # +odown master mymaster 172.28.225.116 6379 #quorum 3/2 1:X 21 Oct 2021 09:25:19.113 # Next failover delay: I will not start a failover before Thu Oct 21 09:31:19 2021 1:X 21 Oct 2021 09:25:19.388 # +config-update-from sentinel 79bf36c262e20b0544b99bd0ac0ac43fe5164789 172.20.0.6 26379 @ mymaster 172.28.225.116 6379 1:X 21 Oct 2021 09:25:19.388 # +switch-master mymaster 172.28.225.116 6379 172.20.0.4 6381 1:X 21 Oct 2021 09:25:19.388 * +slave slave 172.20.0.3:6380 172.20.0.3 6380 @ mymaster 172.20.0.4 6381 1:X 21 Oct 2021 09:25:19.388 * +slave slave 172.28.225.116:6379 172.28.225.116 6379 @ mymaster 172.20.0.4 6381 1:X 21 Oct 2021 09:25:19.391 # Could not rename tmp config file (Device or resource busy) 1:X 21 Oct 2021 09:25:19.391 # WARNING: Sentinel was not able to save the new configuration on disk!!!: Device or resource busy 1:X 21 Oct 2021 09:25:49.476 # +sdown slave 172.28.225.116:6379 172.28.225.116 6379 @ mymaster 172.20.0.4 6381 参考： 使用docker 搭建redis的哨兵机制\n","date":1621555200,"description":"","dir":"post\\Redis\\","excerpt_html":null,"excerpt_text":null,"expirydate":-62135596800,"fuzzywordcount":1100,"html":null,"keywords":null,"kind":"page","lang":"en","lastmod":1621555200,"objectID":"493339fa7669b499f34a58017a000a3b","permalink":"http://localhost:1313/post/redis/redis%E5%AE%9E%E6%88%98----%E5%93%A8%E5%85%B5%E6%9C%BA%E5%88%B6/","publishdate":"2021-05-21T00:00:00Z","readingtime":3,"relpermalink":"/post/redis/redis%E5%AE%9E%E6%88%98----%E5%93%A8%E5%85%B5%E6%9C%BA%E5%88%B6/","section":"post","summary":"本文介绍使用docker操作Redis哨兵机制相关内容 还是使用 docker-compose 来测试 Redis 的哨兵机制。 这里建议使用一个 docker-compose 来测试，我一开始是使用的两个 yaml 文件，会","tags":["Redis"],"title":"Redis实战----哨兵机制","type":"post","url":"/post/redis/redis%E5%AE%9E%E6%88%98----%E5%93%A8%E5%85%B5%E6%9C%BA%E5%88%B6/","weight":0,"wordcount":1062},{"author":null,"categories":["Code"],"content":" 本文介绍Java内存模型相关知识\nJava内存模型和JVM内存结构是两个不同的概念。\nJava内存模型：\nJava 的并发采用的是共享内存模型，Java 线程之间的通信总是隐式进行，整个通信过程对程序员完全透明。如果编写多线程程序的 Java 程序员不理解隐式进行的线程之间通信的工作机制，很可能会遇到各种奇怪的内存可见性问题。\n为了解决可能由共享内存模型引发的内存可见性问题，抽象出了Java内存模型。\nJava 线程之间的通信由 Java 内存模型（本文简称为 JMM）控制，JMM 决定一个线程对共享变量的写入何时对另一个线程可见。从抽象的角度来看，JMM 定义了线程和主内存之间的抽象关系：线程之间的共享变量存储在主内存（main memory）中，每个线程都有一个私有的本地内存（local memory），本地内存中存储了该线程以读 / 写共享变量的副本。本地内存是 JMM 的一个抽象概念，并不真实存在。它涵盖了缓存，写缓冲区，寄存器以及其他的硬件和编译器优化。\nJMM 通过控制主内存与每个线程的本地内存之间的交互，来为 java 程序员提供内存可见性保证。\n重排序：\n在执行程序时为了提高性能，编译器和处理器常常会对指令做重排序。\n编译器优化的重排序。编译器在不改变单线程程序语义的前提下，可以重新安排语句的执行顺序。 指令级并行的重排序。现代处理器采用了指令级并行技术（Instruction-Level Parallelism， ILP）来将多条指令重叠执行。如果不存在数据依赖性，处理器可以改变语句对应机器指令的执行顺序。 内存系统的重排序。由于处理器使用缓存和读 / 写缓冲区，这使得加载和存储操作看上去可能是在乱序执行。 这些重排序都可能会导致多线程程序出现内存可见性问题。\n对于编译器，JMM 的编译器重排序规则会禁止特定类型的编译器重排序（不是所有的编译器重排序都要禁止）。对于处理器重排序，JMM 的处理器重排序规则会要求 java 编译器在生成指令序列时，插入特定类型的内存屏障（memory barriers，intel 称之为 memory fence）指令，通过内存屏障指令来禁止特定类型的处理器重排序（不是所有的处理器重排序都要禁止）。\n数据依赖：\n如果两个操作访问同一个变量，且这两个操作中有一个为写操作，此时这两个操作之间就存在数据依赖性。数据依赖分下列三种类型：\n写后读 写后写 读后写 上面三种情况，只要重排序两个操作的执行顺序，程序的执行结果将会被改变。 前面提到过，编译器和处理器可能会对操作做重排序。编译器和处理器在重排序时，会遵守数据依赖性，编译器和处理器不会改变存在数据依赖关系的两个操作的执行顺序。 注意，这里所说的数据依赖性仅针对单个处理器中执行的指令序列和单个线程中执行的操作，不同处理器之间和不同线程之间的数据依赖性不被编译器和处理器考虑。\nhappens-before:\n如果一个操作执行的结果需要对另一个操作可见，那么这两个操作之间必须存在 happens-before 关系。这里提到的两个操作既可以是在一个线程之内，也可以是在不同线程之间。 与程序员密切相关的 happens-before 规则如下：\n程序次序法则：线程中的每个动作A都happens-before于该线程中的每一个动作B 监视器锁规则：对一个监视器锁的解锁，happens- before 于随后对这个监视器锁的加锁。 volatile 变量规则：对一个 volatile 域的写，happens- before 于任意后续对这个 volatile 域的读。 线程启动法则：在一个线程里，对Thread.start的调用会happens-before于每个启动线程的动作。 线程终结法则：线程中的任何动作都happens-before于其他线程检测到这个线程已经终结、或者从Thread.join调用中成功返回，或Thread.isAlive返回false。 中断法则：一个线程调用另一个线程的interrupt happens-before于被中断的线程发现中断。 终结法则：一个对象的构造函数的结束happens-before于这个对象finalizer的开始。 传递性：如果 A happens- before B，且 B happens- before C，那么 A happens- before C。 一个 happens-before 规则通常对应于多个编译器重排序规则和处理器重排序规则。对于 java 程序员来说，happens-before 规则简单易懂，它避免程序员为了理解 JMM 提供的内存可见性保证而去学习复杂的重排序规则以及这些规则的具体实现。\n重排序对多线程的影响：\npublic class PossibleReordering { static int x = 0, y = 0; static int a = 0, b = 0; public static void main(String[] args) throws InterruptedException { Thread one = new Thread(new Runnable() { public void run() { a = 1; x = b; } }); Thread other = new Thread(new Runnable() { public void run() { b = 1; y = a; } }); one.start();other.start(); one.join();other.join(); System.out.println(“(” + x + “,” + y + “)”); } 很容易想到这段代码的运行结果可能为(1,0)、(0,1)或(1,1)，因为线程one可以在线程two开始之前就执行完了，也有可能反之，甚至有可能二者的指令是同时或交替执行的。\n然而，这段代码的执行结果也可能是(0,0). 因为，在实际运行时，代码指令可能并不是严格按照代码语句顺序执行的。得到(0,0)结果的语句执行过程，如下图所示。值得注意的是，a=1和x=b这两个语句的赋值操作的顺序被颠倒了，或者说，发生了指令“重排序”(reordering)。（事实上，输出了这一结果，并不代表一定发生了指令重排序，内存可见性问题也会导致这样的输出）\n内存屏障:\n内存屏障（Memory Barrier，或有时叫做内存栅栏，Memory Fence）是一种CPU指令，用于控制特定条件下的重排序和内存可见性问题。Java编译器也会根据内存屏障的规则禁止重排序。\n内存屏障可以被分为以下几种类型：\nLoadLoad屏障：对于这样的语句Load1; LoadLoad; Load2，在Load2及后续读取操作要读取的数据被访问前，保证Load1要读取的数据被读取完毕。 StoreStore屏障：对于这样的语句Store1; StoreStore; Store2，在Store2及后续写入操作执行前，保证Store1的写入操作对其它处理器可见。 LoadStore屏障：对于这样的语句Load1; LoadStore; Store2，在Store2及后续写入操作被刷出前，保证Load1要读取的数据被读取完毕。 StoreLoad屏障：对于这样的语句Store1; StoreLoad; Load2，在Load2及后续所有读取操作执行前，保证Store1的写入对所有处理器可见。它的开销是四种屏障中最大的。在大多数处理器的实现中，这个屏障是个万能屏障，兼具其它三种内存屏障的功能。 参考：\nJava内存模型 Java内存访问重排序的研究 ","date":1620950400,"description":"","dir":"post\\JVM\\","excerpt_html":null,"excerpt_text":null,"expirydate":-62135596800,"fuzzywordcount":2600,"html":null,"keywords":null,"kind":"page","lang":"en","lastmod":1620950400,"objectID":"158214856c654d7213e104306bdaa65f","permalink":"http://localhost:1313/post/jvm/java%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B/","publishdate":"2021-05-14T00:00:00Z","readingtime":6,"relpermalink":"/post/jvm/java%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B/","section":"post","summary":"本文介绍Java内存模型相关知识 Java内存模型和JVM内存结构是两个不同的概念。 Java内存模型： Java 的并发采用的是共享内存模型，Java 线","tags":["JVM"],"title":"Java内存模型","type":"post","url":"/post/jvm/java%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B/","weight":0,"wordcount":2582},{"author":null,"categories":["Code"],"content":" 本文介绍Redis缓存相关问题，包括缓存穿透、缓存击穿、缓存雪崩等相关知识\n缓存穿透： 缓存穿透是指缓存和数据库中都没有的数据，而用户不断发起请求，这将导致这个不存在的数据每次请求都要到存储层去查询，失去了缓存的意义。当流量过大时，数据库可能挂掉。\n解决方案：\n用户校验：接口层增加校验，如用户鉴权校验，id做基础校验，id\u0026lt;=0的直接拦截； 对空值缓存：从缓存取不到的数据，在数据库中也没有取到，这时也可以将key-value对写为key-null，缓存有效时间可以设置短点，如30秒（设置太长会导致正常情况也没法使用）。这样可以防止攻击用户反复用同一个id暴力攻击。 布隆过滤器：bloomfilter就类似于一个hash set，用于快速判某个元素是否存在于集合中，其典型的应用场景就是快速判断一个key是否存在于某容器，不存在就直接返回。 缓存击穿： 缓存击穿是指缓存中没有但数据库中有的数据（一般是热门缓存数据时间到期），这时由于大量并发请求，同时读缓存没读到数据，又同时去数据库去取数据，引起数据库压力瞬间增大，可能造成数据库宕机。\n解决方案：\n设置热点数据永远不过期。 接口限流与熔断，降级。重要的接口一定要做好限流策略，防止用户恶意刷接口，同时要降级准备，当接口中的某些 服务 不可用时候，进行熔断，失败快速返回机制。 加互斥锁。 缓存雪崩： 缓存雪崩是指缓存中数据大批量到过期时间，而查询数据量巨大，引起数据库压力过大甚至down机。和缓存击穿不同的是，缓存击穿指并发查同一条数据，缓存雪崩是不同数据都过期了，很多数据都查不到从而查数据库。\n解决方案：\n缓存数据的过期时间设置随机，防止同一时间大量数据过期现象发生。 如果缓存数据库是分布式部署，将热点数据均匀分布在不同的缓存数据库中。 设置热点数据永远不过期。 使用锁或者队列。 缓存污染： 缓存污染问题说的是缓存中一些只会被访问一次或者几次的的数据，被访问完后，再也不会被访问到，但这部分数据依然留存在缓存中，消耗缓存空间。 缓存污染会随着数据的持续增加而逐渐显露，随着服务的不断运行，缓存中会存在大量的永远不会再次被访问的数据。缓存空间是有限的，如果缓存空间满了，再往缓存里写数据时就会有额外开销，影响Redis性能。这部分额外开销主要是指写的时候判断淘汰策略，根据淘汰策略去选择要淘汰的数据，然后进行删除操作。\n缓存一致性问题： 读取缓存步骤一般没有什么问题，但是一旦涉及到数据更新：数据库和缓存更新，就容易出现缓存(Redis)和数据库（MySQL）间的数据一致性问题。\n不管是先写MySQL数据库，再删除Redis缓存；还是先删除缓存，再写库，都有可能出现数据不一致的情况。\n解决方案：\n读的时候，先读缓存，缓存没有的话，就读数据库，然后取出数据后放入缓存，同时返回响应。 更新的时候，先更新数据库，然后再删除缓存。 参考：\nRedis进阶 - 缓存问题：一致性, 穿击, 穿透, 雪崩, 污染等 ","date":1620691200,"description":"","dir":"post\\Redis\\","excerpt_html":null,"excerpt_text":null,"expirydate":-62135596800,"fuzzywordcount":1300,"html":null,"keywords":null,"kind":"page","lang":"en","lastmod":1620691200,"objectID":"4e12633510476324816a032506c8507b","permalink":"http://localhost:1313/post/redis/redis%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86----%E7%BC%93%E5%AD%98%E9%97%AE%E9%A2%98/","publishdate":"2021-05-11T00:00:00Z","readingtime":3,"relpermalink":"/post/redis/redis%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86----%E7%BC%93%E5%AD%98%E9%97%AE%E9%A2%98/","section":"post","summary":"本文介绍Redis缓存相关问题，包括缓存穿透、缓存击穿、缓存雪崩等相关知识 缓存穿透： 缓存穿透是指缓存和数据库中都没有的数据，而用户不断发起请","tags":["Redis"],"title":"Redis相关知识----缓存问题","type":"post","url":"/post/redis/redis%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86----%E7%BC%93%E5%AD%98%E9%97%AE%E9%A2%98/","weight":0,"wordcount":1202},{"author":null,"categories":["Life"],"content":"总想写一点什么，但是又感觉没什么好写的。\n写写一下最近的生活、工作、想法和规划吧。\nLife\n最近的生活也就工作日上班，节假日宅在宿舍。我也知道宅不太好，自己身体也没有以前好了，应该多去运动，跑跑步啥的。平时还是多做做运动吧，毕竟身体是自己的。前段时间，去广州找帆哥玩了几天，看了很多广州景点，也吃了很多好吃的。\n来深圳工作快一年了，也没有好好逛逛。一方面，自己对这座城市的感情没那么深吧，总觉得这是座“打工”的城市。也是第一次到另一座城市生活，独立生活的能力很差，离家的生活比较糟糕，可能是在家对我妈太依赖了吧。另一方面，目前的工资确实很低，这边消费很高，周末又不想一个人出去逛，自己出去的话也买不去很多东西。之前周末还去万科云城那边看看电影，但是最近开始和室友一起在网上看，体验也不差就没出去看电影了。\n最近好像长胖了，肚子一到春天就一堆肉。\n五一要回家看看爸妈，陪陪女朋友。但是只买到2号凌晨3点的票，就很难受。\nWork\n最近的工作，谈不上996，属于比较轻松的吧，但是一般晚上到宿舍也都8点多了。 其实我是只想写后端的，但是工作内容要求前后端都写，所以了解了下工作用的前端组件，然后也给这个博客加了一个搜索功能。 但是我的工资比较低，还需要努力学习技术。\n最近也记录了很多博文，大多是从网上其他博文搬过来的，但自己用心去阅读了这些博文，理解了很多知识点。\nIdea\n要努力啊，不然只能坐3点的高铁。\n勤奋一点，多学习技术。\n自信一点，要相信自己能做到。\nPlan\n想回武汉工作，估计年底就辞职回去吧。但是换工作，想要高工资得技术厉害才行。所以最近看的都是一些八股文类型的，理论很深的知识，然后写博客记录下来。但是很容易遗忘，而且口述出来很难，容易卡住。\n五一来了之后，学学微服务相关的技术。\n5/9 晚 回深圳了\n假期过得好快呀，一晃就过去了，又苦逼地回到深圳打工了。\n不想在这边打工了，无依无靠的，一个人在这边也交不到很多朋友，认清了自己是不喜欢孤独的一个人，还是喜欢热热闹闹的。\n要努力学习技术呀，多看点视频、博客，多花时间动手去学习，不管能不能达到自己想要的目标，开始就挺棒的。\n人生如戏啊 Life is a fucking movie\n2021已经快过了一半，加油吧。\n","date":1619740800,"description":"","dir":"post\\Shuyou\\","excerpt_html":null,"excerpt_text":null,"expirydate":-62135596800,"fuzzywordcount":900,"html":null,"keywords":null,"kind":"page","lang":"en","lastmod":1619740800,"objectID":"0044912a32f2be69fde416fa27080ce6","permalink":"http://localhost:1313/post/shuyou/%E6%9C%80%E8%BF%91%E4%B8%80%E4%BA%9B%E6%83%B3%E6%B3%95/","publishdate":"2021-04-30T00:00:00Z","readingtime":2,"relpermalink":"/post/shuyou/%E6%9C%80%E8%BF%91%E4%B8%80%E4%BA%9B%E6%83%B3%E6%B3%95/","section":"post","summary":"总想写一点什么，但是又感觉没什么好写的。 写写一下最近的生活、工作、想法和规划吧。 Life 最近的生活也就工作日上班，节假日宅在宿舍。我也知道宅不太好","tags":["记录"],"title":"最近一些想法","type":"post","url":"/post/shuyou/%E6%9C%80%E8%BF%91%E4%B8%80%E4%BA%9B%E6%83%B3%E6%B3%95/","weight":0,"wordcount":885},{"author":null,"categories":["Code"],"content":" 本文介绍Redis哨兵相关知识\n哨兵机制（Redis Sentinel）： Redis Sentinel，即Redis哨兵，在Redis 2.8版本开始引入。哨兵的核心功能是主节点的自动故障转移。其他功能：\n监控（Monitoring）：哨兵会不断地检查主节点和从节点是否运作正常。 自动故障转移（Automatic failover）：当主节点不能正常工作时，哨兵会开始自动故障转移操作，它会将失效主节点的其中一个从节点升级为新的主节点，并让其他从节点改为复制新的主节点。 配置提供者（Configuration provider）：客户端在初始化时，通过连接哨兵来获得当前Redis服务的主节点地址。 通知（Notification）：哨兵可以将故障转移的结果发送给客户端。 其中，监控和自动故障转移功能，使得哨兵可以及时发现主节点故障并完成转移；而配置提供者和通知功能，则需要在与客户端的交互中才能体现。\n哨兵集群：哨兵实例之间可以相互发现，要归功于 Redis 提供的 pub/sub 机制，也就是发布 / 订阅机制。\n在主从集群中，主库上有一个名为__sentinel__:hello的频道，不同哨兵就是通过它来相互发现，实现互相通信的。在下图中，哨兵 1 把自己的 IP（172.16.19.3）和端口（26579）发布到__sentinel__:hello频道上，哨兵 2 和 3 订阅了该频道。那么此时，哨兵 2 和 3 就可以从这个频道直接获取哨兵 1 的 IP 地址和端口号。然后，哨兵 2、3 可以和哨兵 1 建立网络连接。\n哨兵监控Redis库\n每个哨兵节点每10秒会向主节点和从节点发送info命令获取最拓扑结构图，哨兵配置时只要配置对主节点的监控即可，通过向主节点发送info，获取从节点的信息，并当有新的从节点加入时可以马上感知到。 每个哨兵节点每隔2秒会向redis数据节点的指定频道上发送该哨兵节点对于主节点的判断以及当前哨兵节点的信息，同时每个哨兵节点也会订阅该频道，来了解其它哨兵节点的信息及对主节点的判断，其实就是通过消息publish和subscribe来完成的。 每隔1秒每个哨兵会向主节点、从节点及其余哨兵节点发送一次ping命令做一次心跳检测，这个也是哨兵用来判断节点是否正常的重要依据 主库下线的判定：\n主观下线：任何一个哨兵都是可以监控探测，并作出Redis节点下线的判断； 客观下线：有哨兵集群共同决定Redis节点是否下线； 当某个哨兵（如下图中的哨兵2）判断主库“主观下线”后，就会给其他哨兵发送 is-master-down-by-addr 命令。接着，其他哨兵会根据自己和主库的连接情况，做出 Y 或 N 的响应，Y 相当于赞成票，N 相当于反对票。\n如果赞成票数（这里是2）是大于等于哨兵配置文件中的 quorum 配置项（比如这里如果是quorum=2）, 则可以判定主库客观下线了。\n领导者哨兵选举流程：\n)每个在线的哨兵节点都可以成为领导者，当它确认（比如哨兵3）主节点下线时，会向其它哨兵发is-master-down-by-addr命令，征求判断并要求将自己设置为领导者，由领导者处理故障转移。 当其它哨兵收到此命令时，可以同意或者拒绝它成为领导者。 哨兵拿到半数以上的赞成票，且拿到的票数同时还需要大于等于哨兵配置文件中的 quorum 值。 哨兵的选举机制是Raft选举算法（一种分布式选举算法）： 选举的票数大于等于num(sentinels)/2+1时，将成为领导者，如果没有超过，继续选举。\n选出新主库：\n过滤掉不健康的（下线或断线），没有回复过哨兵ping响应的从节点 选择salve-priority从节点优先级最高（redis.conf）的 选择复制偏移量最大，指复制最完整的从节点 故障转移机制：\n将新选出的主库脱离原从节点，升级为主节点 将其他从节点指向新的主节点 通知客户端主节点已更换 将原主节点（oldMaster）变成从节点，指向新的主节点 总结： 关于哨兵的原理，关键是了解以下几个概念。\n（1）定时任务：每个哨兵节点维护了3个定时任务。定时任务的功能分别如下：\n通过向主从节点发送info命令获取最新的主从结构 通过发布订阅功能获取其他哨兵节点的信息 通过向其他节点发送ping命令进行心跳检测，判断是否下线 （2）主观下线：在心跳检测的定时任务中，如果其他节点超过一定时间没有回复，哨兵节点就会将其进行主观下线。顾名思义，主观下线的意思是一个哨兵节点“主观地”判断下线；与主观下线相对应的是客观下线。\n（3）客观下线：哨兵节点在对主节点进行主观下线后，会通过sentinel is-master-down-by-addr命令询问其他哨兵节点该主节点的状态；如果判断主节点下线的哨兵数量达到一定数值，则对该主节点进行客观下线。\n需要特别注意的是，客观下线是主节点才有的概念；如果从节点和哨兵节点发生故障，被哨兵主观下线后，不会再有后续的客观下线和故障转移操作。\n（4）选举领导者哨兵节点：当主节点被判断客观下线以后，各个哨兵节点会进行协商，选举出一个领导者哨兵节点，并由该领导者节点对其进行故障转移操作。\n监视该主节点的所有哨兵都有可能被选为领导者，选举使用的算法是Raft算法；Raft算法的基本思路是先到先得：即在一轮选举中，哨兵A向B发送成为领导者的申请，如果B没有同意过其他哨兵，则会同意A成为领导者。选举的具体过程这里不做详细描述，一般来说，哨兵选择的过程很快，谁先完成客观下线，一般就能成为领导者。\n（5）故障转移：选举出的领导者哨兵，开始进行故障转移操作，该操作大体可以分为3个步骤：\n在从节点中选择新的主节点：选择的原则是，首先过滤掉不健康的从节点；然后选择优先级最高的从节点(由slave-priority指定)；如果优先级无法区分，则选择复制偏移量最大的从节点；如果仍无法区分，则选择runid最小的从节点。 更新主从状态：通过slaveof no one命令，让选出来的从节点成为主节点；并通过slaveof命令让其他节点成为其从节点。 将已经下线的主节点(即6379)设置为新的主节点的从节点，当6379重新上线后，它会成为新的主节点的从节点。 通过上述几个关键概念，可以基本了解哨兵的工作原理。为了更形象的说明，下图展示了领导者哨兵节点的日志，包括从节点启动到完成故障转移。\n建议： （1）哨兵节点的数量应不止一个，一方面增加哨兵节点的冗余，避免哨兵本身成为高可用的瓶颈；另一方面减少对下线的误判。此外，这些不同的哨兵节点应部署在不同的物理机上。\n（2）哨兵节点的数量应该是奇数，便于哨兵通过投票做出“决策”：领导者选举的决策、客观下线的决策等。\n（3）各个哨兵节点的配置应一致，包括硬件、参数等；此外，所有节点都应该使用ntp或类似服务，保证时间准确、一致。\n（4）哨兵的配置提供者和通知客户端功能，需要客户端的支持才能实现，如Jedis；如果开发者使用的库未提供相应支持，则可能需要开发者自己实现。\n（5）当哨兵系统中的节点在docker（或其他可能进行端口映射的软件）中部署时，应特别注意端口映射可能会导致哨兵系统无法正常工作，因为哨兵的工作基于与其他节点的通信，而docker的端口映射可能导致哨兵无法连接到其他节点。例如，哨兵之间互相发现，依赖于它们对外宣称的IP和port，如果某个哨兵A部署在做了端口映射的docker中，那么其他哨兵使用A宣称的port无法连接到A。\n参考：\nRedis进阶 - 高可用：哨兵机制（Redis Sentinel）详解 redis之 主从复制和哨兵 实践：\n深入学习Redis（4）：哨兵 ","date":1619568000,"description":"","dir":"post\\Redis\\","excerpt_html":null,"excerpt_text":null,"expirydate":-62135596800,"fuzzywordcount":3100,"html":null,"keywords":null,"kind":"page","lang":"en","lastmod":1619568000,"objectID":"a23408a5d0250119d3d081b026cc8d99","permalink":"http://localhost:1313/post/redis/redis%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86----%E5%93%A8%E5%85%B5%E6%9C%BA%E5%88%B6/","publishdate":"2021-04-28T00:00:00Z","readingtime":7,"relpermalink":"/post/redis/redis%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86----%E5%93%A8%E5%85%B5%E6%9C%BA%E5%88%B6/","section":"post","summary":"本文介绍Redis哨兵相关知识 哨兵机制（Redis Sentinel）： Redis Sentinel，即Redis哨兵，在Redis 2.8版本开始引入。","tags":["Redis"],"title":"Redis相关知识----哨兵机制","type":"post","url":"/post/redis/redis%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86----%E5%93%A8%E5%85%B5%E6%9C%BA%E5%88%B6/","weight":0,"wordcount":3026},{"author":null,"categories":["Code"],"content":" 本文介绍Redis主从复制相关知识\n主从复制：是指将一台Redis服务器的数据，复制到其他的Redis服务器。前者称为主节点(master)，后者称为从节点(slave)；数据的复制是单向的，只能由主节点到从节点。\n作用：\n数据冗余：主从复制实现了数据的热备份，是持久化之外的一种数据冗余方式。 故障恢复：当主节点出现问题时，可以由从节点提供服务，实现快速的故障恢复；实际上是一种服务的冗余。 负载均衡：在主从复制的基础上，配合读写分离，可以由主节点提供写服务，由从节点提供读服务（即写Redis数据时应用连接主节点，读Redis数据时应用连接从节点），分担服务器负载；尤其是在写少读多的场景下，通过多个从节点分担读负载，可以大大提高Redis服务器的并发量。 高可用基石：除了上述作用以外，主从复制还是哨兵和集群能够实施的基础，因此说主从复制是Redis高可用的基础。 主从复制采用的是读写分离的方式\n读操作：主库和从库都可以。 写操作：首先到主库执行，然后，主库将写操作同步给从库。 主从复制原理：\n全量复制：比如第一次同步时 增量复制：只会把主从库网络断连期间主库收到的命令，同步给从库 全量复制： 当我们启动多个 Redis 实例的时候，它们相互之间就可以通过 replicaof（Redis 5.0 之前使用 slaveof）命令形成主库和从库的关系，之后会按照三个阶段完成数据的第一次同步。\n使用replicaof命令确定主从关系：\n#在 172.16.19.7 端使用 replicaof命令 之后172.16.19.7变成 172.16.19.9的从库 replicaof 172.16.19.9 6379 全量复制的三个阶段： 第一阶段是主从库间建立连接、协商同步的过程，主要是为全量复制做准备。 在这一步，从库和主库建立起连接，并告诉主库即将进行同步，主库确认回复后，主从库间就可以开始同步了。 具体来说，从库给主库发送 psync 命令，表示要进行数据同步，主库根据这个命令的参数来启动复制。psync 命令包含了主库的 runID 和复制进度 offset 两个参数。runID，是每个 Redis 实例启动时都会自动生成的一个随机 ID，用来唯一标记这个实例。当从库和主库第一次复制时，因为不知道主库的 runID，所以将 runID 设为“？”。offset，此时设为 -1，表示第一次复制。 主库收到 psync 命令后，会用 FULLRESYNC 响应命令带上两个参数：主库 runID 和主库目前的复制进度 offset，返回给从库。从库收到响应后，会记录下这两个参数。这里有个地方需要注意，FULLRESYNC 响应表示第一次复制采用的全量复制，也就是说，主库会把当前所有的数据都复制给从库。 第二阶段，主库将所有数据同步给从库。 从库收到数据后，在本地完成数据加载。这个过程依赖于内存快照生成的 RDB 文件。 具体来说，主库执行 bgsave 命令，生成 RDB 文件，接着将文件发给从库。从库接收到 RDB 文件后，会先清空当前数据库，然后加载 RDB 文件。这是因为从库在通过 replicaof 命令开始和主库同步前，可能保存了其他数据。为了避免之前数据的影响，从库需要先把当前数据库清空。在主库将数据同步给从库的过程中，主库不会被阻塞，仍然可以正常接收请求。否则，Redis 的服务就被中断了。但是，这些请求中的写操作并没有记录到刚刚生成的 RDB 文件中。为了保证主从库的数据一致性，主库会在内存中用专门的 replication buffer，记录 RDB 文件生成后收到的所有写操作。 第三个阶段，主库会把第二阶段执行过程中新收到的写命令，再发送给从库。 具体的操作是，当主库完成 RDB 文件发送后，就会把此时 replication buffer 中的修改操作发给从库，从库再重新执行这些操作。这样一来，主从库就实现同步了。 增量复制： 如果主从库在命令传播时出现了网络闪断，那么，从库就会和主库重新进行一次全量复制，开销非常大。从 Redis 2.8 开始，网络断了之后，主从库会采用增量复制的方式继续同步。\n增量复制的流程： repl_backlog_buffer：它是为了从库断开之后，如何找到主从差异数据而设计的环形缓冲区，从而避免全量复制带来的性能开销。如果从库断开时间太久，repl_backlog_buffer环形缓冲区被主库的写命令覆盖了，那么从库连上主库后只能乖乖地进行一次全量复制，所以repl_backlog_buffer配置尽量大一些，可以降低主从断开后全量复制的概率。而在repl_backlog_buffer中找主从差异的数据后，如何发给从库呢？这就用到了replication buffer。 replication buffer：Redis和客户端通信也好，和从库通信也好，Redis都需要给分配一个内存buffer进行数据交互，客户端是一个client，从库也是一个client，我们每个client连上Redis后，Redis都会分配一个client buffer，所有数据交互都是通过这个buffer进行的：Redis先把数据写到这个buffer中，然后再把buffer中的数据发到client socket中再通过网络发送出去，这样就完成了数据交互。所以主从在增量同步时，从库作为一个client，也会分配一个buffer，只不过这个buffer专门用来传播用户的写命令到从库，保证主从数据一致，我们通常把它叫做replication buffer。 为什么主从全量复制使用RDB而不使用AOF？\nDB文件内容是经过压缩的二进制数据（不同数据类型数据做了针对性优化），文件很小。而AOF文件记录的是每一次写操作的命令，写操作越多文件会变得很大，其中还包括很多对同一个key的多次冗余操作。 RDB文件存储的都是二进制数据，从库直接按照RDB协议解析还原数据即可，速度会非常快，而AOF需要依次重放每个写命令，这个过程会经历冗长的处理逻辑，恢复速度相比RDB会慢得多，所以使用RDB进行主从全量复制的成本最低。 主 - 从 - 从： 如果从库数量很多，而且都要和主库进行全量复制的话，就会导致主库忙于 fork 子进程生成 RDB 文件，进行数据全量复制。fork 这个操作会阻塞主线程处理正常请求，从而导致主库响应应用程序的请求速度变慢。此外，传输 RDB 文件也会占用主库的网络带宽，同样会给主库的资源使用带来压力。\n我们可以通过“主 - 从 - 从”模式将主库生成 RDB 和传输 RDB 的压力，以级联的方式分散到从库上。\n这样一来，这些从库就会知道，在进行同步时，不用再和主库进行交互了，只要和级联的从库进行写操作同步就行了，这就可以减轻主库上的压力，如下图所示： 读写分离及其中的问题：\n延迟与不一致问题 数据过期问题 故障切换问题 参考：\nRedis进阶 - 高可用：主从复制详解 深入学习Redis（3）：主从复制 ","date":1619481600,"description":"","dir":"post\\Redis\\","excerpt_html":null,"excerpt_text":null,"expirydate":-62135596800,"fuzzywordcount":2700,"html":null,"keywords":null,"kind":"page","lang":"en","lastmod":1619481600,"objectID":"c79df0a883771edd48d26df4e2d9067f","permalink":"http://localhost:1313/post/redis/redis%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86----%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6/","publishdate":"2021-04-27T00:00:00Z","readingtime":6,"relpermalink":"/post/redis/redis%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86----%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6/","section":"post","summary":"本文介绍Redis主从复制相关知识 主从复制：是指将一台Redis服务器的数据，复制到其他的Redis服务器。前者称为主节点(master)，","tags":["Redis"],"title":"Redis相关知识----主从复制","type":"post","url":"/post/redis/redis%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86----%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6/","weight":0,"wordcount":2625},{"author":null,"categories":["Code"],"content":" 本文介绍Redis的持久化相关知识\n简介 Redis是基于内存的数据库，服务器一旦宕机，内存中的数据将全部丢失。\n通常的解决方案是从后端数据库恢复这些数据，但后端数据库有性能瓶颈，如果是大数据量的恢复，1、会对数据库带来巨大的压力，2、数据库的性能不如Redis。导致程序响应慢。\n所以对Redis来说，实现数据的持久化，避免从后端数据库中恢复数据，是至关重要的。\nRedis提供RDB和AOF持久化解决方案。\nRDB持久化 RDB：Redis DataBase，中文叫快照（内存快照）。RDB持久化就是进程中的数据保存到磁盘上的过程（生成rdb文件），由于是某一时刻的快照，那么快照中的值要早于或者等于内存中的值。\n手动触发RDB持久化：\nsave命令：阻塞当前Redis服务器，直到RDB过程完成为止，对于内存 比较大的实例会造成长时间阻塞，线上环境不建议使用。 bgsave命令：Redis进程执行fork操作创建子进程，RDB持久化过程由子 进程负责，完成后自动结束。阻塞只发生在fork阶段，一般时间很短。 自动触发RDB持久化：\nredis.conf中配置save m n，即在m秒内有n次修改时，自动触发bgsave生成rdb文件； 主从复制时，从节点要从主节点进行全量复制时也会触发bgsave操作，生成当时的快照发送到从节点； 执行debug reload命令重新加载redis时也会触发bgsave操作； 默认情况下执行shutdown命令时，如果没有开启aof持久化，那么也会触发bgsave操作； redis.conf中RDB相关配置：\n# 周期性执行条件的设置格式为 save \u0026lt;seconds\u0026gt; \u0026lt;changes\u0026gt; # 默认的设置为： save 900 1 save 300 10 save 60 10000 # 以下设置方式为关闭RDB快照功能 save \u0026#34;\u0026#34; # 文件名称 dbfilename dump.rdb # 文件保存路径 dir /home/work/app/redis/data/ # 如果持久化出错，主进程是否停止写入 stop-writes-on-bgsave-error yes # 是否压缩 rdbcompression yes # 导入时是否检查 rdbchecksum yes 主线程在保证写操作的情况下，RDB如何保证数据一致性？\nRDB中的核心思路是Copy-on-Write，来保证在进行快照操作的这段时间，需要压缩写入磁盘上的数据在内存中不会发生变化。\n在正常的快照操作中，一方面Redis主进程会fork一个新的快照进程专门来做这个事情，这样保证了Redis服务不会停止对客户端包括写请求在内的任何响应。 另一方面这段时间发生的数据变化会以副本的方式存放在另一个新的内存区域，待快照操作结束后才会同步到原来的内存区域。 进行RDB持久化过程中，服务器宕机怎么办？ 如果出现服务器崩溃的情况，会以上一次完整的RDB快照文件作为恢复内存数据的参考。也就是说，在快照操作过程中不能影响上一次的备份数据。\nRedis服务会在磁盘上创建一个临时文件进行数据操作，待操作成功后才会用这个临时文件替换掉上一次的备份。\n可以每秒做一次快照吗？\n不行。\n虽然越短时间做一次快照，可以保证当服务器宕机时数据丢失越少。但是如果频繁地执行全量快照，也会带来其他开销。\n频繁将全量数据写入磁盘，会给磁盘带来很大压力，多个快照竞争有限的磁盘带宽，前一个快照还没有做完，后一个又开始做了，容易造成恶性循环。 bgsave 子进程需要通过 fork 操作从主线程创建出来。虽然，子进程在创建后不会再阻塞主线程，但是，fork 这个创建过程本身会阻塞主线程，而且主线程的内存越大，阻塞时间越长。如果频繁 fork 出 bgsave 子进程，这就会频繁阻塞主线程了。 RDB优缺点：\n优点：\nRDB文件是某个时间节点的快照，默认使用LZF算法进行压缩，压缩后的文件体积远远小于内存大小，适用于备份、全量复制等场景； Redis加载RDB文件恢复数据要远远快于AOF方式； 缺点：\nRDB方式实时性不够，无法做到秒级的持久化； 每次调用bgsave都需要fork子进程，fork子进程属于重量级操作，频繁执行成本较高； RDB文件是二进制的，没有可读性，AOF文件在了解其结构的情况下可以手动修改或者补全； AOF持久化 针对RDB不适合实时持久化的问题，Redis提供了AOF持久化方式来解决。\n与Mysql不同（大多数的数据库采用的是写前日志（WAL），例如MySQL，通过写前日志和两阶段提交，实现数据和逻辑的一致性。），Redis是“写后”日志，Redis先执行命令，把数据写入内存，然后才记录日志。日志里记录的是Redis收到的每一条命令，这些命令是以文本形式保存。\n如何实现AOF AOF日志记录Redis的每个写命令，步骤分为：命令追加（append）、文件写入（write）和文件同步（sync）。\n命令追加 当AOF持久化功能打开了，服务器在执行完一个写命令之后，会以协议格式将被执行的写命令追加到服务器的 aof_buf 缓冲区。 文件写入和同步 关于何时将 aof_buf 缓冲区的内容写入AOF文件中，Redis提供了三种写回策略： redis.conf中配置AOF\n#默认情况下，Redis是没有开启AOF的，可以通过配置redis.conf文件来开启AOF持久化，关于AOF的配置如下： # appendonly参数开启AOF持久化 appendonly no # AOF持久化的文件名，默认是appendonly.aof appendfilename \u0026#34;appendonly.aof\u0026#34; # AOF文件的保存位置和RDB文件的位置相同，都是通过dir参数设置的 dir ./ # 同步策略 # appendfsync always appendfsync everysec # appendfsync no # aof重写期间是否同步 no-appendfsync-on-rewrite no # 重写触发配置 auto-aof-rewrite-percentage 100 auto-aof-rewrite-min-size 64mb # 加载aof出错如何处理 aof-load-truncated yes # 文件重写策略 aof-rewrite-incremental-fsync yes AOF重写机制\nAOF会记录每个写命令到AOF文件，随着时间越来越长，AOF文件会变得越来越大。如果不加以控制，会对Redis服务器，甚至对操作系统造成影响，而且AOF文件越大，数据恢复也越慢。\n为了解决AOF文件体积膨胀的问题，Redis提供AOF文件重写机制来对AOF文件进行“瘦身”。\nAOF重写 Redis通过创建一个新的AOF文件来替换现有的AOF，新旧两个AOF文件保存的数据相同，但新AOF文件没有了冗余命令。 AOF重写会阻塞吗？ fork子进程的时候会阻塞，其他时候不会。\nAOF重写过程是由后台进程bgrewriteaof来完成的。主线程fork出后台的bgrewriteaof子进程，fork会把主线程的内存拷贝一份给bgrewriteaof子进程，这里面就包含了数据库的最新数据。然后，bgrewriteaof子进程就可以在不影响主线程的情况下，逐一把拷贝的数据写成操作，记入重写日志。\nAOF日志何时会重写？\nauto-aof-rewrite-min-size 表示运行AOF重写时文件的最小大小，默认为64MB。 auto-aof-rewrite-percentage 表示如果当前AOF文件的大小超过了上次重写后AOF文件的百分之多少后，就再次开始重写AOF文件。 重写日志时，有新数据写入，怎么保证一致性？\n重写过程总结为：“一个拷贝，两处日志”。\n在重写时，如果有新数据写入，主线程就会将命令记录到两个aof日志内存缓冲区中。\n如果AOF写回策略配置的是always，则直接将命令写回旧的日志文件，并且保存一份命令至AOF重写缓冲区，这些操作对新的日志文件是不存在影响的。（旧的日志文件：主线程使用的日志文件，新的日志文件：bgrewriteaof进程使用的日志文件）\n而在bgrewriteaof子进程完成会日志文件的重写操作后，会提示主线程已经完成重写操作，主线程会将AOF重写缓冲中的命令追加到新的日志文件后面。\n这时候在高并发的情况下，AOF重写缓冲区积累可能会很大，这样就会造成阻塞，Redis后来通过Linux管道技术让aof重写期间就能同时进行回放，这样aof重写结束后只需回放少量剩余的数据即可。\n最后通过修改文件名的方式，保证文件切换的原子性。\n在AOF重写日志期间发生宕机的话，因为日志文件还没切换，所以恢复数据时，用的还是旧的日志文件。\n总结：\n主线程fork出子进程重写aof日志 子进程重写日志完成后，主线程追加aof日志缓冲 替换日志文件 RDB和AOF混合使用 Redis 4.0 中提出了一个混合使用 AOF 日志和内存快照的方法。简单来说，内存快照以一定的频率执行，在两次快照之间，使用 AOF 日志记录这期间的所有命令操作。\n这样一来，快照不用很频繁地执行，这就避免了频繁 fork 对主线程的影响。\n混合持久化同样也是通过bgrewriteaof完成的，不同的是当开启混合持久化时，fork出的子进程先将共享的内存副本全量的以RDB方式写入aof文件，然后在将重写缓冲区的增量命令以AOF方式写入到文件，写入完成后通知主进程更新统计信息，并将新的含有RDB格式和AOF格式的AOF文件替换旧的的AOF文件。\n这个方法既能享受到 RDB 文件快速恢复的好处，又能享受到 AOF 只记录操作命令的简单优势, 实际环境中用的很多。\n从持久化文件中恢复数据 redis重启时判断是否开启aof，如果开启了aof，那么就优先加载aof文件； 如果aof存在，那么就去加载aof文件，加载成功的话redis重启成功，如果aof文件加载失败，那么会打印日志表示启动失败，此时可以去修复aof文件后重新启动； 若aof文件不存在，那么redis就会转而去加载rdb文件，如果rdb文件不存在，redis直接启动成功； 如果rdb文件存在就会去加载rdb文件恢复数据，如加载失败则打印日志提示启动失败，如加载成功，那么redis重启成功，且使用rdb文件恢复数据； 因为AOF保存的数据更完整，所以会优先加载AOF。\n参考：\nRedis进阶 - 持久化：RDB和AOF机制详解 Redis数据持久化之RDB-AOF混合方式 ","date":1619136000,"description":"","dir":"post\\Redis\\","excerpt_html":null,"excerpt_text":null,"expirydate":-62135596800,"fuzzywordcount":3800,"html":null,"keywords":null,"kind":"page","lang":"en","lastmod":1619136000,"objectID":"dd0505253f5d8528b7ae7d2bde00d02e","permalink":"http://localhost:1313/post/redis/redis%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86----%E6%8C%81%E4%B9%85%E5%8C%96/","publishdate":"2021-04-23T00:00:00Z","readingtime":8,"relpermalink":"/post/redis/redis%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86----%E6%8C%81%E4%B9%85%E5%8C%96/","section":"post","summary":"本文介绍Redis的持久化相关知识 简介 Redis是基于内存的数据库，服务器一旦宕机，内存中的数据将全部丢失。 通常的解决方案是从后端数据库恢复","tags":["Redis"],"title":"Redis相关知识----持久化（RDB和AOF）","type":"post","url":"/post/redis/redis%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86----%E6%8C%81%E4%B9%85%E5%8C%96/","weight":0,"wordcount":3768},{"author":null,"categories":["Code"],"content":" 本文介绍下给hux主题添加搜索功能，主要是抄 黄玄 的作业，可以去参考huxpro\n使用插件：hexo-generator-search\nnpm install hexo-generator-search --save 配置 _config.yml：\nsearch: path: index.json field: post limit: 50 enable: true 增加模板 search.ejs:\n\u0026lt;div class=\u0026#34;search-page\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;search-icon-close-container\u0026#34;\u0026gt; \u0026lt;span class=\u0026#34;search-icon-close\u0026#34;\u0026gt; \u0026lt;i class=\u0026#34;fa fa-chevron-down\u0026#34;\u0026gt;\u0026lt;/i\u0026gt; \u0026lt;/span\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;search-main container\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;row\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1\u0026#34;\u0026gt; \u0026lt;form\u0026gt;\u0026lt;/form\u0026gt; \u0026lt;input type=\u0026#34;text\u0026#34; id=\u0026#34;search-input\u0026#34; placeholder=\u0026#34;search: \u0026#34;\u0026gt; \u0026lt;div id=\u0026#34;search-results\u0026#34; class=\u0026#34;mini-post-list\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;script\u0026gt; $(document).ready(function () { var $searchPage = $(\u0026#39;.search-page\u0026#39;); var $searchOpen = $(\u0026#39;.search-icon\u0026#39;); var $searchClose = $(\u0026#39;.search-icon-close\u0026#39;); var $searchInput = $(\u0026#39;#search-input\u0026#39;); var $body = $(\u0026#39;body\u0026#39;); $searchOpen.on(\u0026#39;click\u0026#39;, function (e) { e.preventDefault(); $searchPage.toggleClass(\u0026#39;search-active\u0026#39;); var prevClasses = $body.attr(\u0026#39;class\u0026#39;) || \u0026#39;\u0026#39;; setTimeout(function () { $body.addClass(\u0026#39;no-scroll\u0026#39;); }, 400) if ($searchPage.hasClass(\u0026#39;search-active\u0026#39;)) { $searchClose.on(\u0026#39;click\u0026#39;, function (e) { e.preventDefault(); $searchPage.removeClass(\u0026#39;search-active\u0026#39;); $body.attr(\u0026#39;class\u0026#39;, prevClasses); // from closure }); $searchInput.focus(); } searchFunc(\u0026#39;/index.json\u0026#39;, \u0026#39;search-input\u0026#39;, \u0026#39;search-results\u0026#39;); }); }); \u0026lt;/script\u0026gt; 把search.ejs导入到 layout.ejs中:\n\u0026lt;!-- Search --\u0026gt; \u0026lt;%- partial(\u0026#39;_partial/search\u0026#39;)%\u0026gt; 新增search.js:\n定义 searchFunc 函数\nvar searchFunc = function (path, search_id, content_id) { console.log(\u0026#34;test\u0026#34;); $.ajax({ url: path, dataType: \u0026#34;json\u0026#34;, success: function (datas) { var $input = document.getElementById(search_id); var $resultContent = document.getElementById(content_id); $input.addEventListener(\u0026#39;input\u0026#39;, function () { var keywords = this.value.trim().toLowerCase().split(/[\\s\\-]+/); var str = \u0026#34;\u0026#34;; $resultContent.innerHTML = \u0026#34;\u0026#34;; if (this.value.trim().length \u0026lt;= 0) { return; } datas.forEach(function (data) { var isMatch = true; var content_index = []; var data_title = data.title.trim().toLowerCase(); var data_content = data.content.trim().replace(/\u0026lt;[^\u0026gt;]+\u0026gt;/g, \u0026#34;\u0026#34;).toLowerCase(); var data_url = data.url; var index_title = -1; var index_content = -1; var first_occur = -1; // only match artiles with not empty titles and contents if (data_title != \u0026#39;\u0026#39; \u0026amp;\u0026amp; data_content != \u0026#39;\u0026#39;) { keywords.forEach(function (keyword, i) { index_title = data_title.indexOf(keyword); index_content = data_content.indexOf(keyword); if (index_title \u0026lt; 0 \u0026amp;\u0026amp; index_content \u0026lt; 0) { isMatch = false; } else { if (index_content \u0026lt; 0) { index_content = 0; } if (i == 0) { first_occur = index_content; } } }); } // show search results if (isMatch) { str += \u0026#34;\u0026lt;div class=\u0026#39;post-preview item\u0026#39;\u0026gt;\u0026lt;a href=\u0026#39;\u0026#34; + data_url +\u0026#34;\u0026#39;\u0026gt;\u0026#34; +\u0026#34;\u0026lt;h2 class=\u0026#39;post-title\u0026#39;\u0026gt;\u0026#34; + data_title +\u0026#34;\u0026lt;/h2\u0026gt;\u0026#34; + \u0026#34;\u0026lt;/a\u0026gt;\u0026#34;; var content = data.content.trim().replace(/\u0026lt;[^\u0026gt;]+\u0026gt;/g, \u0026#34;\u0026#34;); if (first_occur \u0026gt;= 0) { // cut out 40 characters var start = first_occur - 20; var end = first_occur + 20; if (start \u0026lt; 0) { start = 0; } if (start == 0) { end = 40; } if (end \u0026gt; content.length) { end = content.length; } var match_content = content.substring(start, end); // highlight all keywords keywords.forEach(function (keyword) { var regS = new RegExp(keyword, \u0026#34;gi\u0026#34;); match_content = match_content.replace(regS, \u0026#34;\u0026lt;em class=\\\u0026#34;search-keyword\\\u0026#34;\u0026gt;\u0026#34; + keyword + \u0026#34;\u0026lt;/em\u0026gt;\u0026#34;); }); str += \u0026#34;\u0026lt;p class=\\\u0026#34;search-result\\\u0026#34;\u0026gt;\u0026#34; + match_content + \u0026#34;...\u0026lt;/p\u0026gt;\u0026lt;hr\u0026gt;\u0026#34; } str+=\u0026#34;\u0026lt;/div\u0026gt;\u0026#34; } }); $resultContent.innerHTML = str; }); } }); } 增加样式：\n抄huxpro的作业：hux\n","date":1619049600,"description":"","dir":"post\\Blog\\","excerpt_html":null,"excerpt_text":null,"expirydate":-62135596800,"fuzzywordcount":600,"html":null,"keywords":null,"kind":"page","lang":"en","lastmod":1619049600,"objectID":"93d3010219f873a6def4ab37113e7753","permalink":"http://localhost:1313/post/blog/hux%E4%B8%BB%E9%A2%98%E6%B7%BB%E5%8A%A0%E6%90%9C%E7%B4%A2/","publishdate":"2021-04-22T00:00:00Z","readingtime":2,"relpermalink":"/post/blog/hux%E4%B8%BB%E9%A2%98%E6%B7%BB%E5%8A%A0%E6%90%9C%E7%B4%A2/","section":"post","summary":"本文介绍下给hux主题添加搜索功能，主要是抄 黄玄 的作业，可以去参考huxpro 使用插件：hexo-generator-search npm install hexo-generator-search --save 配","tags":["Blog"],"title":"Hux主题添加搜索","type":"post","url":"/post/blog/hux%E4%B8%BB%E9%A2%98%E6%B7%BB%E5%8A%A0%E6%90%9C%E7%B4%A2/","weight":0,"wordcount":536},{"author":null,"categories":["Code"],"content":" 本文介绍Redis对象机制相关知识，只是对底层做一些了解，并不深入底层的数据结构。\nRedis的5种基础数据类型，在底层是采用对象机制实现的。\nRedis的每种对象其实都由对象结构(redisObject) 与 对应编码的数据结构组合而成，而每种对象类型对应若干编码方式，不同的编码方式所对应的底层数据结构是不同的。 redisObject: redisObject 是 Redis 类型系统的核心, 数据库中的每个键、值, 以及 Redis 本身处理的参数, 都表示为这种数据类型。 其中type、encoding和ptr是最重要的三个属性。\ntype记录了对象所保存的值的类型，它的值可能是以下常量中的一个： /* - 对象类型 */ #define OBJ_STRING 0 // 字符串 #define OBJ_LIST 1 // 列表 #define OBJ_SET 2 // 集合 #define OBJ_ZSET 3 // 有序集 #define OBJ_HASH 4 // 哈希表 encoding记录了对象所保存的值的编码，它的值可能是以下常量中的一个： /* * 对象编码 */ #define OBJ_ENCODING_RAW 0 /* Raw representation */ #define OBJ_ENCODING_INT 1 /* Encoded as integer */ #define OBJ_ENCODING_HT 2 /* Encoded as hash table */ #define OBJ_ENCODING_ZIPMAP 3 /* 注意：版本2.6后不再使用. */ #define OBJ_ENCODING_LINKEDLIST 4 /* 注意：不再使用了，旧版本2.x中String的底层之一. */ #define OBJ_ENCODING_ZIPLIST 5 /* Encoded as ziplist */ #define OBJ_ENCODING_INTSET 6 /* Encoded as intset */ #define OBJ_ENCODING_SKIPLIST 7 /* Encoded as skiplist */ #define OBJ_ENCODING_EMBSTR 8 /* Embedded sds string encoding */ #define OBJ_ENCODING_QUICKLIST 9 /* Encoded as linked list of ziplists */ #define OBJ_ENCODING_STREAM 10 /* Encoded as a radix tree of listpacks */ ptr是一个指针，指向实际保存值的数据结构，这个数据结构由type和encoding属性决定。如果一个redisObject 的type 属性为OBJ_LIST ， encoding 属性为OBJ_ENCODING_QUICKLIST ，那么这个对象就是一个Redis 列表（List)，它的值保存在一个QuickList的数据结构内，而ptr 指针就指向quicklist的对象； 当执行一个处理数据类型命令的时候，redis执行以下步骤：\n根据给定的key，在数据库字典中查找和他相对应的redisObject，如果没找到，就返回NULL； 检查redisObject的type属性和执行命令所需的类型是否相符，如果不相符，返回类型错误； 根据redisObject的encoding属性所指定的编码，选择合适的操作函数来处理底层的数据结构； 返回数据结构的操作结果作为命令的返回值。 比如现在执行LPOP命令： 字符串对象： 从第一张图可以看出字符串对象的编码类型：\nint 编码：保存的是可以用 long 类型表示的整数值。 embstr 编码：保存长度小于44字节的字符串（redis3.2版本之前是39字节，之后是44字节）。 raw 编码：保存长度大于44字节的字符串（redis3.2版本之前是39字节，之后是44字节）。 列表对象：\n列表对象的编码是quicklist。\n哈希对象： 哈希对象的编码可以是 ziplist 或者 hashtable；对应的底层实现有两种, 一种是ziplist, 一种是dict。 集合对象： 集合对象的编码可以是 intset 或者 hashtable; 底层实现有两种, 分别是intset和dict。 显然当使用intset作为底层实现的数据结构时, 集合中存储的只能是数值数据, 且必须是整数; 而当使用dict作为集合对象的底层实现时, 是将数据全部存储于dict的键中, 值字段闲置不用。 有序集合对象： 有序集合的底层实现依然有两种, 一种是使用ziplist作为底层实现, 另外一种比较特殊, 底层使用了两种数据结构: dict与skiplis。 其实有序集合单独使用字典或跳跃表其中一种数据结构都可以实现，但是这里使用两种数据结构组合起来，原因是假如我们单独使用 字典，虽然能以 O(1) 的时间复杂度查找成员的分值，但是因为字典是以无序的方式来保存集合元素，所以每次进行范围操作的时候都要进行排序；假如我们单独使用跳跃表来实现，虽然能执行范围操作，但是查找操作有 O(1)的复杂度变为了O(logN)。因此Redis使用了两种数据结构来共同实现有序集合。\n参考：\nredis对象与编码(底层结构)对应关系详解 ","date":1618963200,"description":"","dir":"post\\Redis\\","excerpt_html":null,"excerpt_text":null,"expirydate":-62135596800,"fuzzywordcount":1600,"html":null,"keywords":null,"kind":"page","lang":"en","lastmod":1618963200,"objectID":"89669769e14c911cfecd93d3528b385c","permalink":"http://localhost:1313/post/redis/redis%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86----%E5%AF%B9%E8%B1%A1%E6%9C%BA%E5%88%B6/","publishdate":"2021-04-21T00:00:00Z","readingtime":4,"relpermalink":"/post/redis/redis%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86----%E5%AF%B9%E8%B1%A1%E6%9C%BA%E5%88%B6/","section":"post","summary":"本文介绍Redis对象机制相关知识，只是对底层做一些了解，并不深入底层的数据结构。 Redis的5种基础数据类型，在底层是采用对象机制实现的。","tags":["Redis"],"title":"Redis相关知识----对象机制","type":"post","url":"/post/redis/redis%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86----%E5%AF%B9%E8%B1%A1%E6%9C%BA%E5%88%B6/","weight":0,"wordcount":1515},{"author":null,"categories":["Code"],"content":" 本文介绍Redis的数据类型相关知识\nRedis数据结构简介 对于Redis，所有的Key都是字符串。我们在谈Redis基础数据结构时，讨论的是存储值的数据类型，主要包括常见的5种数据类型，分别是：String、List、Set、Zset、Hash。\nString字符串 String是redis中最基本的数据类型，一个key对应一个value。String类型是二进制安全的，意思是 redis 的 string 可以包含任何数据。如数字，字符串，jpg图片或者序列化的对象。\n命令：\n命令 简述 使用 GET 获取存储在给定键中的值 GET value SET 设置存储在给定键中的值 SET value DEL 删除存储在给定键中的值 DEL value INCR 将键存储的值加1 INCR key DECR 将键存储的值减1 DECR key INCRBY 将键存储的值加上整数 INCRBY key amount DECRBY 将键存储的值减去整数 DECRBY key amount 使用场景：\n缓存： 经典使用场景，把常用信息，字符串，图片或者视频等信息放到redis中，redis作为缓存层，mysql做持久化层，降低mysql的读写压力。 计数器：redis是单线程模型，一个命令执行完才会执行下一个，同时数据可以一步落地到其他的数据源。 session：常见方案spring session + redis实现session共享。 List列表 Redis中的List其实就是链表（Redis用双端链表实现List）。\n命令：\n命令 简述 使用 RPUSH 将给定值推入到列表右端 RPUSH key value LPUSH 将给定值推入到列表左端 LPUSH key value RPOP 从列表的右端取出一个值 RPOP key value LPOP 从列表的左端取出一个值 LPOP key value LRANGE 获取列表在给定范围上的所有值 LRANGE key 0 -1 LINDEX 通过索引获取列表中的元素 LINDEX key index 使用Redis List的技巧：\nLPUSH + RPOP 相当于队列 LPUSH + LPOP 相当于栈 LPUSH + BRPOP 相当于消息队列 使用场景：\n微博TimeLine: 有人发布微博，用lpush加入时间轴，展示新的列表信息。 消息队列：可以利用List的 PUSH 操作，将任务存放在List中，然后工作线程再用 POP 操作将任务取出进行执行，相当于生产者消费者模型。 Set集合 Redis 的 Set 是 String 类型的无序集合。集合成员是唯一的，这就意味着集合中不能出现重复的数据。Redis 中集合是通过哈希表实现的，所以添加，删除，查找的复杂度都是 O(1)。\n命令：\n命令 简述 使用 SADD 向集合添加一个或多个成员 SADD set-key value SREM 向集合删除一个或多个成员 SREM set-key value SCARD 获取集合的成员数 SCARD set-key SMEMBERS 返回集合中的所有成员 SMEMBERS set-key SISMEMBER 判断 member 元素是否是集合 key 的成员 SISMEMBER set-key value 使用场景：\n标签（tag）,给用户添加标签，或者用户给消息添加标签，这样有同一标签或者类似标签的可以给推荐关注的事或者关注的人。 点赞，或点踩，收藏等，可以放到set中实现 Hash散列 Redis hash 是一个 string 类型的 field（字段） 和 value（值） 的映射表，hash 特别适合用于存储对象。\n命令：\n命令 简述 使用 HSET 添加键值对 HSET hash-key sub-key1 value1 HGET 获取指定散列键的值 HGET hash-key sub-key1 HGETALL 获取散列中包含的所有键值对 HGETALL hash-key HDEL 如果给定键存在于散列中，那么就移除这个键 HDEL hash-key sub-key1 使用场景：\n缓存： 能直观，相比string更节省空间，的维护缓存信息，如用户信息，视频信息等。 Sorted Sets有序集合 Redis 有序集合和集合一样也是 string 类型元素的集合,且不允许重复的成员。不同的是每个元素都会关联一个 double 类型的分数。redis 正是通过分数来为集合中的成员进行从小到大的排序。\n有序集合的成员是唯一的,但分数(score)却可以重复。集合是通过哈希表实现的，所以添加，删除，查找的复杂度都是 O(1)。\n命令：\n命令 简述 使用 ZADD 将所有指定成员添加到键为key有序集合（sorted set）里面 ZADD zset-key score member1 ZREM 如果给定元素成员存在于有序集合中，那么就移除这个元素 ZREM zset-key member1 ZRANGE 返回存储在有序集合key中的指定范围的元素。 ZRANGE zset-key start stop withccores ZCOUNT 返回有序集key中，score值在min和max之间的成员数。 ZCOUNT zset-key min max 使用场景：\n排行榜：有序集合经典使用场景。例如小说视频等网站需要对用户上传的小说视频做排行榜，榜单可以按照用户关注数，更新时间，字数等打分，做排行。 HyperLogLogs（基数统计） 什么是基数？\n举个例子，A = {1, 2, 3, 4, 5}， B = {3, 5, 6, 7, 9}；那么基数（不重复的元素）= 1, 2, 4, 6, 7, 9； （允许容错，即可以接受一定误差）\nHyperLogLogs 基数统计用来解决什么问题？\n这个结构可以非常省内存的去统计各种计数，比如注册 IP 数、每日访问 IP 数、页面实时UV、在线用户数，共同好友数等。\nBitmap （位存储） Bitmap 即位图数据结构，都是操作二进制位来进行记录，只有0 和 1 两个状态。\n用来解决什么问题？\n比如：统计用户信息，活跃，不活跃！ 登录，未登录！ 打卡，不打卡！ 两个状态的，都可以使用 Bitmaps！\n命令：\n命令 简述 使用 SETBIT 对key所储存的字符串值，设置或清除指定偏移量上的位(bit) SETBIT bit-key offset value GETBIT 对 key 所储存的字符串值，获取指定偏移量上的位(bit) GETBIT bit-key offset BITCOUNT 被设置为 1 的位的数量 BITCOUNT bit-key geospatial (地理位置) Redis 的 Geo 可以推算地理位置的信息: 两地之间的距离, 方圆几里的人。\n参考：\nRedis 5种基础数据类型详解 Redis 3种特殊类型详解 ","date":1618790400,"description":"","dir":"post\\Redis\\","excerpt_html":null,"excerpt_text":null,"expirydate":-62135596800,"fuzzywordcount":2000,"html":null,"keywords":null,"kind":"page","lang":"en","lastmod":1618790400,"objectID":"8a1fe8b89a9e9681643a13ccb6f3a276","permalink":"http://localhost:1313/post/redis/redis%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86----%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/","publishdate":"2021-04-19T00:00:00Z","readingtime":4,"relpermalink":"/post/redis/redis%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86----%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/","section":"post","summary":"本文介绍Redis的数据类型相关知识 Redis数据结构简介 对于Redis，所有的Key都是字符串。我们在谈Redis基础数据结构时，讨论的是","tags":["Redis"],"title":"Redis相关知识----数据类型","type":"post","url":"/post/redis/redis%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86----%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/","weight":0,"wordcount":1963},{"author":null,"categories":["Code"],"content":" 本文介绍BlockingQueue阻塞队列相关知识\n简介 BlockingQueue是JUC包下的一个接口，通常用于一个线程生产对象，而另外一个线程消费这些对象的场景。 方法： BlockingQueue继承Queue接口，因此，对数据元素的基本操作有：\n插入元素\nadd(E e) ：往队列插入数据，当队列满时，插入元素时会抛出IllegalStateException异常； offer(E e)：当往队列插入数据时，插入成功返回true，否则则返回false。 删除元素\nremove(Object o)：从队列中删除数据，成功则返回true，否则为false poll：删除数据，当队列为空时，返回null； 查看元素\nelement：获取队头元素，如果队列为空时则抛出NoSuchElementException异常； peek：获取队头元素，如果队列为空则抛出NoSuchElementException异常 BlockingQueue具有的特殊操作：\n插入数据：\nput：当阻塞队列容量已经满时，往阻塞队列插入数据的线程会被阻塞，直至阻塞队列已经有空余的容量可供使用； offer(E e, long timeout, TimeUnit unit)：若阻塞队列已经满时，同样会阻塞插入数据的线程，直至阻塞队列已经有空余的地方，与put方法不同的是，该方法会有一个超时时间，若超过当前给定的超时时间，插入数据的线程会退出； 删除数据\ntake()：当阻塞队列为空时，获取队头数据的线程会被阻塞； poll(long timeout, TimeUnit unit)：当阻塞队列为空时，获取数据的线程会被阻塞，另外，如果被阻塞的线程超过了给定的时长，该线程会退出 常用实现类 ArrayBlockingQueue： ArrayBlockingQueue是由数组实现的有界队列，ArrayBlockingQueue可作为“有界数据缓冲区”，生产者插入数据到队列容器中，并由消费者提取。ArrayBlockingQueue一旦创建，容量不能改变。\n从ArrayBlockingQueue的构造函数中可以看出，线程访问队列默认是非公平的，但是可以调用另一个构造函数进行设置。\n当队列容量满时，尝试将元素放入队列将导致操作阻塞;尝试从一个空队列中取一个元素也会同样阻塞。\npublic ArrayBlockingQueue(int capacity) { this(capacity, false); } public ArrayBlockingQueue(int capacity, boolean fair) { if (capacity \u0026lt;= 0) throw new IllegalArgumentException(); this.items = new Object[capacity]; lock = new ReentrantLock(fair); notEmpty = lock.newCondition(); notFull = lock.newCondition(); } LinkedBlockingQueue: LinkedBlockingQueue是用链表实现的有界阻塞队列，同样满足FIFO的特性，与ArrayBlockingQueue相比起来具有更高的吞吐量，为了防止LinkedBlockingQueue容量迅速增，损耗大量内存。通常在创建LinkedBlockingQueue对象时，会指定其大小，如果未指定，容量等于Integer.MAX_VALUE。\nLinkedBlockingDeque: LinkedBlockingDeque是基于链表数据结构的有界阻塞双端队列，如果在创建对象时为指定大小时，其默认大小为Integer.MAX_VALUE。与LinkedBlockingQueue相比，主要的不同点在于LinkedBlockingDeque具有双端队列的特性。\nLinkedTransferQueue: LinkedTransferQueue是一个由链表数据结构构成的无界阻塞队列，由于该队列实现了TransferQueue接口，与其他阻塞队列相比主要有以下不同的方法：\ntransfer(E e): 如果当前有线程（消费者）正在调用take()方法或者可延时的poll()方法进行消费数据时，生产者线程可以调用transfer方法将数据传递给消费者线程。如果当前没有消费者线程的话，生产者线程就会将数据插入到队尾，直到有消费者能够进行消费才能退出；\ntryTransfer(E e): tryTransfer方法如果当前有消费者线程（调用take方法或者具有超时特性的poll方法）正在消费数据的话，该方法可以将数据立即传送给消费者线程，如果当前没有消费者线程消费数据的话，就立即返回false。因此，与transfer方法相比，transfer方法是必须等到有消费者线程消费数据时，生产者线程才能够返回。而tryTransfer方法能够立即返回结果退出。\ntryTransfer(E e,long timeout,imeUnit unit) 与transfer基本功能一样，只是增加了超时特性，如果数据才规定的超时时间内没有消费者进行消费的话，就返回false。\nPriorityBlockingQueue:\nPriorityBlockingQueue是一个支持优先级的无界阻塞队列（容量不够时会自动扩容,是二叉树最小堆的实现）。默认情况下元素采用自然顺序进行排序，也可以通过自定义类实现compareTo()方法来指定元素排序规则，或者初始化时通过构造器参数Comparator来指定排序规则。 它的 take 方法在队列为空的时候会阻塞，但是正因为它是无界队列，而且会自动扩容，所以它的队列永远不会满，所以它的 put 方法永远不会阻塞，添加操作始终都会成功。 SynchronousQueue：\nsynchronousQueue 是一个不存储任何元素的阻塞队列，每一个put操作必须等待take操作，否则不能添加元素。同时它也支持公平锁和非公平锁。 synchronousQueue 的容量并不是1，而是0。因为它本身不会持有任何元素，它是直接传递的，synchronousQueue 会把元素从生产者直接传递给消费者，在这个过程中能够是不需要存储的。 线程池 CachedThreadPool 就是利用了该队列，Executors.newCachedThreadPool()，因为这个线程池它的最大线程数是Integer.MAX_VALUE，它是更具需求来创建线程，所有的线程都是临时线程，使用完后空闲60秒则被回收， DelayQueue：\nDelayQueue 是一个使用PriorityBlockingQueue的延迟获取的无界队列。具有“延迟”的功能。 DelayQueue 应用场景：1. 缓存系统的设计：可以用DelayQueue保存缓存元素的有效期，使用一个线程循环查询DelayQueue，一旦能从DelayQueue中获取元素时，表示缓存有效期到了。2. 定时任务调度。使用DelayQueue保存当天将会执行的任务和执行时间，一旦从DelayQueue中获取到任务就开始执行，从比如TimerQueue就是使用DelayQueue实现的。 DelayQueue是一个存放实现Delayed接口的数据的无界阻塞队列，只有当数据对象的延时时间达到时才能插入到队列进行存储。如果当前所有的数据都还没有达到创建时所指定的延时期，则队列没有队头，并且线程通过poll等方法获取数据元素则返回null。所谓数据延时期满时，则是通过Delayed接口的getDelay(TimeUnit.NANOSECONDS)来进行判定，如果该方法返回的是小于等于0则说明该数据元素的延时期已满。 参考：\n并发容器之BlockingQueue JAVA中常见的阻塞队列详解 并发队列-无界阻塞优先级队列PriorityBlockingQueue原理探究 Java-BlockingQueue 接口5大实现类的使用场景 ","date":1618704000,"description":"","dir":"post\\Java Concurrent\\","excerpt_html":null,"excerpt_text":null,"expirydate":-62135596800,"fuzzywordcount":2800,"html":null,"keywords":null,"kind":"page","lang":"en","lastmod":1618704000,"objectID":"2ed4b9e4544b54a9952377edb923c807","permalink":"http://localhost:1313/post/java-concurrent/%E9%98%BB%E5%A1%9E%E9%98%9F%E5%88%97blockingqueue/","publishdate":"2021-04-18T00:00:00Z","readingtime":6,"relpermalink":"/post/java-concurrent/%E9%98%BB%E5%A1%9E%E9%98%9F%E5%88%97blockingqueue/","section":"post","summary":"本文介绍BlockingQueue阻塞队列相关知识 简介 BlockingQueue是JUC包下的一个接口，通常用于一个线程生产对象，而另外一个","tags":["Java并发"],"title":"阻塞队列BlockingQueue","type":"post","url":"/post/java-concurrent/%E9%98%BB%E5%A1%9E%E9%98%9F%E5%88%97blockingqueue/","weight":0,"wordcount":2731},{"author":null,"categories":["Code"],"content":" 本文介绍Mysql ACID特性的实现原理\nACID:\n原子性 一致性 隔离性 持久性 原子性：一个事务是一个不可切割的单位，要么全部执行成功，要么全部失败。\n是采用undo log日志实现的。undo log日志用来记录Mysql逻辑语句的执行。\n当事务对数据库进行修改时，InnoDB会生成对应的undo log；如果事务执行失败或调用了rollback，导致事务需要回滚，便可以利用undo log中的信息将数据回滚到修改之前的样子。\n当发生回滚时，InnoDB会根据undo log的内容做与之前相反的工作：对于每个insert，回滚时会执行delete；对于每个delete，回滚时会执行insert；对于每个update，回滚时会执行一个相反的update，把数据改回去。\n一致性：数据库在事务执行前后都保持一致性状态。在一致性状态下，所有事务对一个数据的读取结果都是相同的。\n从数据库层面，数据库通过原子性、隔离性、持久性来保证一致性。也就是说ACID四大特性之中，C(一致性)是目的，A(原子性)、I(隔离性)、D(持久性)是手段，是为了保证一致性，数据库提供的手段。数据库必须要实现AID三大特性，才有可能实现一致性。例如，原子性无法保证，显然一致性也无法保证。\n但是，如果你在事务里故意写出违反约束的代码，一致性还是无法保证的。例如，你在转账的例子中，你的代码里故意不给B账户加钱，那一致性还是无法保证。因此，还必须从应用层角度考虑。\n从应用层面，通过代码判断数据库数据是否有效，然后决定回滚还是提交数据！\n隔离性：事务内部的操作与其他事务是隔离的，并发执行的各个事务之间不能互相干扰。\n隔离性主要解决并发环境下，事务之间互不干扰，因为并发情况下会出现并发一致性问题。\n丢失修改 脏读 不可重复读 幻读 这些并发一致性问题，从读写角度考虑，可以通过不同的方式解决\n(一个事务)写操作对(另一个事务)写操作的影响：锁机制保证隔离性 (一个事务)写操作对(另一个事务)读操作的影响：MVCC保证隔离性 按照锁的粒度，可以分位表锁和行锁。MyIsam只支持表锁，而InnoDB同时支持表锁和行锁。\nMVCC（Multi-Version Concurrency Control）：多版本并发控制，通过版本链、undo log、ReadView实现。\n隐藏列：InnoDB中每行数据都有隐藏列，隐藏列中包含了本行数据的事务id、指向undo log的指针等。 基于undo log的版本链：前面说到每行数据的隐藏列中包含了指向undo log的指针，而每条undo log也会指向更早版本的undo log，从而形成一条版本链。 ReadView：通过隐藏列和版本链，MySQL可以将数据恢复到指定版本；但是具体要恢复到哪个版本，则需要根据ReadView来确定。所谓ReadView，是指事务（记做事务A）在某一时刻给整个事务系统（trx_sys）打快照，之后再进行读操作时，会将读取到的数据中的事务id与trx_sys快照比较，从而判断数据对该ReadView是否可见，即对事务A是否可见。 ReadView中的重要id\ntrx_ids: 当前系统活跃(未提交)事务版本号集合。 low_limit_id: 创建当前read view 时“当前系统最大事务版本号+1”。 up_limit_id: 创建当前read view 时“系统正处于活跃事务最小版本号” creator_trx_id: 创建当前read view的事务版本号； SQL标准中定义了四种隔离级别，并规定了每种隔离级别下上述几个问题是否存在，mysql默认的隔离级别为RR（可重复读）。 上面说的MVCC用于支持RC和RR的实现，是一种非加锁的形式。\nRR是在事务开始后第一次执行select前创建ReadView，直到事务提交都不会再创建。根据前面的介绍，RR可以避免脏读、不可重复读和幻读。 RC每次执行select前都会重新建立一个新的ReadView，因此如果事务A第一次select之后，事务B对数据进行了修改并提交，那么事务A第二次select时会重新建立新的ReadView，因此事务B的修改对事务A是可见的。因此RC隔离级别可以避免脏读，但是无法避免不可重复读和幻读。 解决幻读：\n通过MVCC非加锁读，也称作快照读、一致性读 加锁读：record lock(记录锁) + gap lock(间隙锁) 持久性：事务一旦提交，它对数据库的改变就应该是永久性的。接下来的其他操作或故障不应该对其有任何影响。\n使用redo log实现，保证数据库在宕机的情况下，数据也不会丢失。\nredo log 和 bin log的区别：\n作用不同：redo log是用于crash recovery的，保证MySQL宕机也不会影响持久性；binlog是用于point-in-time recovery的，保证服务器可以基于时间点恢复数据，此外binlog还用于主从复制。 层次不同：redo log是InnoDB存储引擎实现的，而binlog是MySQL的服务器层(可以参考文章前面对MySQL逻辑架构的介绍)实现的，同时支持InnoDB和其他存储引擎。 redo log是物理日志，内容基于磁盘的Page；binlog的内容是二进制的，根据binlog_format参数的不同，可能基于sql语句、基于数据本身或者二者的混合。 写入时机不同：binlog在事务提交时写入；redo log的写入时机相对多元：事务提交时、master thread每秒刷盘等。 参考：\n深入学习Mysql事务 数据库MVCC ","date":1618531200,"description":"","dir":"post\\Mysql\\","excerpt_html":null,"excerpt_text":null,"expirydate":-62135596800,"fuzzywordcount":2200,"html":null,"keywords":null,"kind":"page","lang":"en","lastmod":1618531200,"objectID":"ca06880ed5e8ba7dd05539e78ec70505","permalink":"http://localhost:1313/post/mysql/mysql%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86%E4%BA%94/","publishdate":"2021-04-16T00:00:00Z","readingtime":5,"relpermalink":"/post/mysql/mysql%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86%E4%BA%94/","section":"post","summary":"本文介绍Mysql ACID特性的实现原理 ACID: 原子性 一致性 隔离性 持久性 原子性：一个事务是一个不可切割的单位，要么全部执行成功，要么全部失败。 是采","tags":["Mysql"],"title":"Mysql相关知识（五）","type":"post","url":"/post/mysql/mysql%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86%E4%BA%94/","weight":0,"wordcount":2181},{"author":null,"categories":["Code"],"content":" 本文介绍Mysql中explain相关知识\nexplain: 当mysql的查询语句执行较慢时，可以通过使用explain命令解释mysql语句，通过结果分析mysql语句执行慢的原因，来优化mysql语句。\nexpain出来的信息有10列：\nid select_type table type possible_keys key key_len ref rows Extra id：SQL执行的顺序的标识,SQL根据id从大到小的执行\nid列的编号是 select 的序列号，有几个 select 就有几个id，并且id的顺序是按 select 出现的顺序增长的。MySQL将 select 查询分为简单查询和复杂查询。复杂查询分为三类：简单子查询、派生表（from语句中的子查询）、union 查询。\nid相同时，执行顺序由上至下 如果是子查询，id的序号会递增，id值越大优先级越高，越先被执行 id如果相同，可以认为是一组，从上往下顺序执行；在所有组中，id值越大，优先级越高，越先执行 select_type：查询中每个select子句的类型\nsimple：简单查询。查询不包含子查询和union primary：复杂查询中最外层的 select subquery：包含在 select 中的子查询（不在 from 子句中） derived：包含在 from 子句中的子查询。MySQL会将结果存放在一个临时表中，也称为派生表（derived的英文含义） mysql\u0026gt; explain select (select 1 from actor where id = 1) from (select * from film where id = 1) der; +----+-------------+------------+--------+---------------+---------+---------+-------+------+-------------+ | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra | +----+-------------+------------+--------+---------------+---------+---------+-------+------+-------------+ | 1 | PRIMARY | \u0026lt;derived3\u0026gt; | system | NULL | NULL | NULL | NULL | 1 | NULL | | 3 | DERIVED | film | const | PRIMARY | PRIMARY | 4 | const | 1 | NULL | | 2 | SUBQUERY | actor | const | PRIMARY | PRIMARY | 4 | const | 1 | Using index | +----+-------------+------------+--------+---------------+---------+---------+-------+------+-------------+ union：在 union 中的第二个和随后的 select union result：从 union 临时表检索结果的 select mysql\u0026gt; explain select 1 union all select 1; +----+--------------+------------+------+---------------+------+---------+------+------+-----------------+ | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra | +----+--------------+------------+------+---------------+------+---------+------+------+-----------------+ | 1 | PRIMARY | NULL | NULL | NULL | NULL | NULL | NULL | NULL | No tables used | | 2 | UNION | NULL | NULL | NULL | NULL | NULL | NULL | NULL | No tables used | | NULL | UNION RESULT | \u0026lt;union1,2\u0026gt; | ALL | NULL | NULL | NULL | NULL | NULL | Using temporary | +----+--------------+------------+------+---------------+------+---------+------+------+-----------------+ table：正在访问哪个表\ntype：表示MySQL在表中找到所需行的方式，又称“访问类型” 常用的类型有： ALL, index, range, ref, eq_ref, const, system, NULL（从左到右，性能从差到好）\nALL：即全表扫描，意味着mysql需要从头到尾去查找所需要的行。通常情况下这需要增加索引来进行优化了 index: 和ALL一样，不同就是mysql只需扫描索引树，这通常比ALL快一些。 range:范围扫描通常出现在 in(), between ,\u0026gt; ,\u0026lt;, \u0026gt;= 等操作中。使用一个索引来检索给定范围的行。 ref: 表示上述表的连接匹配条件，即哪些列或常量被用于查找索引列上的值 eq_ref: 类似ref，区别就在使用的索引是唯一索引，对于每个索引键值，表中只有一条记录匹配，简单来说，就是多表连接中使用primary key或者 unique key作为关联条件 const、system: 当MySQL对查询某部分进行优化，并转换为一个常量时，使用这些类型访问。如将主键置于where列表中，MySQL就能将该查询转换为一个常量,system是const类型的特例，当查询的表只有一行的情况下，使用system NULL: mysql能够在优化阶段分解查询语句，在执行阶段用不着再访问表或索引。例如：在索引列中选取最小值，可以单独查找索引来完成，不需要在执行时访问表。 possible_keys：查询可能使用哪些索引来查找\nkey：实际采用哪个索引来优化对该表的访问\nkey_len：mysql在索引里使用的字节数，通过这个值可以算出具体使用了索引中的哪些列 ref：显示了在key列记录的索引中，表查找值所用到的列或常量\nrows： 表示MySQL根据表统计信息及索引选用情况，估算的找到所需的记录所需要读取的行数\nExtra：展示的是额外信息\ndistinct: 一旦mysql找到了与行相联合匹配的行，就不再搜索了 Using index：这发生在对表的请求列都是同一索引的部分的时候，返回的列数据只使用了索引中的信息，而没有再去访问表中的行记录。是性能高的表现。 Using where：mysql服务器将在存储引擎检索行后再进行过滤。就是先读取整行数据，再按 where 条件进行检查，符合就留下，不符合就丢弃。 Using temporary：mysql需要创建一张临时表来处理查询。出现这种情况一般是要进行优化的，首先是想到用索引来优化。 Using filesort：mysql 会对结果使用一个外部索引排序，而不是按索引次序从表里读取行。此时mysql会根据联接类型浏览所有符合条件的记录，并保存排序关键字和行指针，然后排序关键字并按顺序检索行信息。这种情况下一般也是要考虑使用索引来优化的。 参考：\nMysql Explain详解 ","date":1618272000,"description":"","dir":"post\\Mysql\\","excerpt_html":null,"excerpt_text":null,"expirydate":-62135596800,"fuzzywordcount":1800,"html":null,"keywords":null,"kind":"page","lang":"en","lastmod":1618272000,"objectID":"d35eeb8b3c1358918d3a288666eef1e4","permalink":"http://localhost:1313/post/mysql/mysql%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86%E5%9B%9B/","publishdate":"2021-04-13T00:00:00Z","readingtime":4,"relpermalink":"/post/mysql/mysql%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86%E5%9B%9B/","section":"post","summary":"本文介绍Mysql中explain相关知识 explain: 当mysql的查询语句执行较慢时，可以通过使用explain命令解释mysql语句，通过结果分析","tags":["Mysql"],"title":"Mysql相关知识（四）","type":"post","url":"/post/mysql/mysql%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86%E5%9B%9B/","weight":0,"wordcount":1797},{"author":null,"categories":["Code"],"content":" 本文介绍Mysql索引相关知识\n索引是什么\n索引是一种帮助数据库高效查询数据的数据结构\n索引本身也很大，不可能全部存储在内存中，因此索引往往是存储在磁盘上的文件中。（可能是单独的索引文件，也可能是和数据一起存储在数据文件中）\n通常所说的索引，包括聚集索引、覆盖索引、组合索引、前缀索引、唯一索引等，没有特别说明，默认都是使用B+树结构组织（多路搜索树，并不一定是二叉的）的索引。\n索引的类型\n主键索引：索引列中的值必须是唯一的，且不允许有空值。 唯一索引：索引列中的值必须是唯一的，但允许为空值。 全文索引：只能在文本类型CHAR,VARCHAR,TEXT类型字段上创建全文索引。字段长度比较大时，如果创建普通索引，在进行like模糊查询时效率比较低，这时可以创建全文索引。 组合索引： 可以是单列上创建的索引，也可以是在多列上创建的索引。 普通索引： 最基本的索引类型，没有唯一性之类的限制。 索引相关操作：\n主键索引：\n#建表的时候创建主键索引 create table 表名 (字段1 数据类型，字段2 数据类型,primary key (列名)); #修改表增加主键索引 alter table 表名 add primary key (列名); #删除主键索引 alter table 表名 drop primary key; 唯一索引：\n#创建 create unique index 索引名 on 表名 (列名); 或 alter table 表名 add unique 索引名 (列名); 或 create table 表名 (字段1 数据类型，字段2 数据类型,unique 索引名 (列名)); 全文索引:\ncreate fulltext index 索引名 on 表名 (列名); alter table 表名 add fulltext 索引名 (列名); create table 表名 (字段1 数据类型，字段2 数据类型,fulltext 索引名 (列名)); 组合索引：\ncreate table 表名 (字段1 数据类型，字段2 数据类型,index 索引名 (列名1，列名2)); #需要满足最左原则，因为select语句的 where 条件是依次从左往右执行的，所以在使用 select 语句查询时 where 条件使用的字段顺序必须和组合索引中的排序一致，否则索引将不会生效。 # select * from 表名 where 列名1=\u0026#39;...\u0026#39; and(or) 列名2=\u0026#39;...\u0026#39; 普通索引：\ncreate index 索引名 on 表名 (列名[(length)]); #(列名[(length)]：length 为可选项，如果忽略 length 的值，则使用整个列的值作为索引。如果指定使用列的前 length 个字符来创建索引，这样有利于减小索引文件的大小。 alter table 表名 add index 索引名 (列名); 查看索引：\nshow index from T; show keys form T; Mysql索引的数据结构：B+Tree 不使用下列数据结构的原因：\n二叉树：极端情况下会成为类似链表的结构，而且树高会增大磁盘IO，影响查询效率 平衡二叉树：树高会增大磁盘IO，影响查询效率 hash表：适合等值查询，不适合范围查询会查询整个表 BTree：\n叶子节点和非叶子节点都存放数据，不支持范围查询的快速查询（会多词遍历根节点）。 如果data存储的是行记录，行的大小随着列数的增多，所占空间会变大。这时，一个页中可存储的数据量就会变少，树相应就会变高，磁盘IO次数就会变大。 B+Tree：\n只有叶子节点存储数据，非叶子节点存储键值。叶子节点之间使用双向指针连接，最底层的叶子节点形成了一个双向有序链表。 InnoDB索引：\n主键索引也叫聚簇索引，使用B+Tree构建，叶子节点存放的是数据表的整行记录。一般情况下，聚簇索引等同于主键索引，当一个表没有创建主键索引时，InnoDB会自动创建一个ROWID字段来构建聚簇索引。\n除聚簇索引之外的所有索引都称为辅助索引。辅助索引叶子节点存储的是该行的主键值，在检索时，InnoDB使用此主键值在聚簇索引中搜索行记录，这个过程也称回表。\n哪些情况需要创建索引：\n主键自动建立唯一索引 频繁作为查询条件的字段应该创建索引 查询中与其他表关联的字段，外键关系应该建立索引 频繁更新的字段不适合创建索引 where条件里用不到的字段不创建索引 查询中排序的字段，排序字段若通过索引去访问将大大提高排序速度 查询中统计或者分组的字段 哪些情况不需要创建索引：\n表记录太少 经常增删改的表 数据列有许多重复的内容 索引优化：\n覆盖索引 最左前缀原则 索引下推 参考：\nMysql索引相关知识 Mysql索引概念相关知识 Mysql45讲 ","date":1618012800,"description":"","dir":"post\\Mysql\\","excerpt_html":null,"excerpt_text":null,"expirydate":-62135596800,"fuzzywordcount":1700,"html":null,"keywords":null,"kind":"page","lang":"en","lastmod":1618012800,"objectID":"ee33c150f052bb550d3e0e869b7748d4","permalink":"http://localhost:1313/post/mysql/mysql%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86%E4%B8%89%E7%B4%A2%E5%BC%95/","publishdate":"2021-04-10T00:00:00Z","readingtime":4,"relpermalink":"/post/mysql/mysql%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86%E4%B8%89%E7%B4%A2%E5%BC%95/","section":"post","summary":"本文介绍Mysql索引相关知识 索引是什么 索引是一种帮助数据库高效查询数据的数据结构 索引本身也很大，不可能全部存储在内存中，因此索引往往是存储","tags":["Mysql"],"title":"Mysql相关知识（三）索引","type":"post","url":"/post/mysql/mysql%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86%E4%B8%89%E7%B4%A2%E5%BC%95/","weight":0,"wordcount":1636},{"author":null,"categories":["Code"],"content":" 本文介绍Mysql操作和语句相关知识，包括增删改查、建表、函数、过程等相关知识。\n1.操作 连接Mysql：\nmysql -h 主机地址 -u 用户名 -p 密码 本地连接： mysql -u root -p 修改密码：\nmysqladmin -u 用户名 -p 旧密码 password 新密码 或者 alter user `username`@`host` identified by \u0026#39;password\u0026#39; 增加权限：\ngrant all privileges on databasename.tablename to 用户名@登录主机 identified by 密码 增加一个用户 test1 密码为 abc，让他可以在任何主机上登录，并对所有数据库 有查询、插入、修改、删除的权限： grant select,insert,update,delete on . to `test1`@`localhost` identified by \u0026#34;abc\u0026#34; 删除授权：\nrevoke all privileges on databasename.tablename from `username`@`host` 创建用户：\ncreate user `username`@`host` identified by \u0026#39;password\u0026#39; 要求使用ssl登录 create user `username`@`host` identified by \u0026#39;password\u0026#39; require ssl; 锁定用户\nalter user `username`@`host` account lock; 解锁 alter user `username`@`host` account unlock; 删除用户：\ndrop user `username`@`host` 2.常用命令 数据库：\nshow databases; #显示数据库 create database [if not exists] t [character set=\u0026#39;utf8\u0026#39;]; #建数据库 use t; #使用数据库 drop database t; #删除数据库 show tables; #显示表 #建表 CREATE TABLE `T` ( `id` int NOT NULL AUTO_INCREMENT, `a` varchar(30) DEFAULT NULL, `b` int DEFAULT NULL, PRIMARY KEY (`id`), UNIQUE KEY `a` (`a`), KEY `b` (`b`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_bin; #查看建表语句 show create table `T`; #显示表结构 desc T; #删除表 drop table [if exists] T; 表复制、备份、清除\n#会造成索引丢失，只有表结构，没有主键等信息。 create table 新表名 select * from T; 或 create table 新表名 as(select * from T); #讲旧表中的数据移入新表 insert into 新表 select * from T; #清空表数据 truncate table T; 表相关操作：\n#修改列名 alter table T change \u0026lt;原字段名称\u0026gt; \u0026lt;新字段名称 字段类型\u0026gt; #修改表名 alter table T rename newT #修改字段类型及指定是否为空 alter table T modify(or change) \u0026lt;字段名称\u0026gt; \u0026lt;字段类型\u0026gt; [not null] #增加一个字段 alter table T add column 字段名称 字段类型 (after 某个字段) (first) #删除字段 alter table \u0026lt;表名称\u0026gt; drop column \u0026lt;列名\u0026gt;; 查表：\nSELECT [DISTINCT] \u0026lt;字段名称,用逗号隔开/*\u0026gt; FROM \u0026lt;left_table\u0026gt; [\u0026lt;join_type\u0026gt; JOIN \u0026lt;right_table\u0026gt; ON \u0026lt;连接条件\u0026gt;] WHERE \u0026lt;where条件\u0026gt; GROUP BY \u0026lt;分组字段\u0026gt; HAVING \u0026lt;筛选条件\u0026gt; ORDER BY \u0026lt;排序条件\u0026gt; [desc/asc] LIMIT n[, m] 增改删：\n#增加数据 insert into T values(); #更改数据 update T set *** where ** = ** #删除数据 delete FROM T WHERE ** = ** 索引：\n#创建索引 -- 普通索引 ALTER TABLE 表名称 ADD INDEX index_name (column_list) -- 唯一索引 ALTER TABLE 表名称 ADD UNIQUE (column_list) -- 主键索引 ALTER TABLE 表名称 ADD PRIMARY KEY (column_list) 或者 CREATE INDEX index_name ON 表名称 (column_list) #删除索引 DROP INDEX index_name ON 表名称; ALTER TABLE 表名称 DROP INDEX index_name; #删除主键 alter table T drop primary key; #查看索引 show index from T; show keys from T; 变量：\n#查看满足条件的部分系统变量 show global | session variables like \u0026#39;%char%\u0026#39;; 查看指定的某个系统变量的值 select @@global|session.系统变量名; #为某个系统变量赋值 set global|session 系统变量名 = 值; 或 set @@global|session.系统变量名 = 值; #用户变量声明并初始化 set @用户变量名=值 #使用 select @用户变量名 #声明局部变量 declare 变量名 类型; declare 变量名 类型 default 值; #赋值和使用同用户变量一样 存储过程：\n#创建存储过程 create procedure 存储过程名(参数列表) begin 方法体(一组合法的sql语句) end #存储过程的结尾可以使用delimiter重新设置 delimiter 结束标志 例： delimiter $ #创建过程 delimiter ;; create procedure idata() begin declare i int; set i=1; while(i\u0026lt;=100000)do insert into t values(i, i, i); set i=i+1; end while; end;; delimiter ; #调用 call idata(); #查看存储过程 show create procedure 存储过程名; #删除存储过程 drop procedure 存储过程名; 函数：\n#创建函数 create function 函数名（参数列表）returns 返回类型 begin 函数体 end 注意事项： 1.参数列表包含两部分：参数名 参数类型 2.函数体：必须要有return语句，没有回报错。如果return语句没有放在函数体的最后也不报错，但不建议 3.begin end用法与存储过程相同， #调用语法 select 函数名（参数列表） #例 create function myfunc() returns int begin declare c int default 0; select count(*) into c from T; return c; end; select myfunc(); #查看函数 show create function 函数名; #删除函数 drop function 函数名; 参考：\nMysql-视图、变量、存储过程以及函数 Mysql语句大全 ","date":1617926400,"description":"","dir":"post\\Mysql\\","excerpt_html":null,"excerpt_text":null,"expirydate":-62135596800,"fuzzywordcount":1400,"html":null,"keywords":null,"kind":"page","lang":"en","lastmod":1617926400,"objectID":"feb668d3e60e2515c0a55c293e93c248","permalink":"http://localhost:1313/post/mysql/mysql%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86%E4%BA%8C/","publishdate":"2021-04-09T00:00:00Z","readingtime":3,"relpermalink":"/post/mysql/mysql%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86%E4%BA%8C/","section":"post","summary":"本文介绍Mysql操作和语句相关知识，包括增删改查、建表、函数、过程等相关知识。 1.操作 连接Mysql： mysql -h 主机地址 -u 用户名 -p 密码 本地连接：","tags":["Mysql"],"title":"Mysql相关知识（二）","type":"post","url":"/post/mysql/mysql%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86%E4%BA%8C/","weight":0,"wordcount":1385},{"author":null,"categories":["Code"],"content":" 本文介绍Mysql相关知识，主要包括Mysql的基础架构、事务、索引和日志等知识。\n基础架构 基础架构示意图：\n连接器：管理连接，权限验证 查询缓存：命中则直接返回结果 分析器：词法分析，语法分析 优化器：执行计划生成，索引选择 执行器：操作引擎，返回结果 存储引擎：存储数据，提供读写接口 Mysql可以分为Server层和存储引擎层，不同的存储引擎公用一个Server层，常见的存储引擎有InnoDB、MyISAM、Memory等，现在Mysql主要使用InnoDB做存储引擎。\n大多数情况下不要使用查询缓存，因为查询缓存失效非常频繁，只要对一个表有更新，这个表上的所有存储查询缓存都会被清空。\n日志 redo log：当有一条记录需要更新时，InnoDB引擎会先把记录写到redo log，并更新内存，这个时候更新就算完成了。同时，InnoDB引擎会在适当的时候，将这个操作记录更新到磁盘里面，而这个更新往往是在系统比较空闲的时候。\nredo log这种机制可以保证即使数据库发生异常重启，之前提交的记录也不会丢失，这个称为crash-safe。\nredo log是InnoDB存储引擎特有的日志，而Server层也有自己的日志，称为bin log。\nbin log：MySQL的Server层实现的，所有引擎都可以使用。binlog是逻辑日志，记录的是这个语句的原始逻辑，比如“给ID=2这一行的c字段加1 ”。\n两种日志的不同：\nredo log是InnoDB引擎特有的；binlog是MySQL的Server层实现的，所有引擎都可以使用。 redo log是物理日志，记录的是“在某个数据页上做了什么修改”；binlog是逻辑日志，记录的是这个语句的原始逻辑，比如“给ID=2这一行的c字段加1 ”。 redo log是循环写的，空间固定会用完；binlog是可以追加写入的。“追加写”是指binlog文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。 两阶段提交： 如果不使用“两阶段提交”，那么数据库的状态就有可能和用它的日志恢复出来的库的状态不一致。\n索引 索引是为了提高查询效率，实现索引的方式有很多种：\n哈希表 有序数组 搜索树 哈希表：适用于只有等值查询的场景，区间查询会搜索整个哈希表\n有序数组：等值查询和区间查询场景中的性能都很优秀，但插入和删除数据需要移动后面的记录，代价太大。只适用于静态存储引擎。\n搜索树：使用N叉树，减小树高，提高查询效率。\nInnoDB中，表是根据主键顺序以索引形式存放的，这种存储方式称为索引组织表，使用了B+树索引模型。\n主键索引的叶子节点存的是整行的数据(聚簇索引)，非主键索引的叶子节点内容是主键的值(二级索引)\n使用非主键索引的查询需要多扫描一颗索引树，称为回表，因此尽量使用主键查询。\n一个数据页满了，按照B+Tree算法，新增加一个数据页，叫做页分裂，会导致性能下降。空间利用率降低大概50%。当相邻的两个数据页利用率很低的时候会做数据页合并，合并的过程是分裂过程的逆过程。\n索引覆盖：可以直接提供查询结果，不需要回表，索引已经“覆盖了”查询需求。 索引覆盖可以减少树的搜索次数，提升查询性能。\n最左前缀：B+树这种索引结构，可以利用索引的“最左前缀”，来定位记录。\n索引下推：可以在索引遍历过程中，对索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数。\n","date":1617753600,"description":"","dir":"post\\Mysql\\","excerpt_html":null,"excerpt_text":null,"expirydate":-62135596800,"fuzzywordcount":1400,"html":null,"keywords":null,"kind":"page","lang":"en","lastmod":1617753600,"objectID":"e5a8bbe8db23b47e4fcdea4e5a167605","permalink":"http://localhost:1313/post/mysql/mysql%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86%E4%B8%80/","publishdate":"2021-04-07T00:00:00Z","readingtime":3,"relpermalink":"/post/mysql/mysql%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86%E4%B8%80/","section":"post","summary":"本文介绍Mysql相关知识，主要包括Mysql的基础架构、事务、索引和日志等知识。 基础架构 基础架构示意图： 连接器：管理连接，权限验证 查询缓存","tags":["Mysql"],"title":"Mysql相关知识（一）","type":"post","url":"/post/mysql/mysql%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86%E4%B8%80/","weight":0,"wordcount":1354},{"author":null,"categories":["Code"],"content":" 本文介绍Java线程池相关知识\n前言 线程池：线程池是一种基于池化思想管理线程的工具，经常出现再多线程服务器中。\n线程池解决的问题是什么： 线程池解决的核心问题就是资源管理问题。在并发环境下，系统不能确定任意时刻，有多少任务需要执行，有多少资源需要投入。会存在下列问题：\n频繁申请/销毁资源和调度资源，将带来额外的消耗，可能会非常巨大。 对资源无限申请缺乏抑制手段，可能会引发系统资源耗尽的风险。 系统无法合理管理内部的资源分布，会降低系统的稳定性。 为解决资源分配这个问题，线程池采用了“池化”（Pooling）思想。池化，顾名思义，是为了最大化收益并最小化风险，而将资源统一在一起管理的一种思想。\n线程池的优点：\n降低资源消耗：通过池化技术重复利用已创建的线程，降低线程创建和销毁造成的损耗。 提高响应速度：任务到达时，无需等待线程创建即可立即执行。 提高线程的可管理性：线程是稀缺资源，如果无限制创建，不仅会消耗系统资源，还会因为线程的不合理分布导致资源调度失衡，降低系统的稳定性。使用线程池可以进行统一的分配、调优和监控。 提供更多更强大的功能：线程池具备可拓展性，允许开发人员向其中增加更多的功能。比如延时定时线程池ScheduledThreadPoolExecutor，就允许任务延期执行或定期执行。 TheadPoolExecutor源码设计 Java中线程池核心实现类是ThreadPoolExecutor,它的继承关系： 运行机制： ThreadPoolExecutor内部实际上构建了一个生产者消费者模型，将线程和任务两者解耦，并不直接关联。线程池的运行主要分为两部分：任务管理、线程管理。任务管理部分充当生产者的角色，当任务提交后，线程池会判断该任务后续的流转：\n直接申请线程执行任务 缓存到队列中等待线程执行 拒绝该任务 线程管理部分是消费者，它们被统一维护在线程池内，根据任务请求进行线程的分配，当线程执行完任务后会继续获取新的任务去执行，最终当线程获取不到任务时，线程就会被回收。\n运行状态： ThreadPoolExecutor的运行状态有5种，分别为：\n运行状态 状态描述 RUNNING 能接受新提交的任务，且也能处理阻塞队列中的任务 SHUTDOWN 不再接受新提交的任务，但是能处理阻塞队列中的任务 STOP 不能接受新提交的任务，也不能处理阻塞队列中的任务，会中断正在处理任务的线程 TIDYING 所有任务都已经终止了，workerCount（有效线程数）为0 TERMINATED 在terminated()方法执行完后进入该状态 其生命周期转换如下入所示： 任务调度： 当使用execute方法提交一个任务到ThreadPoolExecutor时，会检查现在的线程池运行状态、运行线程数、运行策略，决定接下来执行的流程，是直接申请线程执行，或是缓冲到队列中执行，亦或是直接拒绝该任务。其执行过程如下：\n首先检测线程池运行状态，如果不是RUNNING，则直接拒绝，线程池要保证在RUNNING的状态下执行任务。 如果workerCount \u0026lt; corePoolSize，则创建并启动一个线程来执行新提交的任务。 如果workerCount \u0026gt;= corePoolSize，则判断任务阻塞队列是否已满，若未蛮则将任务添加到该阻塞队列，若已满则判断工作线程数是否大于最大线程数，如果小于，则创建并启动一个线程来执行新提交的任务，如果大于则根据拒绝策略来处理该任务，默认的处理方式是直接抛异常。 任务申请： 由上文的任务分配部分可知，任务的执行有两种可能：一种是任务直接由新创建的线程执行。另一种是线程从任务队列中获取任务然后执行，执行完任务的空闲线程会再次去从队列中申请任务再去执行。第一种情况仅出现在线程初始创建的时候，第二种是线程获取任务绝大多数的情况。 任务拒绝： 拒绝策略是一个接口：\npublic interface RejectedExecutionHandler { void rejectedExecution(Runnable r, ThreadPoolExecutor executor); } 用户可以通过实现这个接口去定制拒绝策略，也可以选择JDK提供的四种已有拒绝策略，其特点如下： Worker线程： Worker这个工作线程，实现了Runnable接口，并持有一个线程Thread，一个初始化的任务firstTask。thread是在调用构造方法时通过ThreadFactory来创建的线程，firstTask用它来保存传入的第一个任务，这个任务可以有也可以为null。如果这个值是非空的，那么线程就会在启动初期立即执行这个任务，也就对应核心线程创建时的情况；如果这个值是null，那么就需要创建一个线程去执行任务列表（workQueue）中的任务，也就是非核心线程的创建。\n线程池需要管理线程的生命周期，需要在线程长时间不运行的时候进行回收。线程池使用一张Hash表去持有线程的引用，这样可以通过添加引用、移除引用这样的操作来控制线程的生命周期。这个时候重要的就是如何判断线程是否在运行。\nWorker是通过继承AQS，使用AQS来实现独占锁这个功能。没有使用可重入锁ReentrantLock，而是使用AQS，为的就是实现不可重入的特性去反应线程现在的执行状态。\naddWorker增加工作线程：\nprivate boolean addWorker(Runnable firstTask, boolean core) { } addWorker方法有两个参数：firstTask、core。firstTask参数用于指定新增的线程执行的第一个任务，该参数可以为空；core参数为true表示在新增线程时会判断当前活动线程数是否少于corePoolSize，false表示新增线程前需要判断当前活动线程数是否少于maximumPoolSize\n执行流程： 参考：\nJava线程池实现原理 ","date":1617235200,"description":"","dir":"post\\Java Concurrent\\","excerpt_html":null,"excerpt_text":null,"expirydate":-62135596800,"fuzzywordcount":2300,"html":null,"keywords":null,"kind":"page","lang":"en","lastmod":1617235200,"objectID":"189032ae2ae464c008c6fdae04a66b47","permalink":"http://localhost:1313/post/java-concurrent/java%E7%BA%BF%E7%A8%8B%E6%B1%A0/","publishdate":"2021-04-01T00:00:00Z","readingtime":5,"relpermalink":"/post/java-concurrent/java%E7%BA%BF%E7%A8%8B%E6%B1%A0/","section":"post","summary":"本文介绍Java线程池相关知识 前言 线程池：线程池是一种基于池化思想管理线程的工具，经常出现再多线程服务器中。 线程池解决的问题是什么： 线程池解","tags":["Java并发"],"title":"Java线程池相关知识","type":"post","url":"/post/java-concurrent/java%E7%BA%BF%E7%A8%8B%E6%B1%A0/","weight":0,"wordcount":2231},{"author":null,"categories":["Code"],"content":" 本文介绍二叉树相关知识\n定义：树的任意节点至多包含两棵子树。\n数据存储：\n链表 数组 链表方式定义\npublic class TreeNode { public int val; public TreeNode left; public TreeNode right; public TreeNode(int val) { this.val = val; } public TreeNode(int val, TreeNode left, TreeNode right) { this.val = val; this.left = left; this.right = right; } } 二叉树的遍历\n递归：\n//前序遍历 public void preOrder(TreeNode root){ if (root == null){ return; } System.out.println(root.val); preOrder(root.left); preOrder(root.right); } //中序遍历 public void inOrder(TreeNode root){ if (root == null){ return; } inOrder(root.left); System.out.println(root.val); inOrder(root.right); } //后序遍历 public void postOrder(TreeNode root){ if (root == null){ return; } inOrder(root.left); inOrder(root.right); System.out.println(root.val); } //层序遍历 public void BFSOrder(TreeNode root){ if (root == null){ return; } Queue\u0026lt;TreeNode\u0026gt; queue = new LinkedList\u0026lt;TreeNode\u0026gt;(); TreeNode temp = null; queue.offer(root); while (!queue.isEmpty()){ temp = queue.poll(); System.out.println(temp.val); if (temp.left != null){ queue.offer(temp.left); } if (temp.right != null){ queue.offer(temp.right); } } } 迭代：\n//前序 public List\u0026lt;Integer\u0026gt; preorderTraversal(TreeNode root) { List\u0026lt;Integer\u0026gt; list = new ArrayList\u0026lt;\u0026gt;(); if (root == null) { return list; } Deque\u0026lt;TreeNode\u0026gt; deque = new LinkedList\u0026lt;\u0026gt;(); while (!deque.isEmpty() || root != null){ if (root != null){ list.add(root.val); deque.push(root); root =root.left; }else { TreeNode tmp = deque.pop(); root = tmp.right; } } return list; } //中序 public List\u0026lt;Integer\u0026gt; preorderTraversal(TreeNode root) { List\u0026lt;Integer\u0026gt; list = new ArrayList\u0026lt;\u0026gt;(); if (root == null) { return list; } Deque\u0026lt;TreeNode\u0026gt; deque = new LinkedList\u0026lt;\u0026gt;(); while (!deque.isEmpty() || root != null){ if (root != null){ deque.push(root); root =root.left; }else { TreeNode tmp = deque.pop(); list.add(tmp.val); root = tmp.right; } } return list; } //后序 反转前序操作 先添加队首添加节点 先循环右子树 再循环左子树 public List\u0026lt;Integer\u0026gt; postorderTraversal(TreeNode root) { List\u0026lt;Integer\u0026gt; list = new ArrayList\u0026lt;\u0026gt;(); if (root == null) { return list; } Deque\u0026lt;TreeNode\u0026gt; deque = new LinkedList\u0026lt;\u0026gt;(); while (!deque.isEmpty() || root != null){ if (root != null){ deque.push(root); list.add(0,root.val); root =root.right; }else { TreeNode tmp = deque.pop(); root = tmp.left; } } return list; } 二叉搜索树 （BST）\n定义：\n若任意节点的左子树不空，则左子树上所有结点的值均小于它的根结点的值； 任意节点的右子树不空，则右子树上所有结点的值均大于它的根结点的值； 任意节点的左、右子树也分别为二叉查找树。 没有键值相等的节点。 链表方式实现：\npublic class BSTree\u0026lt;T extends Comparable\u0026lt;T\u0026gt;\u0026gt; { private BSTNode\u0026lt;T\u0026gt; mRoot; // 根结点 public class BSTNode\u0026lt;T extends Comparable\u0026lt;T\u0026gt;\u0026gt; { public T key; // 关键字(键值) public BSTNode\u0026lt;T\u0026gt; left; // 左孩子 public BSTNode\u0026lt;T\u0026gt; right; // 右孩子 public BSTNode\u0026lt;T\u0026gt; parent; // 父结点 public BSTNode(T key, BSTNode\u0026lt;T\u0026gt; parent, BSTNode\u0026lt;T\u0026gt; left, BSTNode\u0026lt;T\u0026gt; right) { this.key = key; this.parent = parent; this.left = left; this.right = right; } } } ","date":1617148800,"description":"","dir":"post\\Algorithm\\","excerpt_html":null,"excerpt_text":null,"expirydate":-62135596800,"fuzzywordcount":600,"html":null,"keywords":null,"kind":"page","lang":"en","lastmod":1617148800,"objectID":"4d3946f8724082832884cbf980f970b3","permalink":"http://localhost:1313/post/algorithm/%E4%BA%8C%E5%8F%89%E6%A0%91/","publishdate":"2021-03-31T00:00:00Z","readingtime":2,"relpermalink":"/post/algorithm/%E4%BA%8C%E5%8F%89%E6%A0%91/","section":"post","summary":"本文介绍二叉树相关知识 定义：树的任意节点至多包含两棵子树。 数据存储： 链表 数组 链表方式定义 public class TreeNode { public int val; public TreeNode left; public TreeNode right; public TreeNode(int val) { this.val = val; } public TreeNode(int val, TreeNode left,","tags":["Algorithm"],"title":"二叉树相关知识","type":"post","url":"/post/algorithm/%E4%BA%8C%E5%8F%89%E6%A0%91/","weight":0,"wordcount":587},{"author":null,"categories":["Code"],"content":" 本文简单介绍Spring bean的生命周期相关知识\nSpring IOC 简介 IOC：Inversion of Control,即控制反转。传统Java程序中，我们是自己创建对象，而Spring IoC 是有一个容器来保管我们创建的对象，即将对象交给Spring 容器进行管理。\nbean：在 Spring 中，构成应用程序主干并由 Spring IoC 容器管理的对象称为 bean。 bean 是一个由 Spring IoC 容器实例化，组装和管理的对象。\nBeanDefinition：bean的定义类，用来存储bean的所有属性和方法。\nBeanFactory：BeanFactory接口是Spring IoC的基础。\nApplicationContext：是BeanFactory的子接口，同时还继承了其他的接口，是相对比较高级的 IoC 容器实现。\nbean生命周期核心流程：\n实例化\tInstantiation 注入属性\tPopulate 初始化\tInitialization 销毁\tDestruction bean生命周期经历了各种方法的调用，可以分为几类：\nBean自身的方法：这个包括了Bean本身调用的方法和通过配置文件中的init-method和destroy-method指定的方法 Bean级生命周期接口方法：这个包括了BeanNameAware、BeanFactoryAware、InitializingBean和DiposableBean这些接口的方法 容器级生命周期接口方法：这个包括了InstantiationAwareBeanPostProcessor 和 BeanPostProcessor 这两个接口实现，一般称它们的实现类为“后置处理器”。 Spring Bean生命周期 测试代码：bean生命周期\n扩展点 4个后置处理器：\nInstantiationAwareBeanPostProcessor SmartInstantiationAwareBeanPostProcessor MergedBeanDefinitionPostProcessor SmartInitializingSingleton 影响多个Bean：\nBeanPostProcessor ： postProcessBeforeInitialization postProcessAfterInitialization InstantiationAwareBeanPostProcessor postProcessBeforeInstantiation postProcessAfterInstantiation postProcessProperties MergedBeanDefinitionPostProcessor postProcessMergedBeanDefinition SmartInstantiationAwareBeanPostProcessor determineCandidateConstructors getEarlyBeanReference 影响单个Bean：\nBeanNameAware BeanClassLoaderAware BeanFactoryAware EnvironmentAware EmbeddedValueResolverAware ApplicationContextAware InitializingBean DisposableBean 参考：\nspring bean生命周期 ","date":1617062400,"description":"","dir":"post\\Spring\\","excerpt_html":null,"excerpt_text":null,"expirydate":-62135596800,"fuzzywordcount":700,"html":null,"keywords":null,"kind":"page","lang":"en","lastmod":1617062400,"objectID":"b2c4d15977ace2c7950098cf21edf8f2","permalink":"http://localhost:1313/post/spring/spring-bean%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F/","publishdate":"2021-03-30T00:00:00Z","readingtime":2,"relpermalink":"/post/spring/spring-bean%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F/","section":"post","summary":"本文简单介绍Spring bean的生命周期相关知识 Spring IOC 简介 IOC：Inversion of Control,即控制反转。传统Java程序中，我们是","tags":["Spring"],"title":"Spring bean的生命周期相关知识","type":"post","url":"/post/spring/spring-bean%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F/","weight":0,"wordcount":696},{"author":null,"categories":["Code"],"content":" 本文介绍Java垃圾回收相关知识\n判断一个对象是否可以被回收 回收对象首先需要判断这个对象是否可以被回收，Java虚拟机采用可达性分析算法判断。\n引用计数算法\n给对象添加一个引用计数器，当对象增加一个引用时计数器加一，减少一个引用时计数器减一。引用计数为 0 的对象可被回收。\n两个对象出现循环引用的情况下，此时引用计数器永远不为 0，导致无法对它们进行回收。\n正因为循环引用的存在，因此 Java 虚拟机不使用引用计数算法。\n可达性分析算法 通过 GC Roots 作为起始点进行搜索，能够到达到的对象都是存活的，不可达的对象可被回收。\nJava 虚拟机使用该算法来判断对象是否可被回收，在 Java 中 GC Roots 一般包含以下内容:\n虚拟机栈中引用的对象 本地方法栈中引用的对象 方法区中类静态属性引用的对象 方法区中常量引用的对象 方法区的回收\n方法区的垃圾回收主要包括两部分：废弃的变量和不再使用的类型。\n判断一个常量是否废弃：当没有其他对象引用这个常量时，Java虚拟机会对这个常量进行回收。\n判断一个类型是否属于不再使用的类：\n该类所有的实例都已经被回收，也就是堆中不存在该类的任何实例。 加载该类的 ClassLoader 已经被回收。 该类对应的 Class 对象没有在任何地方被引用，也就无法在任何地方通过反射访问该类方法。 引用类型 强引用 被强引用关联的对象不会被回收。\n使用 new 一个新对象的方式来创建强引用。\nObject obj = new Object(); 软引用 被软引用关联的对象，只有在虚拟机内存不足时才会被回收\n使用 SoftReference 类来创建软引用。\nObject obj = new Object(); SoftReference\u0026lt;Object\u0026gt; sf = new SoftReference\u0026lt;Object\u0026gt;(obj); obj = null; // 使对象只被软引用关联 弱引用 被弱引用关联的对象，在虚拟机下一次GC时会被回收\n使用 WeakReference 类来实现弱引用。\nObject obj = new Object(); WeakReference\u0026lt;Object\u0026gt; sf = new WeakReference\u0026lt;Object\u0026gt;(obj); obj = null; // 使对象只被软引用关联 虚引用 又称为幽灵引用或者幻影引用。\n一个对象是否有虚引用的存在，完全不会对其生存时间构成影响，也无法通过虚引用取得一个对象。\n为一个对象设置虚引用关联的唯一目的就是能在这个对象被回收时收到一个系统通知。\n使用 PhantomReference 来实现虚引用。\nObject obj = new Object(); PhantomReference\u0026lt;Object\u0026gt; pf = new PhantomReference\u0026lt;Object\u0026gt;(obj); obj = null; 垃圾回收算法 标记-清除 首先标记所有需要回收的对象，标记完成后进行统一的回收，也可以反过来标记存活的对象，统一回收所有未被标记的对象。\n缺点：\n执行效率不稳定，标记和清除过程效率都不高； 内存空间的碎片化 标记-整理 在标记-清楚算法的基础上，让所有存活的对象都向一端移动，然后直接清理掉端边界以外的内存。\n标记-复制 将内存划分为大小相等的两块，每次只使用其中一块，当这一块内存用完了就将还存活的对象复制到另一块上面，然后再把使用过的内存空间进行一次清理。\n主要不足是只使用了内存的一半。\n现在的商业虚拟机都采用标记-复制算法来回收新生代，但是并不是将新生代分为大小相等的两块，而是分为一块较大的Eden空间和两块较小的Survivor空间。\n每次使用Eden空间和其中一块Survivor，在回收时，在回收时，将 Eden 和 Survivor 中还存活着的对象一次性复制到另一块 Survivor 空间上，最后清理 Eden 和使用过的那一块 Survivor。\nHotSpot 虚拟机的 Eden 和 Survivor 的大小比例默认为 8:1，保证了内存的利用率达到 90%。如果每次回收有多于 10% 的对象存活，那么一块 Survivor 空间就不够用了，此时需要依赖于老年代进行分配担保，也就是借用老年代的空间存储放不下的对象\n分代收集器\n现在的商业虚拟机采用分代收集算法，它根据对象存活周期将内存分为几块，不同块采用适当的收集算法。\n一般将堆分为新生代和老年代：\n新生代使用 标记-复制 算法 老年代使用 标记-整理 或者 标记-清除 算法 内存分配与回收策略 Minor GC 和 Full GC\nMinor GC发生在新生代上，因为新生代上对象存活时间很短，所以Minor GC会频繁执行，执行的速度一般也很快。 Full GC发生在老年代，老年代对象存活时间长，因此Full GC很少执行，执行速度也比Minor GC慢很多 内存分配策略\n对象优先在Eden上分配 大对象直接进入老年代 长期存活的对象进入老年代 动态对象年龄判定 空间分配担保 回收条件\n对于 Minor GC，其触发条件非常简单，当 Eden 空间满时，就将触发一次 Minor GC。而 Full GC 则相对复杂，有以下条件:\n调用 System.gc() 老年代空间不足 空间分配担保失败 JDK 1.7 及以前的永久代空间不足 Concurrent Mode Failure 参考\n深入理解Java虚拟机 Java 垃圾回收基础 ","date":1616630400,"description":"","dir":"post\\JVM\\","excerpt_html":null,"excerpt_text":null,"expirydate":-62135596800,"fuzzywordcount":1700,"html":null,"keywords":null,"kind":"page","lang":"en","lastmod":1616630400,"objectID":"207ada61e6cf2e150f96adc195a91613","permalink":"http://localhost:1313/post/jvm/java%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/","publishdate":"2021-03-25T00:00:00Z","readingtime":4,"relpermalink":"/post/jvm/java%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/","section":"post","summary":"本文介绍Java垃圾回收相关知识 判断一个对象是否可以被回收 回收对象首先需要判断这个对象是否可以被回收，Java虚拟机采用可达性分析算法判断。","tags":["JVM"],"title":"Java垃圾回收","type":"post","url":"/post/jvm/java%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/","weight":0,"wordcount":1664},{"author":null,"categories":["Code"],"content":" 本文主要介绍JVM内存结构相关知识，需要注意JVM内存结构和Java内存模型是两个概念。\n运行时数据区 Java虚拟机在执行程序时会把它所管理的内存划分为若干个不同的数据区域。这些区域有各自的用途，以及创建和销毁的时间，有的区域随着虚拟机进程的启动而一直存在，有些区域则依赖用户线程的启动和结束而建立和销毁。\n下图是 JVM 整体架构，中间部分就是 Java 虚拟机定义的各种运行时数据区域。 在这里插入图片描述 下面介绍下这些内存结构\n程序计数器 程序计数器（program counter register）是一块较小的内存空间，可以看作是当前线程所执行的字节码的行号指令器。\n在 JVM 规范中，每个线程都有它自己的程序计数器，是线程私有的，生命周期与线程的生命周期一致。 在任何一个时刻，一个处理器都只会处理一条线程中的指令，因此为了线程切换后能恢复到正确的执行位置，每条线程都要有一个独立的程序计数器，各条线程之间的计数器互不影响，我们称这类区域为“线程私有”区域。 它是程序控制流的指示器，分支、循环、跳转、异常处理、线程恢复等基础功能都需要依赖这个计数器来完成 字节码解释器工作时就是通过改变这个计数器的值来选取下一条需要执行的字节码指令 它是唯一一个在 JVM 规范中没有规定任何 OutOfMemoryError 情况的区域 虚拟机栈 与程序计数器一样，Java虚拟机栈（Java Virtual Machine Stacks）也是线程私有的，它的生命周期与线程相同。\nJava虚拟机栈描述的是Java方法执行的线程内存模型：每个方法被执行时，Java虚拟机都会创建一个栈帧用于存储局部变量表、操作数栈、动态链接、方法出口等信息。\n栈不存在垃圾回收的问题：进栈和出栈，出栈相当于释放内存。\n栈中可能出现的异常：\n如果采用固定大小的 Java 虚拟机栈，那每个线程的 Java 虚拟机栈容量可以在线程创建的时候独立选定。如果线程请求分配的栈容量超过 Java 虚拟机栈允许的最大容量，Java 虚拟机将会抛出一个 StackOverflowError 异常 如果 Java 虚拟机栈可以动态扩展，并且在尝试扩展的时候无法申请到足够的内存，或者在创建新的线程时没有足够的内存去创建对应的虚拟机栈，那 Java 虚拟机将会抛出一个OutOfMemoryError异常 可以通过参数-Xss来设置线程的最大栈空间，栈的大小直接决定了函数调用的最大可达深度。\n栈帧是虚拟机栈的最小单位：在用idea进行debug时，看到的就是一个个栈帧。\n栈帧的内部结构：\n局部变量表：主要用于存储方法参数和定义在方法体内的局部变量 操作数栈：主要用于保存计算过程的中间结果，同时作为计算过程中变量临时的存储空间 动态链接：指向运行时常量池的方法引用 方法返回地址：方法正常退出或异常退出的地址 一些附加信息 本地方法栈 本地方法栈与虚拟机栈所发挥的作用是非常相似的，其区别是虚拟机栈为虚拟机执行Java方法服务，而本地方法栈则是为虚拟机使用到的本地方法服务。\n本地方法栈也是线程私有的。\n栈是运行时的单位，而堆是存储的单位。\n栈解决程序的运行问题，即程序如何执行，或者说如何处理数据。堆解决的是数据存储的问题，即数据怎么放、放在哪。\n堆 对Java程序来说，Java堆是虚拟机所管理的内存中最大的一块。Java堆是被所有线程所共享的一块内存区域，在虚拟机启动时创建。\n此内存区域的唯一目的就是存放对象实例\n为了进行高效的垃圾回收，虚拟机把堆内存逻辑上划分成三块区域（分代的唯一理由就是优化 GC 性能）：\n新生代（年轻代）：新对象和没达到一定年龄的对象都在新生代 老年代（养老区）：被长时间使用的对象，老年代的内存空间应该要比年轻代更大 元空间（JDK1.8 之前叫永久代）：像一些方法中的操作临时对象等，JDK1.8 之前是占用 JVM 内存，JDK1.8 之后直接使用物理内存 Java 虚拟机规范规定，Java 堆可以是处于物理上不连续的内存空间中，只要逻辑上是连续的即可，像磁盘空间一样。实现时，既可以是固定大小，也可以是可扩展的，主流虚拟机都是可扩展的（通过 -Xmx 和 -Xms 控制），如果堆中没有完成实例分配，并且堆无法再扩展时，就会抛出 OutOfMemoryError 异常。 年轻代 (Young Generation)\n年轻代是所有新对象创建的地方。当填充年轻代时，执行垃圾收集。这种垃圾收集称为 Minor GC。年轻一代被分为三个部分——伊甸园（Eden Memory）和两个幸存区（Survivor Memory，被称为from/to或s0/s1），默认比例是8:1:1\n大多数新创建的对象都位于 Eden 内存空间中 当 Eden 空间被对象填充时，执行Minor GC，并将所有幸存者对象移动到一个幸存者空间中 Minor GC 检查幸存者对象，并将它们移动到另一个幸存者空间。所以每次，总有一个幸存者空间是空的 经过多次 GC 循环后存活下来的对象被移动到老年代。通常，这是通过设置年轻一代对象的年龄阈值来实现的，然后他们才有资格提升到老一代 老年代(Old Generation)\n旧的一代内存包含那些经过许多轮小型 GC 后仍然存活的对象。通常，垃圾收集是在老年代内存满时执行的。老年代垃圾收集称为 主GC（Major GC），通常需要更长的时间。\n大对象直接进入老年代（大对象是指需要大量连续内存空间的对象）。这样做的目的是避免在 Eden 区和两个Survivor 区之间发生大量的内存拷贝\n方法区 方法区与堆一样是被各个线程所共享的区域，它用于存储已被虚拟机加载的类型信息、常量、静态变量、即时编译器编译后的代码缓存等数据。\n虽然 Java 虚拟机规范把方法区描述为堆的一个逻辑部分，但是它却有一个别名叫 Non-Heap（非堆），目的应该是与 Java 堆区分开。\n运行时常量池（Runtime Constant Pool）是方法区的一部分。Class 文件中除了有类的版本/字段/方法/接口等描述信息外，还有一项信息是常量池（Constant Pool Table），用于存放编译期生成的各种字面量和符号引用，这部分内容将类在加载后进入方法区的运行时常量池中存放。运行期间也可能将新的常量放入池中，这种特性被开发人员利用得比较多的是 String.intern()方法。受方法区内存的限制，当常量池无法再申请到内存时会抛出 OutOfMemoryError 异常。\n除了和Java堆一样不需要连续的内存和可以选择内存大小或者可扩展外，方法区还可以选择不进行垃圾收集。这区域内存的回收目标主要是常量池的回收和对类型的卸载。\n方法区（method area）只是 JVM 规范中定义的一个概念，用于存储类信息、常量池、静态变量、JIT编译后的代码等数据，并没有规定如何去实现它，不同的厂商有不同的实现。而永久代（PermGen）是 Hotspot 虚拟机特有的概念， Java8 的时候又被元空间取代了，永久代和元空间都可以理解为方法区的落地实现。\n永久代物理是堆的一部分，和新生代，老年代地址是连续的（受垃圾回收器管理），而元空间存在于本地内存（我们常说的堆外内存，不受垃圾回收器管理），这样就不受 JVM 限制了，也比较难发生OOM（都会有溢出异常）\n所以对于方法区，Java8 之后的变化：\n移除了永久代（PermGen），替换为元空间（Metaspace）； 永久代中的 class metadata 转移到了 native memory（本地内存，而不是虚拟机）； 永久代参数 （PermSize MaxPermSize） -\u0026gt; 元空间参数（MetaspaceSize MaxMetaspaceSize） 栈、堆、方法区的交互关系 参考\n深入理解Java虚拟机 JVM 基础 - JVM 内存结构 ","date":1616544000,"description":"","dir":"post\\JVM\\","excerpt_html":null,"excerpt_text":null,"expirydate":-62135596800,"fuzzywordcount":2900,"html":null,"keywords":null,"kind":"page","lang":"en","lastmod":1616544000,"objectID":"ad8af84fb9a16aabb591132e508c2e82","permalink":"http://localhost:1313/post/jvm/java%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%86%85%E5%AD%98%E7%BB%93%E6%9E%84/","publishdate":"2021-03-24T00:00:00Z","readingtime":6,"relpermalink":"/post/jvm/java%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%86%85%E5%AD%98%E7%BB%93%E6%9E%84/","section":"post","summary":"本文主要介绍JVM内存结构相关知识，需要注意JVM内存结构和Java内存模型是两个概念。 运行时数据区 Java虚拟机在执行程序时会把它所管理的","tags":["JVM"],"title":"深入了解JVM内存结构","type":"post","url":"/post/jvm/java%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%86%85%E5%AD%98%E7%BB%93%E6%9E%84/","weight":0,"wordcount":2895},{"author":null,"categories":["Code"],"content":" 本文介绍Spring事务相关的知识，包括事务隔离级别和事务传播特性。\n事务 事务是逻辑上的一组操作，要么都执行，要么都不执行。我自己的理解是，数据库操作的最小单位，要么成功，要么失败。\n特性：ACID\n原子性（Atomicity）：事务是最小的执行单位，不允许分割。事务的原子性确保动作要么全部完成，要么全部失败回滚。 一致性（Consistency）：数据库在事务执行前后都保持一致性状态。在一致性状态下，所有事务对一个数据的读取结果都是相同的。 隔离性（Isolation）：一个事务所做的修改在最终提交以前，对其他事务是不可见的。 持久性（Durability）：一旦事务提交，则其所做的修改将会永远保存到数据库中。即使系统发生崩溃，事务执行的结果也不能丢失。 事务隔离级别 在并发环境下，事务的隔离性很难得到保证，因此会出现很多并发一致性的问题。\n丢失修改 T1和T2 两个事务同时对一个数据进行修改，T1修改之后，T2又修改，T2的修改覆盖了T1的修改。\n读脏数据 T1 修改一个数据，T2 随后读取这个数据。如果 T1 撤销了这次修改，那么 T2 读取的数据是脏数据。\n不可重复读 T2 读取一个数据，T1 对该数据做了修改。如果 T2 再次读取这个数据，此时读取的结果和第一次读取的结果不同。\n幻读 T1 读取某个范围的数据，T2 在这个范围内插入新的数据，T1 再次读取这个范围的数据，此时读取的结果和和第一次读取的结果不同。\n不可重复读的重点是修改，幻读的重点在于新增或者删除。\n在Spring中，TransactionDefinition 接口中定义了五个表示隔离级别的常量：\nTransactionDefinition.ISOLATION_DEFAULT：使用数据库默认的事务隔离级别 TransactionDefinition.ISOLATION_READ_UNCOMMITTED（未提交读）：最低的隔离级别，允许读取尚未提交的数据变更，可能会导致脏读、幻读或不可重复读 TransactionDefinition.ISOLATION_READ_COMMITTED（读已提交）：允许读取并发事务已经提交的数据，可以阻止脏读，但是幻读或不可重复读仍有可能发生 TransactionDefinition.ISOLATION_REPEATABLE_READ（可重复读）：对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改，可以阻止脏读和不可重复读，但幻读仍有可能发生。 TransactionDefinition.ISOLATION_SERIALIZABLE（串行化）：最高的隔离级别，完全服从ACID的隔离级别。所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，该级别可以防止脏读、不可重复读以及幻读。但是这将严重影响程序的性能。通常情况下也不会用到该级别。 事务的传播机制 当事务方法被另一个事务方法调用时，必须指定事务应该如何传播。例如：方法可能继续在现有事务中运行，也可能开启一个新事务，并在自己的事务中运行。在TransactionDefinition定义中包括了如下几个表示传播行为的常量：\n支持当前事务：\nTransactionDefinition.PROPAGATION_REQUIRED：如果当前存在事务，则加入该事务；如果当前没有事务，则创建一个新的事务。 TransactionDefinition.PROPAGATION_SUPPORTS： 如果当前存在事务，则加入该事务；如果当前没有事务，则以非事务的方式继续运行。 TransactionDefinition.PROPAGATION_MANDATORY： 如果当前存在事务，则加入该事务；如果当前没有事务，则抛出异常。（mandatory：强制性） 不支持当前事务：\nTransactionDefinition.PROPAGATION_REQUIRES_NEW： 创建一个新的事务，如果当前存在事务，则把当前事务挂起。 TransactionDefinition.PROPAGATION_NOT_SUPPORTED： 以非事务方式运行，如果当前存在事务，则把当前事务挂起。 TransactionDefinition.PROPAGATION_NEVER： 以非事务方式运行，如果当前存在事务，则抛出异常。 其他情况：\nTransactionDefinition.PROPAGATION_NESTED： 如果当前存在事务，则创建一个事务作为当前事务的嵌套事务来运行；如果当前没有事务，则创建一个新的事务。 参考\nSQL DB - 数据库系统核心知识点 Spring事务管理详解 Spring事务传播机制详解 ","date":1616457600,"description":"","dir":"post\\Spring\\","excerpt_html":null,"excerpt_text":null,"expirydate":-62135596800,"fuzzywordcount":1900,"html":null,"keywords":null,"kind":"page","lang":"en","lastmod":1616457600,"objectID":"12b2aa6bd66d66b3a5226123a70e47d3","permalink":"http://localhost:1313/post/spring/spring%E4%BA%8B%E5%8A%A1/","publishdate":"2021-03-23T00:00:00Z","readingtime":4,"relpermalink":"/post/spring/spring%E4%BA%8B%E5%8A%A1/","section":"post","summary":"本文介绍Spring事务相关的知识，包括事务隔离级别和事务传播特性。 事务 事务是逻辑上的一组操作，要么都执行，要么都不执行。我自己的理解是，数","tags":["Spring"],"title":"Spring 事务相关知识","type":"post","url":"/post/spring/spring%E4%BA%8B%E5%8A%A1/","weight":0,"wordcount":1895},{"author":null,"categories":["Code"],"content":" 本文简单介绍Java类加载相关知识\nJava类的生命周期 一个类从被加载到虚拟机内存到卸载出虚拟机内存，它的生命周期会经历：加载、验证、准备、解析、初始化、使用、卸载这七个阶段。其中验证、准备、解析三个部分统称为连接。 其中加载、验证、准备、初始化、卸载这五个阶段的顺序是确定的，类型的加载过程必须按这种顺序，而解析阶段却不一定，它在某些情况下可以在初始化之后再开始，这是为了支持Java语言运行时绑定特性（也称动态绑定）。\n另外注意这里的几个阶段是按顺序开始，而不是按顺序进行或完成，因为这些阶段通常都是互相交叉地混合进行的，通常在一个阶段执行的过程中调用或激活另一个阶段。\n加载 加载是整个类加载的过程中的一个阶段，在加载阶段，Java虚拟机需要完成三件事：\n通过一个类的全限定名来获取定义此类的二进制字节流 将这个字节流所代表的静态存储结构转化为方法区的运行时数据结构 在内存中生成一个代表这个类的java.lang.Class对象，作为方法区这个类的各种数据的访问入口 相对于类加载的其他阶段而言，加载阶段(准确地说，是加载阶段获取类的二进制字节流的动作)是可控性最强的阶段，因为开发人员既可以使用系统提供的类加载器来完成加载，也可以自定义自己的类加载器来完成加载。\n加载阶段完成后，虚拟机外部的二进制字节流就按照虚拟机所需的格式存储在方法区之中，而且在Java堆中也创建一个java.lang.Class类的对象，这样便可以通过该对象访问方法区中的这些数据。\n类加载器并不需要等到某个类被“首次主动使用”时再加载它，JVM规范允许类加载器在预料某个类将要被使用时就预先加载它，如果在预先加载的过程中遇到了.class文件缺失或存在错误，类加载器必须在程序首次主动使用该类时才报告错误(LinkageError错误)如果这个类一直没有被程序主动使用，那么类加载器就不会报告错误。\n验证 验证是连接的第一步，这一阶段的目的是确保Class文件的字节流信息符合规范，保证这些信息被当作代码运行后不会危害虚拟机自身的安全。验证阶段大致会完成4个阶段的检验动作:\n文件格式验证： 验证字节流是否符合Class文件格式的规范；例如: 是否以0xCAFEBABE开头、主次版本号是否在当前虚拟机的处理范围之内、常量池中的常量是否有不被支持的类型。 元数据验证： 对字节码描述的信息进行语义分析(注意: 对比javac编译阶段的语义分析)，以保证其描述的信息符合Java语言规范的要求；例如: 这个类是否有父类，除了java.lang.Object之外。 字节码验证：通过数据流和控制流分析，确定程序语义是合法的、符合逻辑的。 符号引用验证：确保解析动作能正确执行。 准备 准备阶段是正式为类中定义的变量（即静态变量， 被static修饰的变量） 分配内存并设置类变量初始值的阶段， 这些内存都将在方法区中分配。\n这时候进行内存分配的仅包括类变量(static)，而不包括实例变量，实例变量会在对象实例化时随着对象一块分配在Java堆中。\n这里所设置的初始值通常情况下是数据类型默认的零值(如0、0L、null、false等)，而不是被在Java代码中被显式地赋予的值。\n假设一个类变量的定义为: public static int value = 3；那么变量value在准备阶段过后的初始值为0，而不是3，因为这时候尚未开始执行任何Java方法，而把value赋值为3的put static指令是在程序编译后，存放于类构造器()方法之中的，所以把value赋值为3的动作将在初始化阶段才会执行。\n解析 解析阶段是虚拟机将常量池内的符号引用替换为直接引用的过程，解析动作主要针对类或接口、字段、类方法、接口方法、方法类型、方法句柄和调用点限定符7类符号引用进行。\n符号引用就是一组符号来描述目标，可以是任何字面量。\n直接引用就是直接指向目标的指针、相对偏移量或一个间接定位到目标的句柄。\n初始化 初始化为类的静态变量赋予正确的初始值，在Java中对类变量进行初始值设定有两种方式:\n声明类变量时指定初始化值 使用静态代码块为类变量指定初始值 使用 类访问方法区内的数据结构， 对象是Heap区的数据。\n卸载 Java虚拟机将结束生命周期的几种情况：\n执行了System.exit()方法 程序正常执行结束 程序在执行过程中遇到了异常或错误而异常终止 由于操作系统出现错误而导致Java虚拟机进程终止 Java类加载机制 类加载器 站在Java开发人员的角度来看，类加载器可以大致划分为以下三类 :\n启动类加载器： Bootstrap ClassLoader，负责加载存放在JDK\\jre\\lib(JDK代表JDK的安装目录，下同)下，或被-Xbootclasspath参数指定的路径中的，并且能被虚拟机识别的类库(如rt.jar，所有的java.*开头的类均被Bootstrap ClassLoader加载)。启动类加载器是无法被Java程序直接引用的。 扩展类加载器：Extension ClassLoader，该加载器 sun.misc.Launcher$ExtClassLoader实现，它负责加载JDK\\jre\\lib\\ext目录中，或者由java.ext.dirs系统变量指定的路径中的所有类库(如javax.*开头的类)，开发者可以直接使用扩展类加载器。 应用程序类加载器：Application ClassLoader，该类加载器由sun.misc.Launcher$AppClassLoader来实现，它负责加载用户类路径(ClassPath)所指定的类，开发者可以直接使用该类加载器，如果应用程序中没有自定义过自己的类加载器，一般情况下这个就是程序中默认的类加载器。 寻找类加载器\npackage classloader; public class ClassLoaderTest { public static void main(String[] args) { ClassLoader classLoader = ClassLoaderTest.class.getClassLoader(); System.out.println(classLoader.toString()); System.out.println(classLoader.getParent()); System.out.println(classLoader.getParent().getParent()); } } 结果如下: 从上面的结果可以看出，当前类的加载器为AppClassLoader，它的父Loader是ExtClassLoader，并没有获取到ExtClassLoader的父Loader，原因是BootstrapLoader(引导类加载器)是用C语言实现的，找不到一个确定的返回父Loader的方式，于是就返回null。\n接着介绍下JVM的类加载机制：\n全盘负责：当一个类加载器负责加载某个Class时，该Class所依赖的和引用的其他Class也将由该类加载器负责载入，除非显示使用另外一个类加载器来载入。 缓存机制：缓存机制将会保证所有加载过的Class都会被缓存，当程序中需要使用某个Class时，类加载器先从缓存区寻找该Class，只有缓存区不存在，系统才会读取该类对应的二进制数据，并将其转换成Class对象，存入缓存区。这就是为什么修改了Class后，必须重启JVM，程序的修改才会生效。 双亲委派机制： 如果一个类加载器收到了类加载的请求，它首先不会自己去尝试加载这个类，而是把请求委托给父加载器去完成，依次向上，因此，所有的类加载请求最终都应该被传递到顶层的启动类加载器中，只有当父加载器在它的搜索范围中没有找到所需的类时，即无法完成该加载，子加载器才会尝试自己去加载该类。 1、当AppClassLoader加载一个class时，它首先不会自己去尝试加载这个类，而是把类加载请求委派给父类加载器ExtClassLoader去完成。 2、当ExtClassLoader加载一个class时，它首先也不会自己去尝试加载这个类，而是把类加载请求委派给BootStrapClassLoader去完成。 3、如果BootStrapClassLoader加载失败(例如在$JAVA_HOME/jre/lib里未查找到该class)，会使用ExtClassLoader来尝试加载； 4、若ExtClassLoader也加载失败，则会使用AppClassLoader来加载，如果AppClassLoader也加载失败，则会报出异常ClassNotFoundException。 这里的父类并不是继承关系，而是一种组合关系。\n类加载器的默认加载路径\n类加载器 加载路径 Bootstrap ClassLoader 由系统属性sun.boot.class.path指定，通常是$JAVA_HOME/jre/lib Extension ClassLoader 通常是$JAVA_HOMEx/jre/lib/ext，可通过系统属性java.ext.dirs查看路径 Application ClassLoader 通常是当前路径下的Class文件，可通过系统属性java.class.path查看 双亲委托加载方向\n类加载器在加载类时，只能向上递归委托其双亲进行类加载，而不可能从双亲再反向委派当前类加载器来进行类加载。\n双亲委派优势\n系统类防止内存中出现多份同样的字节码 保证Java程序安全稳定运行 参考\n深入理解Java虚拟机 JVM 基础 - Java 类加载机制 ","date":1616371200,"description":"","dir":"post\\JVM\\","excerpt_html":null,"excerpt_text":null,"expirydate":-62135596800,"fuzzywordcount":3600,"html":null,"keywords":null,"kind":"page","lang":"en","lastmod":1616371200,"objectID":"7cd79d216d4d7e086afb1a030d1ac66b","permalink":"http://localhost:1313/post/jvm/java%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%B1%BB%E5%8A%A0%E8%BD%BD%E8%BF%87%E7%A8%8B/","publishdate":"2021-03-22T00:00:00Z","readingtime":8,"relpermalink":"/post/jvm/java%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%B1%BB%E5%8A%A0%E8%BD%BD%E8%BF%87%E7%A8%8B/","section":"post","summary":"本文简单介绍Java类加载相关知识 Java类的生命周期 一个类从被加载到虚拟机内存到卸载出虚拟机内存，它的生命周期会经历：加载、验证、准备、解","tags":["JVM"],"title":"Java类加载机制","type":"post","url":"/post/jvm/java%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%B1%BB%E5%8A%A0%E8%BD%BD%E8%BF%87%E7%A8%8B/","weight":0,"wordcount":3512},{"author":null,"categories":["Code"],"content":" 本文介绍三种常用缓存淘汰算法，即它们的简单实现。\n简介 缓存，就是将程序或系统经常要调用的对象存在内存中，再次调用时可以快速从内存中获取对象，不必再去创建新的重复的实例。 当缓存中的数据太多超过一定值时，通常会采取一些缓存淘汰算法进行处理。\nFIFO（先进先出） FIFO即先进先出算法，队列也具有先进先出的性质，所以可以考虑采用LinkedList实现FIFO算法。但是只用LinkedList的话，查找时时间复杂度为O(n)，所以可以考虑采用HashMap+LinkedList实现。\npublic class FIFOCache\u0026lt;K,V\u0026gt;{ private Map\u0026lt;K,V\u0026gt; cache; private LinkedList\u0026lt;K\u0026gt; list; private volatile int maxCapacity; private final Lock lock; public FIFOCache(){ this(1000); } public FIFOCache(int maxCapacity){ this.maxCapacity = maxCapacity; this.lock = new ReentrantLock(); this.cache = new HashMap\u0026lt;\u0026gt;(); this.list = new LinkedList\u0026lt;\u0026gt;(); } public V get(K key){ this.lock.lock(); V var; try { var = this.cache.get(key); }finally { this.lock.unlock(); } return var; } public void put(K key, V value){ this.lock.lock(); try { this.list.addLast(key); if (maxCapacity \u0026lt; this.list.size()){ K k = this.list.getFirst(); cache.remove(k); this.list.removeFirst(); } this.cache.put(key, value); }finally { this.lock.unlock(); } } @Override public String toString() { return \u0026#34;FIFOCache{\u0026#34; + \u0026#34;cache=\u0026#34; + cache + \u0026#34;, list=\u0026#34; + list + \u0026#34;, maxCapacity=\u0026#34; + maxCapacity + \u0026#34;, lock=\u0026#34; + lock + \u0026#39;}\u0026#39;; } } 测试类\npublic class TestFIFO { public static void main(String[] args) { FIFOCache\u0026lt;String,String\u0026gt; fifoCache = new FIFOCache\u0026lt;String,String\u0026gt;(3); fifoCache.put(\u0026#34;1\u0026#34;,\u0026#34;a\u0026#34;); fifoCache.put(\u0026#34;2\u0026#34;,\u0026#34;b\u0026#34;); fifoCache.put(\u0026#34;3\u0026#34;,\u0026#34;c\u0026#34;); fifoCache.put(\u0026#34;4\u0026#34;,\u0026#34;d\u0026#34;); System.out.println(fifoCache.toString()); System.out.println(fifoCache.get(\u0026#34;2\u0026#34;)); } } LRU（最近最久未使用） LRU（The Least Recently Used，最近最久未使用算法）是一种常见的缓存算法，在很多分布式缓存系统（如Redis, Memcached）中都有广泛使用。\nLRU算法的思想是：如果一个数据在最近一段时间没有被访问到，那么可以认为在将来它被访问的可能性也很小。因此，当空间满时，最久没有访问的数据最先被置换（淘汰）。\n实现\n数组+时间戳 （时间复杂度较高O(n)） 链表 （查询时间复杂度还是O(n)）每次将访问到的节点移到链表尾部 哈希表+双向链表（LinkedHashMap） Dubbo中的LRU实现\npackage cache; import java.util.LinkedHashMap; import java.util.Map; import java.util.concurrent.locks.Lock; import java.util.concurrent.locks.ReentrantLock; /** * @author zousy * @version v1.0 * @Description * @date 2021-03-16 14:46 */ public class LRUCache\u0026lt;K, V\u0026gt; extends LinkedHashMap\u0026lt;K, V\u0026gt; { private static final float DEFAULT_LOAD_FACTOR = 0.75F; private static final int DEFAULT_MAX_CAPACITY = 1000; private final Lock lock; private volatile int maxCapacity; public LRUCache() { this(1000); } public LRUCache(int maxCapacity) { super(16, 0.75F, true); this.lock = new ReentrantLock(); this.maxCapacity = maxCapacity; } @Override protected boolean removeEldestEntry(Map.Entry\u0026lt;K, V\u0026gt; eldest) { return this.size() \u0026gt; this.maxCapacity; } @Override public boolean containsKey(Object key) { this.lock.lock(); boolean var2; try { var2 = super.containsKey(key); } finally { this.lock.unlock(); } return var2; } @Override public V get(Object key) { this.lock.lock(); Object var2; try { var2 = super.get(key); } finally { this.lock.unlock(); } return (V) var2; } @Override public V put(K key, V value) { this.lock.lock(); Object var3; try { var3 = super.put(key, value); } finally { this.lock.unlock(); } return (V) var3; } @Override public V remove(Object key) { this.lock.lock(); Object var2; try { var2 = super.remove(key); } finally { this.lock.unlock(); } return (V) var2; } @Override public int size() { this.lock.lock(); int var1; try { var1 = super.size(); } finally { this.lock.unlock(); } return var1; } @Override public void clear() { this.lock.lock(); try { super.clear(); } finally { this.lock.unlock(); } } public int getMaxCapacity() { return this.maxCapacity; } public void setMaxCapacity(int maxCapacity) { this.maxCapacity = maxCapacity; } } 测试类\npublic class TestLRUCache { private static LRUCache\u0026lt;String, Integer\u0026gt; cache = new LRUCache\u0026lt;\u0026gt;(10); public static void main(String[] args) { for (int i = 0; i \u0026lt; 10; i++) { cache.put(\u0026#34;k\u0026#34; + i, i); } System.out.println(\u0026#34;all cache :\u0026#39;{\u0026#34;+cache+\u0026#34;}\u0026#39;\u0026#34;); cache.get(\u0026#34;k3\u0026#34;); System.out.println(\u0026#34;get k3 :\u0026#39;{\u0026#34;+cache+\u0026#34;}\u0026#39;\u0026#34;); cache.get(\u0026#34;k4\u0026#34;); System.out.println(\u0026#34;get k4 :\u0026#39;{\u0026#34;+cache+\u0026#34;}\u0026#39;\u0026#34;); cache.get(\u0026#34;k4\u0026#34;); System.out.println(\u0026#34;get k4 :\u0026#39;{\u0026#34;+cache+\u0026#34;}\u0026#39;\u0026#34;); cache.put(\u0026#34;k\u0026#34; + 10, 10); System.out.println(\u0026#34;After running the LRU algorithm cache :\u0026#39;{\u0026#34;+cache+\u0026#34;}\u0026#39;\u0026#34;); } } LFU（最近最少使用） LFU（Least Frequently Used ，最近最少使用算法）也是一种常见的缓存算法。\nLFU算法的思想是：如果一个数据在最近一段时间很少被访问到，那么可以认为在将来它被访问的可能性也很小。因此，当空间满时，最小频率访问的数据最先被淘汰。\n算法实现策略：考虑到 LFU 会淘汰访问频率最小的数据，我们需要一种合适的方法按大小顺序维护数据访问的频率。\n实现\n计算器+链表+哈希表 LIFO的Deque数组+哈希表+链表 这里看下Dubbo中的LFU算法的实现。\npublic class LFUCache\u0026lt;K, V\u0026gt; { private Map\u0026lt;K, LFUCache.CacheNode\u0026lt;K, V\u0026gt;\u0026gt; map; private LFUCache.CacheDeque\u0026lt;K, V\u0026gt;[] freqTable; private final int capacity; private int evictionCount; private int curSize; private final ReentrantLock lock; private static final int DEFAULT_LOAD_FACTOR = 1000; private static final float DEFAULT_EVICTION_CAPACITY = 0.75F;//默认淘汰因子 public LFUCache() { this(1000, 0.75F); } public LFUCache(final int maxCapacity, final float evictionFactor) { this.curSize = 0; this.lock = new ReentrantLock(); if (maxCapacity \u0026lt;= 0) { throw new IllegalArgumentException(\u0026#34;Illegal initial capacity: \u0026#34; + maxCapacity); } else { boolean factorInRange = evictionFactor \u0026lt;= 1.0F || evictionFactor \u0026lt; 0.0F; if (factorInRange \u0026amp;\u0026amp; !Float.isNaN(evictionFactor)) { this.capacity = maxCapacity; this.evictionCount = (int)((float)this.capacity * evictionFactor); this.map = new HashMap(); this.freqTable = new LFUCache.CacheDeque[this.capacity + 1]; int i; for(i = 0; i \u0026lt;= this.capacity; ++i) { this.freqTable[i] = new LFUCache.CacheDeque(); } for(i = 0; i \u0026lt; this.capacity; ++i) { this.freqTable[i].nextDeque = this.freqTable[i + 1]; } this.freqTable[this.capacity].nextDeque = this.freqTable[this.capacity]; } else { throw new IllegalArgumentException(\u0026#34;Illegal eviction factor value:\u0026#34; + evictionFactor); } } } public int getCapacity() { return this.capacity; } public V put(final K key, final V value) { this.lock.lock(); LFUCache.CacheNode node; try { if (this.map.containsKey(key)) { node = (LFUCache.CacheNode)this.map.get(key); if (node != null) { LFUCache.CacheNode.withdrawNode(node); } node.value = value; this.freqTable[0].addLastNode(node); this.map.put(key, node); } else { node = this.freqTable[0].addLast(key, value); this.map.put(key, node); ++this.curSize; if (this.curSize \u0026gt; this.capacity) { this.proceedEviction(); } } } finally { this.lock.unlock(); } return node.value; } public V remove(final K key) { LFUCache.CacheNode\u0026lt;K, V\u0026gt; node = null; this.lock.lock(); try { if (this.map.containsKey(key)) { node = (LFUCache.CacheNode)this.map.remove(key); if (node != null) { LFUCache.CacheNode.withdrawNode(node); } --this.curSize; } } finally { this.lock.unlock(); } return node != null ? node.value : null; } public V get(final K key) { LFUCache.CacheNode\u0026lt;K, V\u0026gt; node = null; this.lock.lock(); try { if (this.map.containsKey(key)) { node = (LFUCache.CacheNode)this.map.get(key); LFUCache.CacheNode.withdrawNode(node); //数组下一个位置存放访问过的值,相当于访问过n次。淘汰时，根据对应位置链表长度进行淘汰。 node.owner.nextDeque.addLastNode(node); } } finally { this.lock.unlock(); } return node != null ? node.value : null; } private int proceedEviction() { int targetSize = this.capacity - this.evictionCount;//允许缓存的大小 = 容量-容量*淘汰因子 int evictedElements = 0; for(int i = 0; i \u0026lt;= this.capacity; ++i) { while(!this.freqTable[i].isEmpty()) { LFUCache.CacheNode\u0026lt;K, V\u0026gt; node = this.freqTable[i].pollFirst(); this.remove(node.key); if (targetSize \u0026gt;= this.curSize) { return evictedElements; } ++evictedElements; } } return evictedElements; } public int getSize() { return this.curSize; } static class CacheDeque\u0026lt;K, V\u0026gt; { LFUCache.CacheNode\u0026lt;K, V\u0026gt; last = new LFUCache.CacheNode(); LFUCache.CacheNode\u0026lt;K, V\u0026gt; first = new LFUCache.CacheNode(); LFUCache.CacheDeque\u0026lt;K, V\u0026gt; nextDeque; CacheDeque() { this.last.next = this.first; this.first.prev = this.last; } LFUCache.CacheNode\u0026lt;K, V\u0026gt; addLast(final K key, final V value) { LFUCache.CacheNode\u0026lt;K, V\u0026gt; node = new LFUCache.CacheNode(key, value); node.owner = this; node.next = this.last.next; node.prev = this.last; node.next.prev = node; this.last.next = node; return node; } LFUCache.CacheNode\u0026lt;K, V\u0026gt; addLastNode(final LFUCache.CacheNode\u0026lt;K, V\u0026gt; node) { node.owner = this; node.next = this.last.next; node.prev = this.last; node.next.prev = node; this.last.next = node; return node; } LFUCache.CacheNode\u0026lt;K, V\u0026gt; pollFirst() { LFUCache.CacheNode\u0026lt;K, V\u0026gt; node = null; if (this.first.prev != this.last) { node = this.first.prev; this.first.prev = node.prev; this.first.prev.next = this.first; node.prev = null; node.next = null; } return node; } boolean isEmpty() { return this.last.next == this.first; } } static class CacheNode\u0026lt;K, V\u0026gt; { LFUCache.CacheNode\u0026lt;K, V\u0026gt; prev; LFUCache.CacheNode\u0026lt;K, V\u0026gt; next; K key; V value; LFUCache.CacheDeque owner; CacheNode() { } CacheNode(final K key, final V value) { this.key = key; this.value = value; } static \u0026lt;K, V\u0026gt; LFUCache.CacheNode\u0026lt;K, V\u0026gt; withdrawNode(final LFUCache.CacheNode\u0026lt;K, V\u0026gt; node) { if (node != null \u0026amp;\u0026amp; node.prev != null) { node.prev.next = node.next; if (node.next != null) { node.next.prev = node.prev; } } return node; } } } 数据结构 测试\npublic class TestLFUCache { private static LFUCache\u0026lt;String, Integer\u0026gt; cache = new LFUCache\u0026lt;String, Integer\u0026gt;(3,0); public static void main(String[] args) { cache.put(\u0026#34;k1\u0026#34;,1); cache.get(\u0026#34;k1\u0026#34;); cache.put(\u0026#34;k2\u0026#34;,2); cache.get(\u0026#34;k2\u0026#34;); cache.put(\u0026#34;k3\u0026#34;,3); cache.put(\u0026#34;k4\u0026#34;,4); cache.put(\u0026#34;k5\u0026#34;,5); cache.put(\u0026#34;k6\u0026#34;,6); System.out.println(cache.get(\u0026#34;k1\u0026#34;)); System.out.println(cache.get(\u0026#34;k2\u0026#34;)); System.out.println(cache.get(\u0026#34;k3\u0026#34;)); System.out.println(cache.get(\u0026#34;k4\u0026#34;)); System.out.println(cache.get(\u0026#34;k5\u0026#34;)); System.out.println(cache.get(\u0026#34;k6\u0026#34;)); } } 结果 CacheDeque链表数组相当于给访问过的值计数，每访问过一次就移动到下一个下标处。\n","date":1615852800,"description":"","dir":"post\\Algorithm\\","excerpt_html":null,"excerpt_text":null,"expirydate":-62135596800,"fuzzywordcount":1900,"html":null,"keywords":null,"kind":"page","lang":"en","lastmod":1615852800,"objectID":"10ccc46765fc24acc7a2967017b9a80a","permalink":"http://localhost:1313/post/algorithm/%E7%BC%93%E5%AD%98%E6%B7%98%E6%B1%B0%E7%AE%97%E6%B3%95/","publishdate":"2021-03-16T00:00:00Z","readingtime":4,"relpermalink":"/post/algorithm/%E7%BC%93%E5%AD%98%E6%B7%98%E6%B1%B0%E7%AE%97%E6%B3%95/","section":"post","summary":"本文介绍三种常用缓存淘汰算法，即它们的简单实现。 简介 缓存，就是将程序或系统经常要调用的对象存在内存中，再次调用时可以快速从内存中获取对象，不","tags":["Algorithm"],"title":"FIFO、LRU、LFU三种缓存淘汰算法","type":"post","url":"/post/algorithm/%E7%BC%93%E5%AD%98%E6%B7%98%E6%B1%B0%E7%AE%97%E6%B3%95/","weight":0,"wordcount":1888},{"author":null,"categories":["Code"],"content":" 本文介绍LinkedHashMap的相关知识\n简介 之前了解过HashMap，HashMap是无序的，当我们希望有顺序地去存储key-value时，就需要使用LinkedHashMap了。\nLinkedHashMap由哈希表+双向链表组成，它继承自HashMap，重写了HashMap的一些方法，可以用于LRU算法，它和HashMap一样不是线程安全的。\npublic class TestLinkedHashMap { public static void main(String[] args) { LinkedHashMap\u0026lt;String, String\u0026gt; linkedHashMap = new LinkedHashMap\u0026lt;String, String\u0026gt;(16,0.75f,true); linkedHashMap.put(\u0026#34;name1\u0026#34;, \u0026#34;josan1\u0026#34;); linkedHashMap.put(\u0026#34;name2\u0026#34;, \u0026#34;josan2\u0026#34;); linkedHashMap.put(\u0026#34;name3\u0026#34;, \u0026#34;josan3\u0026#34;); System.out.println(\u0026#34;LinkedHashMap遍历时顺序：\u0026#34;); for (Entry\u0026lt;String, String\u0026gt; entry : linkedHashMap.entrySet()){ String key = (String) entry.getKey(); String value = (String) entry.getValue(); System.out.println(\u0026#34;key:\u0026#34; + key + \u0026#34;,value:\u0026#34; + value); } HashMap\u0026lt;String, String\u0026gt; hashMap = new HashMap\u0026lt;String, String\u0026gt;(16); hashMap.put(\u0026#34;name1\u0026#34;, \u0026#34;josan1\u0026#34;); hashMap.put(\u0026#34;name2\u0026#34;, \u0026#34;josan2\u0026#34;); hashMap.put(\u0026#34;name3\u0026#34;, \u0026#34;josan3\u0026#34;); System.out.println(\u0026#34;HashMap遍历时顺序：\u0026#34;); for (Entry\u0026lt;String, String\u0026gt; entry : hashMap.entrySet()){ String key = (String) entry.getKey(); String value = (String) entry.getValue(); System.out.println(\u0026#34;key:\u0026#34; + key + \u0026#34;,value:\u0026#34; + value); } } } 结果可知，LinkedHashMap是有序的，且默认为插入顺序。\n构造函数 public class LinkedHashMap\u0026lt;K,V\u0026gt; extends HashMap\u0026lt;K,V\u0026gt; implements Map\u0026lt;K,V\u0026gt; { public LinkedHashMap() { super(); //accessOrder默认是false，则迭代时输出的顺序是插入节点的顺序。若为true，则输出的顺序是按照访问节点的顺序。 accessOrder = false; } public LinkedHashMap(int initialCapacity) { super(initialCapacity); accessOrder = false; } //指定初始化时的容量，和扩容的加载因子 public LinkedHashMap(int initialCapacity, float loadFactor) { super(initialCapacity, loadFactor); accessOrder = false; } public LinkedHashMap(int initialCapacity, float loadFactor, boolean accessOrder) { super(initialCapacity, loadFactor); this.accessOrder = accessOrder; } public LinkedHashMap(Map\u0026lt;? extends K, ? extends V\u0026gt; m) { super(); accessOrder = false; putMapEntries(m, false); } } LinkedHashMap 继承了HashMap，实现了Map接口。\nLinkedHashMap的accessOrder变量默认为false，则迭代时输出的顺序是插入节点的顺序。若为true，则输出的顺序是按照访问节点的顺序。\n数据结构 Entry的next是用于维护HashMap指定table位置上连接的Entry的顺序的，before、After是用于维护Entry插入的先后顺序的。\n//LinkedHashMap内部类 Entry继承HashMap的Node内部类，是一个双向链表 static class Entry\u0026lt;K,V\u0026gt; extends HashMap.Node\u0026lt;K,V\u0026gt; { Entry\u0026lt;K,V\u0026gt; before, after; Entry(int hash, K key, V value, Node\u0026lt;K,V\u0026gt; next) { super(hash, key, value, next); } } 该循环双向链表的头部存放的是最久访问的节点或最先插入的节点，尾部为最近访问的或最近插入的节点，迭代器遍历方向是从链表的头部开始到链表尾部结束。\n增 LinkedHashMap并没有重写任何put方法。但是其重写了构建新节点的newNode()方法. newNode()会在HashMap的putVal()方法里被调用，putVal()方法会在批量插入数据putMapEntries()或者插入单个数据public V put(K key, V value)时被调用。\nLinkedHashMap重写了newNode()，在每次构建新节点时，通过linkNodeLast，将新节点链接在内部双向链表的尾部。\nNode\u0026lt;K,V\u0026gt; newNode(int hash, K key, V value, Node\u0026lt;K,V\u0026gt; e) { LinkedHashMap.Entry\u0026lt;K,V\u0026gt; p = new LinkedHashMap.Entry\u0026lt;K,V\u0026gt;(hash, key, value, e); linkNodeLast(p); return p; } private void linkNodeLast(LinkedHashMap.Entry\u0026lt;K,V\u0026gt; p) { LinkedHashMap.Entry\u0026lt;K,V\u0026gt; last = tail; //tail 指向尾节点即插入的节点 tail = p; if (last == null) head = p; else { p.before = last; last.after = p; } } HashMap专门预留给LinkedHashMap的afterNodeAccess() afterNodeInsertion() afterNodeRemoval() 方法。\n// Callbacks to allow LinkedHashMap post-actions void afterNodeAccess(Node\u0026lt;K,V\u0026gt; p) { } void afterNodeInsertion(boolean evict) { } void afterNodeRemoval(Node\u0026lt;K,V\u0026gt; p) { } //回调函数，新节点插入之后回调 ， 根据evict 和 判断是否需要删除最老插入的节点。如果实现LruCache会用到这个方法。 void afterNodeInsertion(boolean evict) { // possibly remove eldest LinkedHashMap.Entry\u0026lt;K,V\u0026gt; first; //LinkedHashMap 默认返回false 则不删除节点 if (evict \u0026amp;\u0026amp; (first = head) != null \u0026amp;\u0026amp; removeEldestEntry(first)) { K key = first.key; removeNode(hash(key), key, null, false, true); } } //LinkedHashMap 默认返回false 则不删除节点。 返回true 代表要删除最早的节点。通常构建一个LruCache会在达到Cache的上限是返回true protected boolean removeEldestEntry(Map.Entry\u0026lt;K,V\u0026gt; eldest) { return false; } 删 LinkedHashMap也没有重写remove()方法，因为它的删除逻辑和HashMap并无区别。 但它重写了afterNodeRemoval()这个回调方法。该方法会在Node\u0026lt;K,V\u0026gt; removeNode()方法中回调，removeNode()会在所有涉及到删除节点的方法中被调用。\n//双向链表删除节点 void afterNodeRemoval(Node\u0026lt;K,V\u0026gt; e) { // unlink LinkedHashMap.Entry\u0026lt;K,V\u0026gt; p = (LinkedHashMap.Entry\u0026lt;K,V\u0026gt;)e, b = p.before, a = p.after; //要删除的节点p before和after 置空 p.before = p.after = null; //p的前置节点为null，则p是头节点，head指向p的后置节点 if (b == null) head = a; else//b不为null，b的后置节点为p的后置节点 b.after = a; //p的后置节点为null，则p是尾节点，tail指向p的前置节点 if (a == null) tail = b; else//a不为null，a的前置节点为p的前置节点 a.before = b; } 改 更改value时，发生hash冲突，逻辑和HashMap的put逻辑一样。\n查 重写了HashMap的get方法，调用getNode方法，LinkedHashMap只是增加了在成员变量(构造函数时赋值)accessOrder为true的情况下，要去回调void afterNodeAccess()函数，在afterNodeAccess()函数中，会将当前被访问到的节点e，移动至内部的双向链表的尾部。\npublic V get(Object key) { Node\u0026lt;K,V\u0026gt; e; if ((e = getNode(hash(key), key)) == null) return null; if (accessOrder) afterNodeAccess(e); return e.value; } void afterNodeAccess(Node\u0026lt;K,V\u0026gt; e) { // move node to last LinkedHashMap.Entry\u0026lt;K,V\u0026gt; last;//原尾节点 //如果accessOrder 是true ，且原尾节点不等于e if (accessOrder \u0026amp;\u0026amp; (last = tail) != e) { //节点e强转成双向链表节点p LinkedHashMap.Entry\u0026lt;K,V\u0026gt; p = (LinkedHashMap.Entry\u0026lt;K,V\u0026gt;)e, b = p.before, a = p.after; //p现在是尾节点， 后置节点一定是null p.after = null; //如果p的前置节点是null，则p以前是头结点，所以更新现在的头结点是p的后置节点a if (b == null) head = a; else//否则更新p的前直接点b的后置节点为 a b.after = a; //如果p的后置节点不是null，则更新后置节点a的前置节点为b if (a != null) a.before = b; else//如果原本p的后置节点是null，则p就是尾节点。 此时 更新last的引用为 p的前置节点b last = b; if (last == null) //原本尾节点是null 则，链表中就一个节点 head = p; else {//否则 更新 当前节点p的前置节点为 原尾节点last， last的后置节点是p p.before = last; last.after = p; } //尾节点的引用赋值成p tail = p; //修改modCount。 ++modCount; } } containsValue LinkedHashMap重写了该方法，相比HashMap的实现，遍历双向链表更为高效。\npublic boolean containsValue(Object value) { for (LinkedHashMap.Entry\u0026lt;K,V\u0026gt; e = head; e != null; e = e.after) { V v = e.value; if (v == value || (value != null \u0026amp;\u0026amp; value.equals(v))) return true; } return false; } 对比HashMap，是用两个for循环遍历，相对低效。\npublic boolean containsValue(Object value) { Node\u0026lt;K,V\u0026gt;[] tab; V v; if ((tab = table) != null \u0026amp;\u0026amp; size \u0026gt; 0) { for (int i = 0; i \u0026lt; tab.length; ++i) { for (Node\u0026lt;K,V\u0026gt; e = tab[i]; e != null; e = e.next) { if ((v = e.value) == value || (value != null \u0026amp;\u0026amp; value.equals(v))) return true; } } } return false; } 总结\nLinkedHashMap通过继承HashMap重写了它的一些方法，实现了有序性。 accessOrder ,默认是false，则迭代时输出的顺序是插入节点的顺序。若为true，则输出的顺序是按照访问节点的顺序。为true时，可以在这基础之上构建一个LRUCache。 LinkedHashMap不是线程安全的，内部结构是哈希表+双向链表。 LinkedHashMap和HashMap一样，允许一对键值为null，key不能重复，但value可以重复。 参考\nLinkedHashMap源码解析（JDK8） 图解LinkedHashMap原理 Java集合之LinkedHashMap ","date":1615852800,"description":"","dir":"post\\Java Base\\","excerpt_html":null,"excerpt_text":null,"expirydate":-62135596800,"fuzzywordcount":2800,"html":null,"keywords":null,"kind":"page","lang":"en","lastmod":1615852800,"objectID":"472cac7cdc1563067e54d6e52f09dd6b","permalink":"http://localhost:1313/post/java-base/linkedhashmap/","publishdate":"2021-03-16T00:00:00Z","readingtime":6,"relpermalink":"/post/java-base/linkedhashmap/","section":"post","summary":"本文介绍LinkedHashMap的相关知识 简介 之前了解过HashMap，HashMap是无序的，当我们希望有顺序地去存储key-value","tags":["Java集合"],"title":"深入了解LinkedHashMap","type":"post","url":"/post/java-base/linkedhashmap/","weight":0,"wordcount":2790},{"author":null,"categories":["Code"],"content":" 本文介绍一些Java线程常用通信工具类，主要介绍怎么使用。\n简介 常见的线程间通信方法有：\nwait()和notify() +加锁机制synchronized和lock 还有线程的join()方法 Condition接口的awiat() 和 signAll()方法 + 加锁机制synchronized和lock 生产者消费者模式 这里介绍一些JDK中java.util.concurrent包下的一些通信工具类。\n类 作用 Semaphore 限制线程的数量 Exchanger 两个线程交换数据 CountDownLatch 线程等待直到计数器减为0时开始工作 CyclicBarrier 作用跟CountDownLatch类似，但是可以重复使用 1.Semaphore Semaphore即信号，以前学操作系统时，学过信号量机制。Semaphore往往用于资源有限的场景中，去限制线程的数量，这里介绍下这个类的使用。举个例子，我想限制同时只能有3个线程在工作：\npackage threadcon; import java.util.Random; import java.util.concurrent.Semaphore; /** * @author zousy * @version v1.0 * @Description * @date 2021-03-11 17:47 */ public class SemaphoreDemo { static class MyThread implements Runnable { private int value; private Semaphore semaphore; public MyThread(int value, Semaphore semaphore) { this.value = value; this.semaphore = semaphore; } @Override public void run() { try { semaphore.acquire(); // 获取permit System.out.println(String.format(\u0026#34;当前线程是%d, 还剩%d个资源，还有%d个线程在等待\u0026#34;, value, semaphore.availablePermits(), semaphore.getQueueLength())); // 睡眠随机时间，打乱释放顺序 Random random =new Random(); Thread.sleep(random.nextInt(1000)); System.out.println(String.format(\u0026#34;线程%d释放了资源\u0026#34;, value)); } catch (InterruptedException e) { e.printStackTrace(); } finally{ semaphore.release(); // 释放permit } } } public static void main(String[] args) { Semaphore semaphore = new Semaphore(3); for (int i = 0; i \u0026lt; 10; i++) { new Thread(new MyThread(i, semaphore)).start(); } } } 原理：\nSemaphore(int)型构造函数\npublic Semaphore(int permits) { sync = new NonfairSync(permits); } 该构造函数会创建具有给定的许可数和非公平机制的Semaphore。\n这里即设置AQS中的state为3，调用acquire会将state-1，调用release会将state+1。\n与AQS的队列操作大同小异，这里不再详细介绍。\n2.Exchanger Exchanger类用于两个线程交换数据。它支持泛型，也就是说你可以在两个线程之间传送任何数据。\npackage threadcon; import java.util.concurrent.Exchanger; /** * @author zousy * @version v1.0 * @Description * @date 2021-03-11 18:38 */ public class ExchangerDemo { public static void main(String[] args) { Exchanger\u0026lt;String\u0026gt; exchanger =new Exchanger\u0026lt;\u0026gt;(); new Thread(new Runnable() { @Override public void run() { try { String a = \u0026#34;这是来自线程A的数据\u0026#34;; System.out.println(\u0026#34;这是线程A，得到了另一个线程的数据：\u0026#34; + exchanger.exchange(a) + \u0026#34; hashcode \u0026#34; + exchanger.exchange(a).hashCode()); } catch (InterruptedException e) { e.printStackTrace(); } } },\u0026#34;A\u0026#34;).start(); new Thread(new Runnable() { @Override public void run() { try { String b = \u0026#34;这是来自线程B的数据\u0026#34;; System.out.println(\u0026#34;这是线程B，得到了另一个线程的数据：\u0026#34; + exchanger.exchange(b) + \u0026#34; hashcode \u0026#34; + exchanger.exchange(b).hashCode()); } catch (InterruptedException e) { e.printStackTrace(); } } },\u0026#34;A\u0026#34;).start(); } } Exchanger只能是两个线程交换数据吗？那三个调用同一个实例的exchange方法会发生什么呢？答案是只有前两个线程会交换数据，第三个线程会进入阻塞状态。\n需要注意的是，exchange是可以重复使用的。也就是说。两个线程可以使用Exchanger在内存中不断地再交换数据。\n3.CountDownLatch 先来解读一下CountDownLatch这个类名字的意义。CountDown代表计数递减，Latch是“门闩”的意思。也有人把它称为“屏障”。而CountDownLatch这个类的作用也很贴合这个名字的意义，假设某个线程在执行任务之前，需要等待其它线程完成一些前置任务，必须等所有的前置任务都完成，才能开始执行本线程的任务。\npackage threadcon; import java.util.concurrent.CountDownLatch; /** * @author zousy * @version v1.0 * @Description * @date 2021-03-11 18:48 */ public class CountDownLatchDemo { public static void main(String[] args) { CountDownLatch countDownLatch = new CountDownLatch(3); for (int i = 0; i \u0026lt; 3; i++) { new Thread(new Runnable() { @Override public void run() { try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(Thread.currentThread().getName() + \u0026#34; is running \u0026#34;); countDownLatch.countDown(); } },\u0026#34;ThreadA \u0026#34;+ i + \u0026#34; \u0026#34;).start(); } try { countDownLatch.await(); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(\u0026#34;ThreadAs run over\u0026#34;); for (int i = 0; i \u0026lt; 5; i++) { new Thread(new Runnable() { @Override public void run() { try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(Thread.currentThread().getName() + \u0026#34; is running \u0026#34;); countDownLatch.countDown(); } },\u0026#34;ThreadB \u0026#34;+ i + \u0026#34; \u0026#34;).start(); } } } CountDownLatch类的内部同样是一个基层了AQS的实现类Sync，且实现起来还很简单，可能是JDK里面AQS的子类中最简单的实现了。\n需要注意的是构造器中的计数值（count）实际上就是闭锁需要等待的线程数量。这个值只能被设置一次，而且CountDownLatch没有提供任何机制去重新设置这个计数值。\n4.CyclicBarrier CyclicBarrirer从名字上来理解是“循环的屏障”的意思。前面提到了CountDownLatch一旦计数值count被降为0后，就不能再重新设置了，它只能起一次“屏障”的作用。而CyclicBarrier拥有CountDownLatch的所有功能，还可以使用reset()方法重置屏障。\npackage thread; import java.util.concurrent.BrokenBarrierException; import java.util.concurrent.CyclicBarrier; import java.util.concurrent.ExecutorService; import java.util.concurrent.Executors; /** * @author zousy * @version v1.0 * @Description * @date 2021-03-12 9:37 */ public class CyclicBarrierTest2 { private static CyclicBarrier cyclicBarrier = new CyclicBarrier(2, new Runnable() { @Override public void run() { System.out.println(Thread.currentThread().getName() + \u0026#34; step done\u0026#34;); } }); public static void main(String[] args) { ExecutorService executorService = Executors.newFixedThreadPool(2); executorService.submit(new Runnable() { @Override public void run() { for (int i = 0; i \u0026lt; 2; i++) { System.out.println(Thread.currentThread().getName() +\u0026#34; \u0026#34;+ i+ \u0026#34; step doing \u0026#34;); try { cyclicBarrier.await(); } catch (InterruptedException e) { e.printStackTrace(); } catch (BrokenBarrierException e) { e.printStackTrace(); } } } }); executorService.submit(new Runnable() { @Override public void run() { for (int i = 0; i \u0026lt; 2; i++) { System.out.println(Thread.currentThread().getName() +\u0026#34; \u0026#34;+ i+ \u0026#34; step doing \u0026#34;); try { cyclicBarrier.await(); } catch (InterruptedException e) { e.printStackTrace(); } catch (BrokenBarrierException e) { e.printStackTrace(); } } } }); executorService.shutdown(); } } CyclicBarrier没有分为await()和countDown()，而是只有单独的一个await()方法。 一旦调用await()方法的线程数量等于构造方法中传入的任务总量（这里是2），就代表达到屏障了。CyclicBarrier允许我们在达到屏障的时候可以执行一个任务，可以在构造方法传入一个Runnable类型的对象。\n和CountDonwLatch再对比\nCountDownLatch减计数，CyclicBarrier加计数。 CountDownLatch是一次性的，CyclicBarrier可以重用。 CountDownLatch和CyclicBarrier都有让多个线程等待同步然后再开始下一步动作的意思，但是CountDownLatch的下一步的动作实施者是主线程，具有不可重复性；而CyclicBarrier的下一步动作实施者还是“其他线程”本身，具有往复多次实施动作的特点。 参考\nJAVA线程通信工具类 JUC工具类: CyclicBarrier详解 JAVA并发编程之美 ","date":1615507200,"description":"","dir":"post\\Java Concurrent\\","excerpt_html":null,"excerpt_text":null,"expirydate":-62135596800,"fuzzywordcount":2400,"html":null,"keywords":null,"kind":"page","lang":"en","lastmod":1615507200,"objectID":"7fe1f62d2b987ff915ebd57201c3c89c","permalink":"http://localhost:1313/post/java-concurrent/java%E7%BA%BF%E7%A8%8B%E9%80%9A%E4%BF%A1%E5%B7%A5%E5%85%B7%E7%B1%BB%E7%9A%84%E4%BD%BF%E7%94%A8/","publishdate":"2021-03-12T00:00:00Z","readingtime":5,"relpermalink":"/post/java-concurrent/java%E7%BA%BF%E7%A8%8B%E9%80%9A%E4%BF%A1%E5%B7%A5%E5%85%B7%E7%B1%BB%E7%9A%84%E4%BD%BF%E7%94%A8/","section":"post","summary":"本文介绍一些Java线程常用通信工具类，主要介绍怎么使用。 简介 常见的线程间通信方法有： wait()和notify() +加锁机制synchro","tags":["Java并发"],"title":"Java线程通信工具类的使用","type":"post","url":"/post/java-concurrent/java%E7%BA%BF%E7%A8%8B%E9%80%9A%E4%BF%A1%E5%B7%A5%E5%85%B7%E7%B1%BB%E7%9A%84%E4%BD%BF%E7%94%A8/","weight":0,"wordcount":2375},{"author":null,"categories":["Code"],"content":" 本文分析JDK1.8中的ReentrantReadWriteLock类\n简介 由于ReentrantLock是独占锁，某时只有一个线程可以获取该锁，而实际中会有写少读多的场景，所以ReentrantReadWriteLock应运而生，采用读写分离的策略，允许多个线程同时获取该锁。\nReentrantReadWriteLock即可重入读写锁，内部维护一个ReadLock和一个WriteLock，他们依赖Sync来实现，而Sync继承AbstractQueuedSynchronizer，并且也提供了公平和非公平的实现。\n内部类 Sync\n抽象类Sync继承自AQS\nabstract static class Sync extends AbstractQueuedSynchronizer {} 一些属性\n//高16位为读锁，低16位为写锁 static final int SHARED_SHIFT = 16; //共享锁读锁 状态单位值65536 static final int SHARED_UNIT = (1 \u0026lt;\u0026lt; SHARED_SHIFT); //共享锁读锁 最大个数65535 static final int MAX_COUNT = (1 \u0026lt;\u0026lt; SHARED_SHIFT) - 1; //排它锁写锁掩码 15个1 static final int EXCLUSIVE_MASK = (1 \u0026lt;\u0026lt; SHARED_SHIFT) - 1; // 返回读锁线程数 c右移 16位 static int sharedCount(int c) { return c \u0026gt;\u0026gt;\u0026gt; SHARED_SHIFT; } //返回写锁可重入个数 c \u0026amp; 15个1 static int exclusiveCount(int c) { return c \u0026amp; EXCLUSIVE_MASK; } //本地线程计数器 private transient ThreadLocalHoldCounter readHolds; //缓存计数器 private transient HoldCounter cachedHoldCounter; //第一个读线程 private transient Thread firstReader = null; //第一个读线程的计数 private transient int firstReaderHoldCount; Sync内部类\nstatic final class HoldCounter { //重入的次数 int count = 0; //线程id final long tid = getThreadId(Thread.currentThread()); } static final class ThreadLocalHoldCounter extends ThreadLocal\u0026lt;HoldCounter\u0026gt; { // 重写初始化方法，在没有进行set的情况下，获取的都是该HoldCounter值 public HoldCounter initialValue() { return new HoldCounter(); } } 锁的获取与释放 WriteLock 写锁的获取与释放\nlock\npublic void lock() { sync.acquire(1); } public final void acquire(int arg) { //获取锁失败则插入AQS阻塞队列尾部 if (!tryAcquire(arg) \u0026amp;\u0026amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt(); } protected final boolean tryAcquire(int acquires) { //当前线程 Thread current = Thread.currentThread(); //获取状态值 int c = getState(); //获取写线程数量 int w = exclusiveCount(c); //c!=0 说明写锁或者读锁已经被某线程获取 if (c != 0) { //w=0说明已经有线程获取了读锁返回false，w!=0并且当前线程不是写锁的拥有者，则返回false if (w == 0 || current != getExclusiveOwnerThread()) return false; //超过最高写线程数量 if (w + exclusiveCount(acquires) \u0026gt; MAX_COUNT) throw new Error(\u0026#34;Maximum lock count exceeded\u0026#34;); // 设置AQS状态 setState(c + acquires); return true; } //c == 0 说明目前没有线程获取到读锁和写锁，非公平锁则线程抢占式执行CAS尝试获取写锁 if (writerShouldBlock() || !compareAndSetState(c, c + acquires)) return false; // 设置独占线程 setExclusiveOwnerThread(current); return true; } 首先会获取state，判断是否为0，若为0，表示此时没有读锁线程，再判断写线程是否应该被阻塞，而在非公平策略下线程抢占式执行CAS尝试获取写锁，在公平策略下会进行判断(判断同步队列中是否有等待时间更长的线程，若存在，则需要被阻塞，否则，无需阻塞)，之后在设置状态state，然后返回true。若state不为0，则表示此时存在读锁或写锁线程，若写锁线程数量为0或者当前线程为独占锁线程，则返回false，表示不成功，否则，判断写锁线程的重入次数是否大于了最大值，若是，则抛出异常，否则，设置状态state，返回true，表示成功。 lockInterruptibly 会对中断进行响应，也就是当其他线程调用了该线程的interrupt()方法中断了当前线程，当前线程会抛出异常InterruptedException。\nunlock\n//释放写锁 public void unlock() { sync.release(1); } public final boolean release(int arg) { //释放锁成功，取AQS阻塞队列的头节点，并激活 if (tryRelease(arg)) { Node h = head; if (h != null \u0026amp;\u0026amp; h.waitStatus != 0) unparkSuccessor(h); return true; } return false; } protected final boolean tryRelease(int releases) { //是否是写锁拥有者调用的unlock if (!isHeldExclusively()) throw new IllegalMonitorStateException(); //释放写锁后的 写锁的数量 int nextc = getState() - releases; boolean free = exclusiveCount(nextc) == 0; //写锁数量为0 则释放锁 if (free) setExclusiveOwnerThread(null); //更新状态值 setState(nextc); return free; } 首先会判断该线程是否为独占线程，若不为独占线程，则抛出异常，否则，计算释放资源后的写锁的数量，若为0，表示成功释放，资源不将被占用，否则，表示资源还被占用。其函数流程图如下。\nReadLock 读锁的获取与释放 lock\npublic void lock() { sync.acquireShared(1); } public final void acquireShared(int arg) { //获取锁，如果返回值\u0026lt;0说明失败了 if (tryAcquireShared(arg) \u0026lt; 0) //加入队列 自旋去获取锁 doAcquireShared(arg); } protected final int tryAcquireShared(int unused) { //获取当前线程 Thread current = Thread.currentThread(); //获取状态 int c = getState(); //有写锁占用并且不是当前线程，则直接返回获取失败 if (exclusiveCount(c) != 0 \u0026amp;\u0026amp; getExclusiveOwnerThread() != current) return -1; //获取读锁的线程数 int r = sharedCount(c); // 读线程是否应该被阻塞、并且小于最大值、并且比较设置成功 if (!readerShouldBlock() \u0026amp;\u0026amp; r \u0026lt; MAX_COUNT \u0026amp;\u0026amp; compareAndSetState(c, c + SHARED_UNIT)) { // 如果读锁持有数为0，则说明当前线程是第一个reader，分别给firstReader和firstReaderHoldCount初始化 if (r == 0) { firstReader = current; firstReaderHoldCount = 1; } else if (firstReader == current) {// 如果读锁持有数不为0且当前线程就是firstReader，那么直接给firstReaderHoldCount+1，表示读锁重入 firstReaderHoldCount++; } else {// 读锁数量不为0并且不为当前线程 HoldCounter rh = cachedHoldCounter; if (rh == null || rh.tid != getThreadId(current)) cachedHoldCounter = rh = readHolds.get(); else if (rh.count == 0) readHolds.set(rh); rh.count++; } return 1; } // 应该阻塞或者CAS失败则进入此方法获取锁 return fullTryAcquireShared(current); } private void doAcquireShared(int arg) { //将节点挂在到队列 并设置其为尾结点 final Node node = addWaiter(Node.SHARED); boolean failed = true; try { boolean interrupted = false; for (;;) { //p是node的前节点 final Node p = node.predecessor(); if (p == head) { // 如果前一个节点是头节点，则尝试获取锁 int r = tryAcquireShared(arg); if (r \u0026gt;= 0) {//获取锁成功 setHeadAndPropagate(node, r);//设置头节点 p.next = null; // help GC if (interrupted) selfInterrupt(); failed = false; return; } } if (shouldParkAfterFailedAcquire(p, node) \u0026amp;\u0026amp; parkAndCheckInterrupt()) interrupted = true; } } finally { if (failed) cancelAcquire(node); } } unlock\npublic void unlock() { sync.releaseShared(1); } public final boolean releaseShared(int arg) { if (tryReleaseShared(arg)) { doReleaseShared(); return true; } return false; } protected final boolean tryReleaseShared(int unused) { //当前线程 Thread current = Thread.currentThread(); //当前线程是否为第一个读线程 if (firstReader == current) { // assert firstReaderHoldCount \u0026gt; 0; //重入数为1 则置空 if (firstReaderHoldCount == 1) firstReader = null; //可重入数-1 else firstReaderHoldCount--; } else { //得到缓存的计算 HoldCounter rh = cachedHoldCounter; if (rh == null || rh.tid != getThreadId(current)) // 获取当前线程对应的计数器 rh = readHolds.get(); // 获取计数 int count = rh.count; if (count \u0026lt;= 1) { readHolds.remove(); if (count \u0026lt;= 0) throw unmatchedUnlockException(); } //计数-1 --rh.count; } for (;;) { int c = getState(); // 获取释放后状态 int nextc = c - SHARED_UNIT; if (compareAndSetState(c, nextc)) //CAS自旋设置 return nextc == 0; } } private void doReleaseShared() { for (;;) { //自旋激活等待节点 Node h = head; if (h != null \u0026amp;\u0026amp; h != tail) { int ws = h.waitStatus; if (ws == Node.SIGNAL) { if (!compareAndSetWaitStatus(h, Node.SIGNAL, 0)) continue; // loop to recheck cases unparkSuccessor(h); } else if (ws == 0 \u0026amp;\u0026amp; !compareAndSetWaitStatus(h, 0, Node.PROPAGATE)) continue; // loop on failed CAS } if (h == head) // loop if head changed break; } } ReentrantReadWriteLock的使用 package thread3; import java.util.ArrayList; import java.util.concurrent.ThreadLocalRandom; import java.util.concurrent.locks.Lock; import java.util.concurrent.locks.ReentrantReadWriteLock; /** * @author zousy * @version v1.0 * @Description * @date 2021-03-09 14:55 */ public class ReentrantReadWriteLockList { private ArrayList\u0026lt;String\u0026gt; array = new ArrayList\u0026lt;\u0026gt;(); private final ReentrantReadWriteLock lock = new ReentrantReadWriteLock(); private final Lock readLock = lock.readLock(); private final Lock writeLock = lock.writeLock(); public void add(String e){ System.out.println(Thread.currentThread().getName() + \u0026#34; try writeLock lock value \u0026#34;+e); writeLock.lock(); try { array.add(e); } catch (Exception exception) { exception.printStackTrace(); }finally { writeLock.unlock(); System.out.println(Thread.currentThread().getName() + \u0026#34; try writeLock unlock\u0026#34;); } } public String get(int index){ System.out.println(Thread.currentThread().getName() + \u0026#34; try readLock lock\u0026#34;); readLock.lock(); try { return array.get(index); }catch (Exception e){ return new String(\u0026#34;越界访问\u0026#34;); } finally { readLock.unlock(); } } public static void main(String[] args) { ReentrantReadWriteLockList reentrantLockList = new ReentrantReadWriteLockList(); for (int i = 0; i \u0026lt; 3; i++) { new Thread(new Runnable() { @Override public void run() { String s = String.valueOf(ThreadLocalRandom.current().nextInt(100)); reentrantLockList.add(s); } }).start(); final int j = i; new Thread(new Runnable() { @Override public void run() { System.out.println(Thread.currentThread().getName()+ \u0026#34; try readLock unlock value \u0026#34; + reentrantLockList.get(j)); } }).start(); new Thread(new Runnable() { @Override public void run() { System.out.println(Thread.currentThread().getName()+ \u0026#34; try readLock unlock value \u0026#34; + reentrantLockList.get(j)); } }).start(); } } } 运行结果： 小结\nReentrantReadWriteLock 有公平和非公平两种机制，默认使用非公平锁。 在线程持有读锁的情况下，该线程不能取得写锁(因为获取写锁的时候，如果发现当前的读锁被占用，就马上获取失败，不管读锁是不是被当前线程持有)。 在线程持有写锁的情况下，该线程可以继续获取读锁（获取读锁时如果发现写锁被占用，只有写锁没有被当前线程占用的情况才会获取失败）。 读锁能同时被多个线程持有，而写锁是独占锁同一时刻只能有一个线程持有。 锁降级：线程获取写入锁后可以获取读取锁，然后释放写入锁，这样就从写入锁变成了读取锁，从而实现锁降级特性。 ReentrantReadWriteLock 使用int 类型的变量 高16为表示拥有读锁线程数，低16为表示写锁可重入数。 参考\nJava并发编程之美 JDK1.8源码分析之ReentrantReadWriteLock 读写锁——ReentrantReadWriteLock原理详解 ","date":1615334400,"description":"","dir":"post\\Java Concurrent\\","excerpt_html":null,"excerpt_text":null,"expirydate":-62135596800,"fuzzywordcount":2900,"html":null,"keywords":null,"kind":"page","lang":"en","lastmod":1615334400,"objectID":"637a21bfa9771b0b4954c63a2c539046","permalink":"http://localhost:1313/post/java-concurrent/reentrantreadwritelock/","publishdate":"2021-03-10T00:00:00Z","readingtime":6,"relpermalink":"/post/java-concurrent/reentrantreadwritelock/","section":"post","summary":"本文分析JDK1.8中的ReentrantReadWriteLock类 简介 由于ReentrantLock是独占锁，某时只有一个线程可以获取该","tags":["Java并发"],"title":"深入了解ReentrantReadWriteLock","type":"post","url":"/post/java-concurrent/reentrantreadwritelock/","weight":0,"wordcount":2851},{"author":null,"categories":["Code"],"content":" 本篇介绍Java并发的基础知识，主要包括线程安全，共享变量的内存可见性，synchronized和volatile关键字，指令重排序，伪共享等相关知识。\n并发与并行 并发是指同一时间段内多个任务执行。 并行是指同一时刻，多个任务同时执行。 并发是单位时间内，一个CPU切换时间片对多个任务进行处理\n并行是同一时刻，多个CPU对多个任务同时进行处理\n线程安全 共享资源：该资源被多个线程所持有。\n线程安全问题是指当多线程同时读写一个共享资源并且没有任何同步措施时，导致出现脏数据或者其他不可预见的结果的问题\nJava中共享变量的内存可见性 Java内存模型规定，将所有变量存放在主内存中，当线程使用变量时，会把主内存里面的变量复制到自己的工作内存，线程读写变量时操作的是自己工作内存中的变量。 当线程A和线程B同时处理一个共享变量X。\n线程A首先获取共享变量X的值，由于两级Cache都没有命中，所以加载主内存中X的值，假如是0，然后把X=0缓存到二级缓存，并刷新到主内存。此时二级缓存和主内存中X的值都是1。 线程B获取X的值，一级缓存未命中，二级缓存命中，返回X=1。然后线程B将X的值改为2，并缓存到二级缓存，刷新到主内存。此时二级缓存和主内存中X的值都是1 线程A再次获取X的值，一级缓存命中，此时线程A工作内存中的X=1。这样就出现了问题，二级缓存和主内存中X的值已经被线程B修改为2了。这就是共享变量的内存不可见问题，也就是线程B写入的值对线程A不可见。 Java中的原子性操作和指令重排序 所谓原子性操作，是指在执行一系列操作时，要么全部执行，要么全部不执行，不存在只执行其中一部分的情况。\n指令重排序：Java内存模型运行编译器和处理器对指令重排序以提高运行效率，只会对不存在数据依赖的指令重排序。重排序在单线程下可以保证最终的执行结果，在多线程下不能保证。\nsynchronized和volatile关键字 synchronized：\nsynchronized块是Java提供的一种原子性内置锁，内置锁是排它锁，也就是当一个线程获取该锁时，其他线程必须等待该线程释放锁后才能获取该锁。 进入synchronized块的内存语义是把synchronized块内使用到的变量从线程工作内存中清除，这样线程使用到的变量会从主内存中获取。退出synchronized块的内存语义是把synchronized块内对共享变量的修改刷新到主内存。 synchronized关键字保证了原子性、共享变量的内存可见性、有序性。这里注意的是，synchronized没有禁止指令重排序，但是却保证了有序性，这是因为synchronized块中只能有一个线程运行，所以保证了最终执行的结果。 volatile：\n对于解决内存可见性问题，使用锁太笨重，因为它会带来线程上下文切换开销。volatile关键字确保对一个变量的更新对其他线程可见。 写入volatile的内存语义是将写入线程工作内存的变量刷新到主内存，读取volatile的内存语义是先清空线程的工作内存再从主内存中读取。 volatile关键字只保证共享变量的内存可见性，并且禁止指令重排序，但不保证原子性。 伪共享 缓存行（Cache line）：在高速缓存Cache内部，是按行存储的，每一行被称为一个缓存行。缓存行是Cache与主内存进行数据交换的单位。Cache行的大小一般为2的幂次方字节。\n伪共享：当多个线程，修改一个缓存行中的多个变量时，由于同时只能有一个线程操作缓存行（这就没有做到多个线程同时操作多个变量），所以相比将每个变量放到不同的缓存行，性能会下降，这就是伪共享。\nJava中的CAS操作 CAS：compare and swap，是JDK提供的非阻塞原子性操作，它通过硬件保证了比较\u0026ndash;更新操作的原子性。\npublic final native boolean compareAndSwapObject(Object var1, long var2, Object var4, Object var5);\rpublic final native boolean compareAndSwapInt(Object var1, long var2, int var4, int var5);\rpublic final native boolean compareAndSwapLong(Object var1, long var2, long var4, long var6);\rJDK中的Unsafe类提供了这三种CAS方法，有四个操作数，分别为：对象的内存位置，对象的变量的偏移量，变量预期值，变量新的值。\nABA问题：ABA问题是指，线程1获取变量X的值A后在使用CAS修改X的值之前，线程2使用CAS修改X的值为B，然后又使用CAS修改X的值为A，此时线程1获取的X的值A已经不是之前获取的A了。 给每个变量的状态值，配备时间戳可避免ABA问题。\n","date":1615161600,"description":"","dir":"post\\Java Concurrent\\","excerpt_html":null,"excerpt_text":null,"expirydate":-62135596800,"fuzzywordcount":1800,"html":null,"keywords":null,"kind":"page","lang":"en","lastmod":1615161600,"objectID":"d15a18a5772fffc5bac4ac0bccdb55ae","permalink":"http://localhost:1313/post/java-concurrent/java%E5%B9%B6%E5%8F%91%E5%9F%BA%E7%A1%80/","publishdate":"2021-03-08T00:00:00Z","readingtime":4,"relpermalink":"/post/java-concurrent/java%E5%B9%B6%E5%8F%91%E5%9F%BA%E7%A1%80/","section":"post","summary":"本篇介绍Java并发的基础知识，主要包括线程安全，共享变量的内存可见性，synchronized和volatile关键字，指令重排序，伪共享","tags":["Java并发"],"title":"Java并发基础","type":"post","url":"/post/java-concurrent/java%E5%B9%B6%E5%8F%91%E5%9F%BA%E7%A1%80/","weight":0,"wordcount":1783},{"author":null,"categories":["Code"],"content":" 本文分析ThreadLocal的原理和使用\n1.ThreadLocal简介 多线程访问共享变量时容易出现并发问题，为了保证线程安全，一般会给共享变量进行适当的加锁同步。如果不想加锁呢？ ThreadLocal可以做到线程隔离，多个线程访问共享变量时，访问的是自己线程的变量。 ThreadLocal提供了线程本地变量，如果创建了一个ThreadLocal变量，那么访问这个变量的每个线程都会有这个变量的一个本地副本，当多线程操作这个变量时，实际操作的是自己本地内存的变量，从而避免线程安全的问题。\n2.ThreadLocal使用 public class ThreadLocalDemo { static ThreadLocal\u0026lt;String\u0026gt; stringThreadLocal = new ThreadLocal\u0026lt;String\u0026gt;(); public static void main(String[] args) { CountDownLatch countDownLatch = new CountDownLatch(10); for (int i = 0; i \u0026lt; 10; i++) { Thread thread = new Thread(new Runnable() { @Override public void run() { stringThreadLocal.set(Thread.currentThread().getName()); System.out.println(stringThreadLocal.get()); countDownLatch.countDown(); } },\u0026#34;i am thread --\u0026#34;+i); thread.start(); } } } 运行结果 3.ThreadLocal的原理 Thread类中有两个包访问变量，一个是threadLocals ，一个是inheritableThreadLocals，它们都是ThreadLocalMap类型的变量。 而ThreadLocalMap又是ThreadLocal的内部类。 默认情况下，每个线程的这两个变量都为null，只有当线程第一次调用ThreadLocal 的set 或者get方法时才会创建他们。 每个线程的本地变量是存在调用线程的threadLocals变量中的，ThreadLocal通过set方法把value放在调用线程的threadLocals变量中，通过get方法取出调用线程的threadLocals中的值。 Thread里面的threadLocals为何设计为map结构？因为每个线程可以关联多个ThreadLocal变量。\n下面分析下 ThreadLocal 的set、get及remove方法 1.set\npublic void set(T value) { //获取当前调用线程 Thread t = Thread.currentThread(); //将当前线程作为key 去查对应的线程变量threadLocals ThreadLocalMap map = getMap(t); //当前线程的threadLocals不为null if (map != null) //将当前ThreadLocal 对象作为key传入map map.set(this, value); else //创建map createMap(t, value); } ThreadLocalMap getMap(Thread t) { return t.threadLocals; } void createMap(Thread t, T firstValue) { //当前线程的threadLocals 赋值 以当前ThreadLocal 对象作为key 创建的ThreadLocalMap t.threadLocals = new ThreadLocalMap(this, firstValue); } //ThreadLocalMap的构造函数 ThreadLocalMap(ThreadLocal\u0026lt;?\u0026gt; firstKey, Object firstValue) { //Entry为ThreadLocalMap的内部类 INITIAL_CAPACITY = 16 table = new Entry[INITIAL_CAPACITY]; //计算应该存放的位置 i 因INITIAL_CAPACITY = 16 相当于对 16 取余 int i = firstKey.threadLocalHashCode \u0026amp; (INITIAL_CAPACITY - 1); //存放到table[i] table[i] = new Entry(firstKey, firstValue); size = 1; setThreshold(INITIAL_CAPACITY); } 2.get\npublic T get() { //获取当前调用线程 Thread t = Thread.currentThread(); //将当前线程作为key 去查对应的线程变量threadLocals ThreadLocalMap map = getMap(t); if (map != null) { //以当前ThreadLocal对象作为key 去取 map中的 entry ThreadLocalMap.Entry e = map.getEntry(this); if (e != null) { @SuppressWarnings(\u0026#34;unchecked\u0026#34;) T result = (T)e.value; return result; } } return setInitialValue(); } private T setInitialValue() { T value = initialValue(); Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else createMap(t, value); return value; } remove\npublic void remove() { ThreadLocalMap m = getMap(Thread.currentThread()); if (m != null) m.remove(this); } private void remove(ThreadLocal\u0026lt;?\u0026gt; key) { //拿到table数组 Entry[] tab = table; int len = tab.length; //找到在数组中存放的位置 i int i = key.threadLocalHashCode \u0026amp; (len-1); for (Entry e = tab[i]; e != null; e = tab[i = nextIndex(i, len)]) { //判断key是否相等 if (e.get() == key) { //清除 e.clear(); expungeStaleEntry(i); return; } } } ThreadLocalMap内部类 Enrtry\nstatic class Entry extends WeakReference\u0026lt;ThreadLocal\u0026lt;?\u0026gt;\u0026gt; { Object value; Entry(ThreadLocal\u0026lt;?\u0026gt; k, Object v) { super(k); value = v; } } Enrtry 继承自软引用，当对应的ThreadLocal对象为null时，此Entry对象会被JVM回收，避免出现内存泄漏。\n4.ThreadLocal不支持继承性 public class TestThreadLocal { public static ThreadLocal\u0026lt;String\u0026gt; threadLocal = new ThreadLocal\u0026lt;String\u0026gt;(); public static void main(String[] args) { threadLocal.set(\u0026#34;hello\u0026#34;); new Thread(new Runnable() { @Override public void run() { System.out.println(\u0026#34;sub thread: \u0026#34; + threadLocal.get()); } }).start(); System.out.println(\u0026#34;main: \u0026#34;+ threadLocal.get()); } } 也就是说，同一个ThreadLocal变量在父线程中被设置值后，在子线程中是获取不到的。\n5.InheritableThreadLocal类 利用InheritableThreadLocal类，子线程可以访问父线程中的本地变量。\n//继承ThreadLocal类 public class InheritableThreadLocal\u0026lt;T\u0026gt; extends ThreadLocal\u0026lt;T\u0026gt; { protected T childValue(T parentValue) { return parentValue; } //返回当前线程的inheritableThreadLocals变量 ThreadLocalMap getMap(Thread t) { return t.inheritableThreadLocals; } //初始化当前线程的inheritableThreadLocals变量 void createMap(Thread t, T firstValue) { t.inheritableThreadLocals = new ThreadLocalMap(this, firstValue); } } 当子线程初始化时会判断父线程的inheritableThreadLocals变量是否为null，不为null 则会赋值给子线程inheritableThreadLocals变量 static ThreadLocalMap createInheritedMap(ThreadLocalMap parentMap) { //这个构造函数 仅此方法createInheritedMap调用 return new ThreadLocalMap(parentMap); } private ThreadLocalMap(ThreadLocalMap parentMap) { Entry[] parentTable = parentMap.table; int len = parentTable.length; setThreshold(len); table = new Entry[len]; for (int j = 0; j \u0026lt; len; j++) { Entry e = parentTable[j]; if (e != null) { @SuppressWarnings(\u0026#34;unchecked\u0026#34;) ThreadLocal\u0026lt;Object\u0026gt; key = (ThreadLocal\u0026lt;Object\u0026gt;) e.get(); if (key != null) { //这里调用InheritableThreadLocal类覆盖的 childValue方法 Object value = key.childValue(e.value); Entry c = new Entry(key, value); int h = key.threadLocalHashCode \u0026amp; (len - 1); while (table[h] != null) h = nextIndex(h, len); table[h] = c; size++; } } } } 改为InheritableThreadLocal 运行\npublic class TestInheritableThreadLocal { public static ThreadLocal\u0026lt;String\u0026gt; threadLocal = new InheritableThreadLocal\u0026lt;\u0026gt;(); public static void main(String[] args) { threadLocal.set(\u0026#34;hello\u0026#34;); new Thread(new Runnable() { @Override public void run() { System.out.println(\u0026#34;sub thread: \u0026#34; + threadLocal.get()); } }).start(); System.out.println(\u0026#34;main: \u0026#34;+ threadLocal.get()); } } ","date":1614902400,"description":"","dir":"post\\Java Concurrent\\","excerpt_html":null,"excerpt_text":null,"expirydate":-62135596800,"fuzzywordcount":2000,"html":null,"keywords":null,"kind":"page","lang":"en","lastmod":1614902400,"objectID":"cc9f436f0b5fd081016fe104dd156ad8","permalink":"http://localhost:1313/post/java-concurrent/threadlocal/","publishdate":"2021-03-05T00:00:00Z","readingtime":4,"relpermalink":"/post/java-concurrent/threadlocal/","section":"post","summary":"本文分析ThreadLocal的原理和使用 1.ThreadLocal简介 多线程访问共享变量时容易出现并发问题，为了保证线程安全，一般会给共享","tags":["Java并发"],"title":"深入了解ThreadLocal","type":"post","url":"/post/java-concurrent/threadlocal/","weight":0,"wordcount":1981},{"author":null,"categories":["Code"],"content":" 本文将深入源码分析ConcurrentHashMap的相关内容\n1.ConcurrentHashMap简介 由于HashMap是非线程安全的，所以如果想在多线程下安全的操作Map，有下面几个解决方案：\n使用HashTable 使用Collections.synchronizedMap 使用ConcurrentHashMap HashTable HashTable类是一个线程安全的类，它的底层给几乎所有的多线程操作方法都加上了synchronized关键字，相当于锁住整个HashTable，多线程访问时，只要有一个线程访问或操作该对象，其他线程只能阻塞等待锁的释放，性能非常差，所以HashTable不推荐使用。\nCollections.synchronizedMap 底层也是使用对象锁来保证线程安全，本质上也相当于是全表锁。\nCocurrentHashMap JDK1.7: 在JDK1.7中，采用分段锁。所谓分段锁，是将HashMap中的Entry数组进行切割，分成许多小数组即Segment,Segment继承ReetrantLock（可重入锁）。 JDK1.8 在JDK1.8中，取消了Segment分段锁，采用CAS+synchronized来保证并发安全，synchronized只锁住table数组中链表或者红黑树的头节点，只要插入节点的hash不冲突,就不会产生线程竞争。\njdk1.8中的ConcurrentHashMap相比于jdk1.7 锁的粒度更小，性能更好。\n2.底层数据结构 同jdk1.8中的HashMap一样，底层也采用了数组+链表/红黑树的数据结构，这样当hash冲突较多时，查询效率会更好。\nNode和TreeNode同HashMap中的差不多，不过Node中的Value 和 next 用 volatile修饰\nstatic class Node\u0026lt;K,V\u0026gt; implements Map.Entry\u0026lt;K,V\u0026gt; { final int hash; final K key; //val和next都会在扩容时发生变化，所以加上volatile来保持可见性和禁止重排序 volatile V val; volatile Node\u0026lt;K,V\u0026gt; next; Node(int hash, K key, V val, Node\u0026lt;K,V\u0026gt; next) { this.hash = hash; this.key = key; this.val = val; this.next = next; } public final K getKey() { return key; } public final V getValue() { return val; } public final int hashCode() { return key.hashCode() ^ val.hashCode(); } public final String toString(){ return key + \u0026#34;=\u0026#34; + val; } public final V setValue(V value) { throw new UnsupportedOperationException(); } public final boolean equals(Object o) { Object k, v, u; Map.Entry\u0026lt;?,?\u0026gt; e; return ((o instanceof Map.Entry) \u0026amp;\u0026amp; (k = (e = (Map.Entry\u0026lt;?,?\u0026gt;)o).getKey()) != null \u0026amp;\u0026amp; (v = e.getValue()) != null \u0026amp;\u0026amp; (k == key || k.equals(key)) \u0026amp;\u0026amp; (v == (u = val) || v.equals(u))); } /** * Virtualized support for map.get(); overridden in subclasses. */ Node\u0026lt;K,V\u0026gt; find(int h, Object k) { Node\u0026lt;K,V\u0026gt; e = this; if (k != null) { do { K ek; if (e.hash == h \u0026amp;\u0026amp; ((ek = e.key) == k || (ek != null \u0026amp;\u0026amp; k.equals(ek)))) return e; } while ((e = e.next) != null); } return null; } } TreeBin TreeBin并不是红黑树的存储节点，TreeBin通过root属性维护红黑树的根结点，因为红黑树在旋转的时候，根结点可能会被它原来的子节点替换掉，在这个时间点，如果有其他线程要写这棵红黑树就会发生线程不安全问题，所以在ConcurrentHashMap中TreeBin通过waiter属性维护当前使用这棵红黑树的线程，来防止其他线程的进入。\nstatic final class TreeBin\u0026lt;K,V\u0026gt; extends Node\u0026lt;K,V\u0026gt; { //指向TreeNode链表的根节点 TreeNode\u0026lt;K,V\u0026gt; root; volatile TreeNode\u0026lt;K,V\u0026gt; first; volatile Thread waiter; volatile int lockState; // 锁的状态 static final int WRITER = 1; // 持有写锁时的状态 static final int WAITER = 2; // 等待写锁时的状态 static final int READER = 4; // 增加数据时读锁的状态 //构造函数 hash未 TREEBIN = -2，以b节点为头节点， 代表红黑树头节点hash\u0026lt;0 TreeBin(TreeNode\u0026lt;K,V\u0026gt; b) { super(TREEBIN, null, null, null); this.first = b; TreeNode\u0026lt;K,V\u0026gt; r = null; for (TreeNode\u0026lt;K,V\u0026gt; x = b, next; x != null; x = next) { next = (TreeNode\u0026lt;K,V\u0026gt;)x.next; x.left = x.right = null; if (r == null) { x.parent = null; x.red = false; r = x; } else { K k = x.key; int h = x.hash; Class\u0026lt;?\u0026gt; kc = null; for (TreeNode\u0026lt;K,V\u0026gt; p = r;;) { int dir, ph; K pk = p.key; if ((ph = p.hash) \u0026gt; h) dir = -1; else if (ph \u0026lt; h) dir = 1; else if ((kc == null \u0026amp;\u0026amp; (kc = comparableClassFor(k)) == null) || (dir = compareComparables(kc, k, pk)) == 0) dir = tieBreakOrder(k, pk); TreeNode\u0026lt;K,V\u0026gt; xp = p; if ((p = (dir \u0026lt;= 0) ? p.left : p.right) == null) { x.parent = xp; if (dir \u0026lt;= 0) xp.left = x; else xp.right = x; r = balanceInsertion(r, x); break; } } } } this.root = r; assert checkInvariants(root); } } ForwardingNode 扩容用到的数据结构，代表正在进行扩容\nstatic final class ForwardingNode\u0026lt;K,V\u0026gt; extends Node\u0026lt;K,V\u0026gt; { final Node\u0026lt;K,V\u0026gt;[] nextTable; //hash 为 MOVED = -1 代变正在进行扩容 ForwardingNode(Node\u0026lt;K,V\u0026gt;[] tab) { super(MOVED, null, null, null); this.nextTable = tab; } } 3.常用方法 put方法\npublic V put(K key, V value) { return putVal(key, value, false); } final V putVal(K key, V value, boolean onlyIfAbsent) { //key和value 都不能为null if (key == null || value == null) throw new NullPointerException(); //获取key 的 hash int hash = spread(key.hashCode()); int binCount = 0; for (Node\u0026lt;K,V\u0026gt;[] tab = table;;) { Node\u0026lt;K,V\u0026gt; f; int n, i, fh; if (tab == null || (n = tab.length) == 0) //table 为 null 或者长度为 0 初始化table tab = initTable(); // f 代表数组 hash\u0026amp;（n-1）位置的元素，如果f为null 则调用casTabAt方法利用Unsafe.compareAndSwapObject插入Node节点 else if ((f = tabAt(tab, i = (n - 1) \u0026amp; hash)) == null) { if (casTabAt(tab, i, null, new Node\u0026lt;K,V\u0026gt;(hash, key, value, null))) break; // no lock when adding to empty bin } //MOVED = -1 如果f.hash等于 -1 意味着有其它线程正在扩容，则当前线程一起进行扩容 else if ((fh = f.hash) == MOVED) //如果在进行扩容，则先进行扩容操作 tab = helpTransfer(tab, f); else { V oldVal = null; //锁住链表或红黑树的头节点 synchronized (f) { //再次确认，防止其他线程修改 if (tabAt(tab, i) == f) { //hash \u0026gt;=0 说明时链表的节点，如果有相等的key，则修改它的value，否则在链表尾部插入 if (fh \u0026gt;= 0) { binCount = 1; for (Node\u0026lt;K,V\u0026gt; e = f;; ++binCount) { K ek; if (e.hash == hash \u0026amp;\u0026amp; ((ek = e.key) == key || (ek != null \u0026amp;\u0026amp; key.equals(ek)))) { oldVal = e.val; if (!onlyIfAbsent) e.val = value; break; } Node\u0026lt;K,V\u0026gt; pred = e; if ((e = e.next) == null) { pred.next = new Node\u0026lt;K,V\u0026gt;(hash, key, value, null); break; } } } // f 是 TreeBin类型，则f为红黑树根节点 else if (f instanceof TreeBin) { Node\u0026lt;K,V\u0026gt; p; binCount = 2; if ((p = ((TreeBin\u0026lt;K,V\u0026gt;)f).putTreeVal(hash, key, value)) != null) { oldVal = p.val; if (!onlyIfAbsent) p.val = value; } } } } if (binCount != 0) { //binCount \u0026gt;= TREEIFY_THRESHOLD(默认是8) 则进行链表转红黑树操作 if (binCount \u0026gt;= TREEIFY_THRESHOLD) treeifyBin(tab, i); if (oldVal != null) return oldVal; break; } } } //扩容判断 addCount(1L, binCount); return null; } 对要存放的元素，利用spread方法对key的hashcode进行一次hash运算，由运算后的 hash\u0026amp;（n-1）来确定这个元素应该存放在数组中的位置 如果当前table没有初始化，则先初始化数组table 如果数组当前位置为null，则使用CAS操作直接放入 如果这个位置存在节点，说明发生hash碰撞，首先根据此位置元素的hash判断数组是否正在进行扩容（(fh = f.hash) == MOVED），如果正在进行扩容，则一起进行扩容 如果没正在扩容，则判断当前节点是否为链表节点，依次向后遍历确定这个新加入的值所在位置。如果遇到hash值与key值都与新加入节点是一致的情况，则只需要更新value值即可。否则依次向后遍历，直到链表尾插入这个结点； 如果这个节点的类型是TreeBin的话，直接调用红黑树的插入方法进行插入新的节点； 插入完节点之后再次检查链表长度，如果长度大于8，就把这个链表转换成红黑树； 对当前容器存放的元素容量进行检查，如果超过临界值（实际大小 * 加载因子）就需要进行扩容 initTable方法 初始化table数组\nprivate final Node\u0026lt;K,V\u0026gt;[] initTable() { Node\u0026lt;K,V\u0026gt;[] tab; int sc; //table为初始化才进行初始化 while ((tab = table) == null || tab.length == 0) { //sizeCtl默认为0 ，使用volatile修饰，当sizeCtl《0时，代表其他线程正在初始化，当前线程只需让出CPU时间片 if ((sc = sizeCtl) \u0026lt; 0) Thread.yield(); // 利用UnSafe.compareAndSwapInt,更改SIZECTL值为 -1 代表此时有线程在进行扩容 else if (U.compareAndSwapInt(this, SIZECTL, sc, -1)) { try { //再次确认table未初始化 if ((tab = table) == null || tab.length == 0) { int n = (sc \u0026gt; 0) ? sc : DEFAULT_CAPACITY; @SuppressWarnings(\u0026#34;unchecked\u0026#34;) Node\u0026lt;K,V\u0026gt;[] nt = (Node\u0026lt;K,V\u0026gt;[])new Node\u0026lt;?,?\u0026gt;[n]; table = tab = nt; sc = n - (n \u0026gt;\u0026gt;\u0026gt; 2); } } finally { sizeCtl = sc; } break; } } return tab; } helpTransfer方法 帮助扩容\nfinal Node\u0026lt;K,V\u0026gt;[] helpTransfer(Node\u0026lt;K,V\u0026gt;[] tab, Node\u0026lt;K,V\u0026gt; f) { Node\u0026lt;K,V\u0026gt;[] nextTab; int sc; // f 是 ForWardingNode 类型 且 f的nextTable 不为 null if (tab != null \u0026amp;\u0026amp; (f instanceof ForwardingNode) \u0026amp;\u0026amp; (nextTab = ((ForwardingNode\u0026lt;K,V\u0026gt;)f).nextTable) != null) { //帮忙扩容，得到一个标识 int rs = resizeStamp(tab.length); // 如果 nextTab 没有被并发修改 且 tab 也没有被并发修改 // 且 sizeCtl \u0026lt; 0 （说明还在扩容） 自旋 while (nextTab == nextTable \u0026amp;\u0026amp; table == tab \u0026amp;\u0026amp; (sc = sizeCtl) \u0026lt; 0) { // 如果 sizeCtl 无符号右移 16 不等于 rs （ sc前 16 位如果不等于标识符，则标识符变化了） // 或者 sizeCtl == rs + 1 （扩容结束了，不再有线程进行扩容）（默认第一个线程设置 sc ==rs 左移 16 位 + 2，当第一个线程结束扩容了，就会将 sc 减一。这个时候，sc 就等于 rs + 1） // 或者 sizeCtl == rs + 65535 （如果达到最大帮助线程的数量，即 65535） // 或者转移下标正在调整 （扩容结束） // 结束循环，返回 table if ((sc \u0026gt;\u0026gt;\u0026gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 || sc == rs + MAX_RESIZERS || transferIndex \u0026lt;= 0) break; // 如果以上都不是, 将 sizeCtl + 1, （表示增加了一个线程帮助其扩容） if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1)) { transfer(tab, nextTab); break; } } return nextTab; } return table; } static final int resizeStamp(int n) { return Integer.numberOfLeadingZeros(n) | (1 \u0026lt;\u0026lt; (RESIZE_STAMP_BITS - 1)); } 关于 sizeCtl 变量：\n高RESIZE_STAMP_BITS位 低RESIZE_STAMP_SHIFT位 扩容标记 并行扩容线程数 + 1 resizeStamp 方法返回一个与table容量n大小有关的扩容标记\nInteger.numberOfLeadingZeros(n)用于获取当前int从高位到低位第一个1前面0的个数。 RESIZE_STAMP_BITS = 16 ， 1 \u0026laquo; (RESIZE_STAMP_BITS - 1) 后的结果是 1左移15位 也就是 0000 0000 0000 0000 1000 0000 0000 0000 addCount()方法 put完元素的最后，对当前元素容量大小进行检查，判断是否需要扩容\nprivate final void addCount(long x, int check) { CounterCell[] as; long b, s; // s = sumCount() 统计容器中元素的个数，并将 BASECOUNT +1 if ((as = counterCells) != null || !U.compareAndSwapLong(this, BASECOUNT, b = baseCount, s = b + x)) { CounterCell a; long v; int m; boolean uncontended = true; if (as == null || (m = as.length - 1) \u0026lt; 0 || (a = as[ThreadLocalRandom.getProbe() \u0026amp; m]) == null || !(uncontended = U.compareAndSwapLong(a, CELLVALUE, v = a.value, v + x))) { fullAddCount(x, uncontended); return; } if (check \u0026lt;= 1) return; s = sumCount(); } //check就是binCount，该值在`putVal()`里面一定是\u0026gt;=0的，所以这个条件一定会为true if (check \u0026gt;= 0) { Node\u0026lt;K,V\u0026gt;[] tab, nt; int n, sc; //自旋 while (s \u0026gt;= (long)(sc = sizeCtl) \u0026amp;\u0026amp; (tab = table) != null \u0026amp;\u0026amp; (n = tab.length) \u0026lt; MAXIMUM_CAPACITY) { int rs = resizeStamp(n); //已经有线程进行扩容 if (sc \u0026lt; 0) { if ((sc \u0026gt;\u0026gt;\u0026gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 || sc == rs + MAX_RESIZERS || (nt = nextTable) == null || transferIndex \u0026lt;= 0) break; //CAS SIZECTL 增加一个线程帮助扩容 if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1)) transfer(tab, nt); } // 它是第一个扩容的线程， SIZECTL 低16位 置为 0000 0000 0000 0010 else if (U.compareAndSwapInt(this, SIZECTL, sc, (rs \u0026lt;\u0026lt; RESIZE_STAMP_SHIFT) + 2)) transfer(tab, null); s = sumCount(); } } } transfer() 方法 扩容方法\nprivate final void transfer(Node\u0026lt;K,V\u0026gt;[] tab, Node\u0026lt;K,V\u0026gt;[] nextTab) { int n = tab.length, stride; // 将 n / 8 然后除以 CPU核心数。如果得到的结果小于 16，那么就使用 16。 // 这里的目的是让每个 CPU 处理的桶一样多，避免出现转移任务不均匀的现象，如果桶较少的话，默认一个 CPU（一个线程）处理 16 个桶 if ((stride = (NCPU \u0026gt; 1) ? (n \u0026gt;\u0026gt;\u0026gt; 3) / NCPU : n) \u0026lt; MIN_TRANSFER_STRIDE) stride = MIN_TRANSFER_STRIDE; // subdivide range // 新的 table 尚未初始化 if (nextTab == null) { // initiating try { @SuppressWarnings(\u0026#34;unchecked\u0026#34;) //新tab大小 为原来的2倍 Node\u0026lt;K,V\u0026gt;[] nt = (Node\u0026lt;K,V\u0026gt;[])new Node\u0026lt;?,?\u0026gt;[n \u0026lt;\u0026lt; 1]; nextTab = nt; } catch (Throwable ex) { // try to cope with OOME sizeCtl = Integer.MAX_VALUE; return; } nextTable = nextTab; transferIndex = n; } int nextn = nextTab.length; ForwardingNode\u0026lt;K,V\u0026gt; fwd = new ForwardingNode\u0026lt;K,V\u0026gt;(nextTab); boolean advance = true; boolean finishing = false; // to ensure sweep before committing nextTab // 死循环,i 表示下标，bound 表示当前线程可以处理的当前桶区间最小下标 for (int i = 0, bound = 0;;) { Node\u0026lt;K,V\u0026gt; f; int fh; while (advance) { int nextIndex, nextBound; // 对 i 减一，判断是否大于等于 bound （正常情况下，如果大于 bound 不成立，说明该线程上次领取的任务已经完成了。那么，需要在下面继续领取任务） // 如果对 i 减一大于等于 bound（还需要继续做任务），或者完成了，修改推进状态为 false，不能推进了。任务成功后修改推进状态为 true。 // 通常，第一次进入循环，i-- 这个判断会无法通过，从而走下面的 nextIndex 赋值操作（获取最新的转移下标）。其余情况都是：如果可以推进，将 i 减一，然后修改成不可推进。如果 i 对应的桶处理成功了，改成可以推进。 if (--i \u0026gt;= bound || finishing) advance = false; else if ((nextIndex = transferIndex) \u0026lt;= 0) { i = -1; advance = false; } else if (U.compareAndSwapInt (this, TRANSFERINDEX, nextIndex, nextBound = (nextIndex \u0026gt; stride ? nextIndex - stride : 0))) { bound = nextBound; i = nextIndex - 1; advance = false; } } if (i \u0026lt; 0 || i \u0026gt;= n || i + n \u0026gt;= nextn) { int sc; if (finishing) { nextTable = null; table = nextTab; sizeCtl = (n \u0026lt;\u0026lt; 1) - (n \u0026gt;\u0026gt;\u0026gt; 1); return; } if (U.compareAndSwapInt(this, SIZECTL, sc = sizeCtl, sc - 1)) { if ((sc - 2) != resizeStamp(n) \u0026lt;\u0026lt; RESIZE_STAMP_SHIFT) return; finishing = advance = true; i = n; // recheck before commit } } else if ((f = tabAt(tab, i)) == null) advance = casTabAt(tab, i, null, fwd); else if ((fh = f.hash) == MOVED) advance = true; // already processed else { synchronized (f) { if (tabAt(tab, i) == f) { Node\u0026lt;K,V\u0026gt; ln, hn; if (fh \u0026gt;= 0) { // hash \u0026amp; n 之后，因 n为 2^m 判断 m位 为0 还是1 int runBit = fh \u0026amp; n; Node\u0026lt;K,V\u0026gt; lastRun = f; for (Node\u0026lt;K,V\u0026gt; p = f.next; p != null; p = p.next) { int b = p.hash \u0026amp; n; if (b != runBit) { runBit = b; lastRun = p; } } //为0 则 低位是 lastRun if (runBit == 0) { ln = lastRun; hn = null; } //否则 高位是 lastRun else { hn = lastRun; ln = null; } //遍历链表 找到ln 或 hn for (Node\u0026lt;K,V\u0026gt; p = f; p != lastRun; p = p.next) { int ph = p.hash; K pk = p.key; V pv = p.val; if ((ph \u0026amp; n) == 0) ln = new Node\u0026lt;K,V\u0026gt;(ph, pk, pv, ln); else hn = new Node\u0026lt;K,V\u0026gt;(ph, pk, pv, hn); } //利用CAS 交换到nextTab 并 将tab[i] 标记为正在扩容 setTabAt(nextTab, i, ln); setTabAt(nextTab, i + n, hn); setTabAt(tab, i, fwd); advance = true; } else if (f instanceof TreeBin) { TreeBin\u0026lt;K,V\u0026gt; t = (TreeBin\u0026lt;K,V\u0026gt;)f; TreeNode\u0026lt;K,V\u0026gt; lo = null, loTail = null; TreeNode\u0026lt;K,V\u0026gt; hi = null, hiTail = null; int lc = 0, hc = 0; for (Node\u0026lt;K,V\u0026gt; e = t.first; e != null; e = e.next) { int h = e.hash; TreeNode\u0026lt;K,V\u0026gt; p = new TreeNode\u0026lt;K,V\u0026gt; (h, e.key, e.val, null, null); if ((h \u0026amp; n) == 0) { if ((p.prev = loTail) == null) lo = p; else loTail.next = p; loTail = p; ++lc; } else { if ((p.prev = hiTail) == null) hi = p; else hiTail.next = p; hiTail = p; ++hc; } } ln = (lc \u0026lt;= UNTREEIFY_THRESHOLD) ? untreeify(lo) : (hc != 0) ? new TreeBin\u0026lt;K,V\u0026gt;(lo) : t; hn = (hc \u0026lt;= UNTREEIFY_THRESHOLD) ? untreeify(hi) : (lc != 0) ? new TreeBin\u0026lt;K,V\u0026gt;(hi) : t; setTabAt(nextTab, i, ln); setTabAt(nextTab, i + n, hn); setTabAt(tab, i, fwd); advance = true; } } } } } } 通过计算 CPU 核心数和 Map 数组的长度得到每个线程（CPU）要帮助处理多少个桶，并且这里每个线程处理都是平均的。默认每个线程处理 16 个桶。因此，如果长度是 16 的时候，扩容的时候只会有一个线程扩容。\n初始化临时变量 nextTable。将其在原有基础上扩容两倍。\n死循环开始转移。多线程并发转移就是在这个死循环中，根据一个 finishing 变量来判断，该变量为 true 表示扩容结束，否则继续扩容。\n3.1 进入一个 while 循环，分配数组中一个桶的区间给线程，默认是 16. 从大到小进行分配。当拿到分配值后，进行 i\u0026ndash; 递减。这个 i 就是数组下标。（其中有一个 bound 参数，这个参数指的是该线程此次可以处理的区间的最小下标，超过这个下标，就需要重新领取区间或者结束扩容，还有一个 advance 参数，该参数指的是是否继续递减转移下一个桶，如果为 true，表示可以继续向后推进，反之，说明还没有处理好当前桶，不能推进)\n3.2 出 while 循环，进 if 判断，判断扩容是否结束，如果扩容结束，清空临死变量，更新 table 变量，更新库容阈值。如果没完成，但已经无法领取区间（没了），该线程退出该方法，并将 sizeCtl 减一，表示扩容的线程少一个了。如果减完这个数以后，sizeCtl 回归了初始状态，表示没有线程再扩容了，该方法所有的线程扩容结束了。（这里主要是判断扩容任务是否结束，如果结束了就让线程退出该方法，并更新相关变量）。然后检查所有的桶，防止遗漏。\n3.3 如果没有完成任务，且 i 对应的槽位是空，尝试 CAS 插入占位符，让 putVal 方法的线程感知。\n3.4 如果 i 对应的槽位不是空，且有了占位符，那么该线程跳过这个槽位，处理下一个槽位。\n3.5 如果以上都是不是，说明这个槽位有一个实际的值。开始同步处理这个桶。\n3.6 到这里，都还没有对桶内数据进行转移，只是计算了下标和处理区间，然后一些完成状态判断。同时，如果对应下标内没有数据或已经被占位了，就跳过了。\n锁住头结点，同步处理\n4.1 链表，那么就将这个链表根据 length 取于拆成两份，取于结果是 0 的放在新表的低位，取于结果是 1 放在新表的高位。\n4.2 红黑数，那么也拆成 2 份，方式和链表的方式一样，然后，判断拆分过的树的节点数量，如果数量小于等于 6，改造成链表。反之，继续使用红黑树结构。\n4.小结 不采用Segment而采用内部类Node作为数组，锁住链表或红黑树的头结点来减小锁粒度 不允许有 null Key 和 null Value 扩容时，允许多线程帮助进行扩容，新数组长度为原来的2倍 大量使用了CAS相关操作，定义了许多特殊的数据结构和变量，比如ForwardingNode和sizeCtl 参考链接\nConcurrentHashMap#transfer() 扩容逐行分析 为并发而生的 ConcurrentHashMap（Java 8） ConcurrentHashMap原理分析 ","date":1614729600,"description":"","dir":"post\\Java Base\\","excerpt_html":null,"excerpt_text":null,"expirydate":-62135596800,"fuzzywordcount":5800,"html":null,"keywords":null,"kind":"page","lang":"en","lastmod":1614729600,"objectID":"61d4c81c2b0ef03cbb4711f0c184b5ab","permalink":"http://localhost:1313/post/java-base/concurrenthashmap/","publishdate":"2021-03-03T00:00:00Z","readingtime":12,"relpermalink":"/post/java-base/concurrenthashmap/","section":"post","summary":"本文将深入源码分析ConcurrentHashMap的相关内容 1.ConcurrentHashMap简介 由于HashMap是非线程安全的，所","tags":["Java集合"],"title":"深入了解ConcurrentHashMap","type":"post","url":"/post/java-base/concurrenthashmap/","weight":0,"wordcount":5750},{"author":null,"categories":["Code"],"content":" 本篇分析HashMap的 hash()函数 和 底层数据结构 以及 常用方法 和 常见面试相关题目\n1. HashMap简介 HashMap 是一个K，V键值对的常用集合类，它实现了Map接口。 jdk1.8 之前 HashMap 采用 数组 + 链表 的方式实现，链表存储key值冲突的数据。 jdk1.8 采用 数组 + 链表 / 红黑树 的方式实现，在满足下面两个条件之后，会执行链表转红黑树操作，以此来加快搜索速度。\n链表长度大于阈值（默认为 8） HashMap 数组长度超过 64 2. HashMap底层数据结构 类的属性\npublic class HashMap\u0026lt;K,V\u0026gt; extends AbstractMap\u0026lt;K,V\u0026gt; implements Map\u0026lt;K,V\u0026gt;, Cloneable, Serializable { // 默认的初始容量是16 static final int DEFAULT_INITIAL_CAPACITY = 1 \u0026lt;\u0026lt; 4; // 最大容量 static final int MAXIMUM_CAPACITY = 1 \u0026lt;\u0026lt; 30; // 默认的填充因子 static final float DEFAULT_LOAD_FACTOR = 0.75f; // 当桶(bucket)上的结点数大于这个值时会转成红黑树 static final int TREEIFY_THRESHOLD = 8; // 当桶(bucket)上的结点数小于这个值时树转链表 static final int UNTREEIFY_THRESHOLD = 6; // 桶中结构转化为红黑树对应的table的最小大小 static final int MIN_TREEIFY_CAPACITY = 64; // 存储元素的数组，大小总是2的幂次倍 transient Node\u0026lt;k,v\u0026gt;[] table; // 存放具体元素的集 transient Set\u0026lt;map.entry\u0026lt;k,v\u0026gt;\u0026gt; entrySet; // 存放元素的个数，注意这个不等于数组的长度。 transient int size; // 每次扩容和更改map结构的计数器 transient int modCount; // 临界值 当实际大小(容量*填充因子)超过临界值时，会进行扩容 int threshold; // 加载因子 final float loadFactor; } 一些内部类\nEntrySet、EntryIterator、EntrySpliterator\t键值Set 迭代器 分离器s KeySet、KeyIterator、KeySpliterator\t键Set 迭代器 分离器 Values、ValueIterator、ValueSpliterator\t值Set 迭代器 分离器 使用\npublic class TestMap { public static void main(String[] args) { HashMap\u0026lt;String, String\u0026gt; test = new HashMap\u0026lt;\u0026gt;(); System.out.println(test.size()); test.put(\u0026#34;A\u0026#34;,\u0026#34;AAA\u0026#34;); test.put(\u0026#34;B\u0026#34;,\u0026#34;BBB\u0026#34;); test.put(\u0026#34;C\u0026#34;,\u0026#34;CCC\u0026#34;); test.put(\u0026#34;D\u0026#34;,\u0026#34;DDD\u0026#34;); test.put(\u0026#34;E\u0026#34;,\u0026#34;EEE\u0026#34;); Iterator\u0026lt;Map.Entry\u0026lt;String, String\u0026gt;\u0026gt; iterator = test.entrySet().iterator(); while (iterator.hasNext()){ System.out.println(iterator.next()); } Iterator\u0026lt;String\u0026gt; iterator1 = test.keySet().iterator(); while (iterator1.hasNext()){ System.out.println(iterator1.next()); } Spliterator\u0026lt;String\u0026gt; spliterator = test.values().spliterator(); System.out.println(spliterator.estimateSize()); spliterator.forEachRemaining(System.out::println); System.out.println(test.size()); } } Node节点类\nstatic class Node\u0026lt;K, V\u0026gt; implements Entry\u0026lt;K, V\u0026gt; { final int hash; //hash值，存放元素时用来与其他元素的hash值进行比较。 final K key; // 键 key 唯一 一个key 一个 value 允许有一个 key为 null V value; // 值 HashMap.Node\u0026lt;K, V\u0026gt; next; //指向下一个Node Node(int var1, K var2, V var3, HashMap.Node\u0026lt;K, V\u0026gt; var4) { this.hash = var1; this.key = var2; this.value = var3; this.next = var4; } public final K getKey() { return this.key; } public final V getValue() { return this.value; } public final String toString() { return this.key + \u0026#34;=\u0026#34; + this.value; } public final int hashCode() { return Objects.hashCode(this.key) ^ Objects.hashCode(this.value); } public final V setValue(V var1) { Object var2 = this.value; this.value = var1; return var2; } public final boolean equals(Object var1) { if (var1 == this) { return true; } else { if (var1 instanceof Entry) { Entry var2 = (Entry)var1; if (Objects.equals(this.key, var2.getKey()) \u0026amp;\u0026amp; Objects.equals(this.value, var2.getValue())) { return true; } } return false; } } } TreeNode源码\n// 一些方法这里就不贴了 static final class TreeNode\u0026lt;K,V\u0026gt; extends LinkedHashMap.Entry\u0026lt;K,V\u0026gt; { TreeNode\u0026lt;K,V\u0026gt; parent; // 红黑树 父节点 TreeNode\u0026lt;K,V\u0026gt; left; TreeNode\u0026lt;K,V\u0026gt; right; TreeNode\u0026lt;K,V\u0026gt; prev; // 删除后需要取消链接 boolean red; TreeNode(int hash, K key, V val, Node\u0026lt;K,V\u0026gt; next) { super(hash, key, val, next); } } 数组+链表 Node\u0026lt;k,v\u0026gt;[] table\n数组+红黑树 3.HashMap常用方法 构造函数\n/** * 构造一个具有指定初始容量和负载因子的空HashMap 。 */ public HashMap(int initialCapacity, float loadFactor) { //如果初始化大小小于0，抛出异常 if (initialCapacity \u0026lt; 0) throw new IllegalArgumentException(\u0026#34;Illegal initial capacity: \u0026#34; + initialCapacity); //HashMap 中table的最大值为2^30 如果初始化大小大于2^30，则为2^30 if (initialCapacity \u0026gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; if (loadFactor \u0026lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException(\u0026#34;Illegal load factor: \u0026#34; + loadFactor); this.loadFactor = loadFactor; this.threshold = tableSizeFor(initialCapacity); } /** * 自定义初始容量 使用默认加载因子（0.75）构造一个空的HashMap 。 */ public HashMap(int initialCapacity) { this(initialCapacity, DEFAULT_LOAD_FACTOR); } /** * 使用默认的初始容量（16）和默认的加载因子（0.75）构造一个空的HashMap 。 */ public HashMap() { this.loadFactor = DEFAULT_LOAD_FACTOR; // all other fields defaulted } 填装因子:loadFactor 表示填装因子的大小，简单的介绍一下填装因子：假设数组大小为16，每个放到数组中的元素mod 9，所有元素取模后放的位置是（0–9） 此时填装因子的大小为 9/16 ,装填因子就为0.75啦。\nHashMap初始化过程就是新建一个大小为capacity，类型为Node的数组，Node上面已经介绍过这个类，包含一个指针一个key，一个value，和一个hash。capacity是2的次幂，至于为什么是2的次幂后面会有介绍的。\nhash函数\nstatic final int hash(Object key) { int h; //用 key 的 hashCode 高16位 与 低16位 进行 异或运算 结果放在低16位 return (key == null) ? 0 : (h = key.hashCode()) ^ (h \u0026gt;\u0026gt;\u0026gt; 16); } 因为hashmap的 put 和 get 都要将 key 经过hash函数处理之后 与 数组大小 length -1 进行与运算，而数组的大小通常不会超过2^16，所以始终是低16位参与运算，所以将key的hashCode的高16位与低16位进行异或运算，得到的值会更具有散列的特性。\nput 函数\npublic V put(K key, V value) { return putVal(hash(key), key, value, false, true); } final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) { Node\u0026lt;K,V\u0026gt;[] tab; Node\u0026lt;K,V\u0026gt; p; int n, i; //table未初始化 或者 长度为 0 进行扩容 if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; // 确定存放的下标，如果此时为null，则新生Node 放入当前数组下标的位置。 if ((p = tab[i = (n - 1) \u0026amp; hash]) == null) tab[i] = newNode(hash, key, value, null); //数组当前位置已存在元素 else { Node\u0026lt;K,V\u0026gt; e; K k; //数组当前位置p 的hash 与要插入元素的hash相等 且 key 相等 if (p.hash == hash \u0026amp;\u0026amp; ((k = p.key) == key || (key != null \u0026amp;\u0026amp; key.equals(k)))) //当前位置p 赋值给 e e = p; // hash值不等，即key不等，p为红黑树节点 else if (p instanceof TreeNode) //插入红黑树 e = ((TreeNode\u0026lt;K,V\u0026gt;)p).putTreeVal(this, tab, hash, key, value); //为链表节点 else { //在链表节点最末端插入 for (int binCount = 0; ; ++binCount) { if ((e = p.next) == null) { //插入最末端 p.next = newNode(hash, key, value, null); //结点数量达到阈值(默认为 8 )，执行 treeifyBin 方法 // 这个方法会根据 HashMap 数组来决定是否转换为红黑树。 // 只有当数组长度大于或者等于 64 的情况下，才会执行转换红黑树操作，否则就是只是对数组扩容。 if (binCount \u0026gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; } //找到了key 值一样的节点 跳出循环 修改value if (e.hash == hash \u0026amp;\u0026amp; ((k = e.key) == key || (key != null \u0026amp;\u0026amp; key.equals(k)))) break; p = e; } } // 找到key值、hash值与插入元素相等的结点 if (e != null) { // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) //修改旧值 e.value = value; afterNodeAccess(e); //返回旧值 return oldValue; } } //结构性修改 ++modCount; // 实际大小大于阈值则扩容 if (++size \u0026gt; threshold) resize(); afterNodeInsertion(evict); return null; } 先判断数组 table 是否为null 或者 长度为0，如果是则 resize() 扩容，否则根据 hash 确定存放的下标，如果此时数组对应位置为null，则新生Node 放入当前数组下标的位置。 对应位置有值，判断key是否一致，一致则直接修改value值 ，否则进行遍历在末端插入。 判断此时链表长度是否大于默认阈值（8），不大于则直接返回旧值，否则将链表转换为红黑树。 get函数\npublic V get(Object key) { Node\u0026lt;K,V\u0026gt; e; return (e = getNode(hash(key), key)) == null ? null : e.value; } final Node\u0026lt;K,V\u0026gt; getNode(int hash, Object key) { Node\u0026lt;K,V\u0026gt;[] tab; Node\u0026lt;K,V\u0026gt; first, e; int n; K k; if ((tab = table) != null \u0026amp;\u0026amp; (n = tab.length) \u0026gt; 0 \u0026amp;\u0026amp; (first = tab[(n - 1) \u0026amp; hash]) != null) { // hash计算出的 数组位置 第一个元素存在 if (first.hash == hash \u0026amp;\u0026amp; // 数组当位置的第一个元素 hash 与要取的 hash相当 且 key相等 ((k = first.key) == key || (key != null \u0026amp;\u0026amp; key.equals(k)))) return first; if ((e = first.next) != null) { // 存在第二个元素 if (first instanceof TreeNode) // 第一个元素是一个树节点 调红黑树的取值方法 return ((TreeNode\u0026lt;K,V\u0026gt;)first).getTreeNode(hash, key); do {// 链表节点 循环遍历 找到节点相等的key 和 hash if (e.hash == hash \u0026amp;\u0026amp; ((k = e.key) == key || (key != null \u0026amp;\u0026amp; key.equals(k)))) return e; } while ((e = e.next) != null); } } // 没有 返回null return null; } 扩容 什么时候进行扩容操作？\n数组 table 为null 或者长度为 0 数组中元素实际个数大于阈值 threshold 会扩容 链表中的长度超过了TREEIFY_THRESHOLD（8），但表长度却小于MIN_TREEIFY_CAPACITY（64） final Node\u0026lt;K,V\u0026gt;[] resize() { Node\u0026lt;K,V\u0026gt;[] oldTab = table; int oldCap = (oldTab == null) ? 0 : oldTab.length; int oldThr = threshold; int newCap, newThr = 0; //旧容量大于 0 if (oldCap \u0026gt; 0) { //旧容量大于等于最大容量 阈值等于最大容量 并返回旧容量值 if (oldCap \u0026gt;= MAXIMUM_CAPACITY) { threshold = Integer.MAX_VALUE; return oldTab; } //新容量 为 旧容量的2倍 且 旧容量大于默认初始化容量且小于最大容量 else if ((newCap = oldCap \u0026lt;\u0026lt; 1) \u0026lt; MAXIMUM_CAPACITY \u0026amp;\u0026amp; oldCap \u0026gt;= DEFAULT_INITIAL_CAPACITY) newThr = oldThr \u0026lt;\u0026lt; 1; // 新阈值为 旧阈值的2倍 } else if (oldThr \u0026gt; 0) //旧容量小于等于0时 新容量 等于 旧阈值 newCap = oldThr; else { // 阈值和容量 都初始化为默认值 newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); } if (newThr == 0) { float ft = (float)newCap * loadFactor; newThr = (newCap \u0026lt; MAXIMUM_CAPACITY \u0026amp;\u0026amp; ft \u0026lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); } threshold = newThr; @SuppressWarnings({\u0026#34;rawtypes\u0026#34;,\u0026#34;unchecked\u0026#34;}) Node\u0026lt;K,V\u0026gt;[] newTab = (Node\u0026lt;K,V\u0026gt;[])new Node[newCap]; table = newTab; if (oldTab != null) { //遍历旧数组 oldTab for (int j = 0; j \u0026lt; oldCap; ++j) { Node\u0026lt;K,V\u0026gt; e; if ((e = oldTab[j]) != null) { //旧数组 j 位置置空 oldTab[j] = null; //只有一个值，根据hash放入新数组的对应位置 if (e.next == null) newTab[e.hash \u0026amp; (newCap - 1)] = e; //为红黑树 else if (e instanceof TreeNode) ((TreeNode\u0026lt;K,V\u0026gt;)e).split(this, newTab, j, oldCap); else { // 链表优化重hash的节点 Node\u0026lt;K,V\u0026gt; loHead = null, loTail = null; Node\u0026lt;K,V\u0026gt; hiHead = null, hiTail = null; Node\u0026lt;K,V\u0026gt; next; do { next = e.next; //原索引 if ((e.hash \u0026amp; oldCap) == 0) { if (loTail == null) loHead = e; else loTail.next = e; loTail = e; } //原索引 + oldCap else { if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; } } while ((e = next) != null); // 原索引放到新数组 j 上 if (loTail != null) { loTail.next = null; newTab[j] = loHead; } // 原索引+oldCap放到新数组 j+oldCap 上 if (hiTail != null) { hiTail.next = null; newTab[j + oldCap] = hiHead; } } } } } return newTab; } 小结 扩容是一个特别耗性能的操作，所以当使用HashMap的时候，估算map的大小，初始化的时候给一个大致的数值，避免map进行频繁的扩容。 负载因子是可以修改的，也可以大于1，但是不建议轻易修改。 HashMap不是线程安全的，不要在并发环境下同时操作HashMap，建设使用ConcurrentHashMap。 JDK1.8之前 HashMap是数组+链表的实现，JDK1.8采用数组+链表/红黑树的方式实现。 4.常见面试题 1.HashMap的底层数据结构 JDK1.8之前 数组+链表 JDK1.8之后\t数组+链表/红黑树\n2.HashMap的工作原理 HashMap底层是数组+链表，由Node内部类实现，通过put方法存放、get方法获取数据。\n存储数据时，将K，V键值传给put方法\n调用hash(K)方法，计算K的hash值，hash\u0026amp;(n-1)计算此时应放的数组下标。 判断当前数组位置是否有值，没有则直接插入，有值则判断他们的key是否相等，相等则修改它的值。 否则，判断当前节点是红黑树还是链表，若是链表则遍历，在末端插入元素，若存在key相等的节点，则修改它的值，若是红黑树则遍历红黑树并插入元素 判断插入链表后，链表的长度是否大于8，如果大于且数组长度大于64，则链表转为红黑树，数组长度小于64，则数组扩容。 判断插入之后，HashMap实际元素的个数是否大于 capacity * loadfactor，大于会进行扩容。 获取数据时，将K值传给get方法。\n调用 hash(K) 方法（计算 K 的 hash 值）从而获取该键值所在链表的数组下标 遍历链表或者红黑树，equals()方法查找相同 Node 链表中 K 值对应的 V 值。 3.HashMap 的底层数组长度为何总是2的n次方\n数据分布均匀，减少hash碰撞 当长度n总是2的n次方时 hash\u0026amp;（n-1）相当于 hash%n-1，即相当于取模运算，而且在速度、效率上比直接取模要快得多 4.HashMap允许空键空值么 HashMap允许Key有一个为null，允许多个Value为null\n5.HashMap线程安全方面会出现什么问题 JDK1.7 扩容会出现循环链或数据丢失 JDK1.8 put会出现数据覆盖\n参考链接\n美团技术团队 Java 8系列之重新认识HashMap JavaGuide HashMap(JDK1.8)源码+底层数据结构分析 HashMap 常见面试题 ","date":1614643200,"description":"","dir":"post\\Java Base\\","excerpt_html":null,"excerpt_text":null,"expirydate":-62135596800,"fuzzywordcount":4100,"html":null,"keywords":null,"kind":"page","lang":"en","lastmod":1614643200,"objectID":"6e8b1879f6e562455e4f77abe0e71a1f","permalink":"http://localhost:1313/post/java-base/hashmap/","publishdate":"2021-03-02T00:00:00Z","readingtime":9,"relpermalink":"/post/java-base/hashmap/","section":"post","summary":"本篇分析HashMap的 hash()函数 和 底层数据结构 以及 常用方法 和 常见面试相关题目 1. HashMap简介 HashMap 是一个K，V键值对的常用集合类，它","tags":["Java集合"],"title":"深入了解HashMap","type":"post","url":"/post/java-base/hashmap/","weight":0,"wordcount":4069},{"author":null,"categories":["Code"],"content":" 这篇文章对Java集合相关类进行介绍，包括Collection、List、Set、Map、Queue这些常见得集合相关接口和类。\n1.集合概述 常用的集合有List、Set、Map、Queue等，他们之间的关系如下图。List、Queue、Set继承Collection接口。Iterable接口是迭代器，这里不进行过多介绍。Map接口是一个单独的接口，这里不进行介绍。 List、Set、Queue、Map的区别\nList存储可以重复、有序的元素 Set存储不可以重复、无序的元素 Queue存储有序的元素且先进先出，是一个队列 Map是键值对存储结构，Key 是无序的、不可重复的，value 是无序的、可重复的，每个键最多映射到一个值，key和value都可以为null 2.List List：元素有序，元素可重复，添加的元素放在最后（按照插入顺序保存元素）\n常用子类有：\nArrayList LinkedList Vector 2.1 ArrayList ArrayList继承AbstractList抽象类，实现List、Serializable、Cloneable、RandomAccess接口。 使用 Object[] 数组存储元素 因此查询快，增删操作慢，没有实现线程同步。\n常用方法：\nadd(E e) 向数组末尾添加一个元素 clear() 清除所有元素，数组里的元素为null，size置为0 contains(Object o) 是否包含某个元素 get(int index) 获取第i个元素 remove(int index) 删除第i个元素 remove(Object o) 删除某个元素 size() 返回存储了多少个元素 2.2 LinkedList LinkedList继承AbstractSequentialList抽象类，实现List、Serializable、Cloneable、Deque接口。 使用双向链表存储元素（内有Node私有静态内部类） 因此查询慢，增删操作快，没有实现线程同步。 因为实现了Deque接口，所以除了拥有列表相关的常用方法外，还有队列相关的方法。\n实现List接口的方法（列表相关操作）常用方法和ArrayList类似 实现Deque接口的方法（队列相关操作） addFirst(E e) offerFirst(E e) 插入链表首位 addLast(E e) offerLast(E e) 插入链表末尾 getFirst() element() 获取链表首位元素 getLast() 获取链表末尾元素 offer(E e) 链表末尾添加元素 peek() peekFirst() 获取链表首位元素 但不删除 poll() pollFirst() 获取链表首位元素 且删除 2.3 Vector 使用 Object[] 数组存储元素 因此查询快，增删操作慢，实现线程同步。 Vector非常类似ArrayList，但是Vector是同步的。由Vector创建的Iterator，虽然和 ArrayList创建的Iterator是同一接口，但是，因为Vector是同步的，当一个Iterator被创建而且正在被使用，另一个线程改变了 Vector的状态（例如，添加或删除了一些元素），这时调用Iterator的方法时将抛出 ConcurrentModificationException。\n3.Set Set：元素无序，元素不可重复。\n无序性不等于随机性 ，无序性是指存储的数据在底层数组中并非按照数组索引的顺序添加 ，而是根据数据的哈希值决定的。 不可重复性是指添加的元素按照 equals()判断时 ，返回 false，需要同时重写 equals()方法和 HashCode()方法。 常用子类：\nHashSet LinkedHashSet TreeSet 3.1 HashSet 使用HashMap来保存所有元素，线程不安全的，集合元素可以是null,但只能放入一个null，不能保证迭代的顺序与插入的顺序一致。\n当使用add方法添加元素时，底层是使用HashMap的put方法，会计算key的hash值，以hashcode值不同来去除重复的值，值得注意的是当添加对象作为键时，应该重写对象类的hashCode()和equals()方法 如果自定义的类中没有重写equals()，那么比较的还是地址，返回值不同，则判断为两个对象不相等，都被添加到了集合中，所以也要重写equals()。 所以自定义对象添加到Set集合类中一定要重写hashCode()与equals()，缺一不可 。\n3.2 LinkedHashSet 使用LinkedHashMap来保存所有元素，线程不安全的，集合元素不能重复，迭代输出的顺序与插入的顺序保持一致。 继承HashSet\n3.3 TreeSet TreeSet 是 SortedSet 接口的实现类，TreeSet 可以确保集合元素处于排序状态。TreeSet底层使用红黑树结构存储数据(使用TreeMap存储元素)，默认情况下，TreeSet 采用自然排序。 向TreeSet中添加的数据，要求是相同类的对象，需要实现使用至少一种排序方式，Comparable（自然排序）和Comparator（定制排序）。比较两个对象是否相同的标准是重写的方法是否返回0，不再equals()。\n4.Queue 队列通常但不一定以FIFO（先进先出）的方式对元素进行排序。 例外情况包括优先级队列（根据提供的比较器对元素进行排序或元素的自然排序）和LIFO队列（或堆栈），对LIFO进行排序（后进先出）。\n很多常用queue子类都与线程有关，LinkedList已经介绍过了，这里只介绍ArrayDeque。\nDeque Deque扩展了Queue，有队列的所有方法，还可以看做栈，有栈的基本方法push/pop/peek，还有明确的操作两端的方法如addFirst/removeLast等。\n该接口定义了访问双端队列两端的元素的方法。 提供了用于插入，删除和检查元素的方法。 这些方法中的每一种都以两种形式存在：一种在操作失败时引发异常，另一种返回一个特殊值（取决于操作，为null或false ）。 插入操作的后一种形式是专门为容量受限的Deque实现而设计的。 在大多数实现中，插入操作不会失败。\n4.1 ArrayDeque ArrayDeque实现了Deque接口，同LinkedList一样，它的队列长度也是没有限制的，底层使用 Object[] 数组存储元素 ArrayDeque 是一个可扩容的数组，LinkedList 是链表结构； ArrayDeque 里不可以存 null 值，但是 LinkedList 可以； ArrayDeque 在操作头尾端的增删操作时更高效，但是 LinkedList 只有在当要移除中间某个元素且已经找到了这个元素后的移除才是 O(1) 的；因为ArrayDeque底层使用的是一个逻辑循环数组 ArrayDeque 在内存使用方面更高效。 推荐使用ArrayDeque\n5.Map接口 Map接口与List、Set接口不同，它并未继承Collection接口，是由一系列键值对组成的接口，是一个独立的接口。常用的实现Map的类有HashMap、ConcurrentHashMap、HashTable、TreeMap。\nMap接口有一个内部接口Entry，它是存储元素键值对的条目。\n5.1HashMap和HashTable的区别 线程安全：HashMap不是线程安全的，HashTable是线程安全的，它通过使用synchonrized修饰方法保证线程安全 效率：HashMap比HashTable效率高，如果为了保证线程安全推荐使用ConcurrentHashMap，不推荐使用HashTable 对 Null key 和 Null value 的支持： HashMap 可以存储 null 的 key 和 value，但 null 作为键只能有一个，null 作为值可以有多个；HashTable 不允许有 null 键和 null 值，否则会抛出 NullPointerException。 初始容量大小和每次扩充容量大小的不同 ： ① 创建时如果不指定容量初始值，Hashtable 默认的初始大小为 11，之后每次扩充，容量变为原来的 2n+1。HashMap 默认的初始化大小为 16。之后每次扩充，容量变为原来的 2 倍。② 创建时如果给定了容量初始值，那么 Hashtable 会直接使用你给定的大小，而 HashMap 会将其扩充为 2 的幂次方大小（HashMap 中的tableSizeFor()方法保证，下面给出了源代码）。也就是说 HashMap 总是使用 2 的幂作为哈希表的大小,后面会介绍到为什么是 2 的幂次方。 底层数据结构： JDK1.8 以后的 HashMap 在解决哈希冲突时有了较大的变化，当链表长度大于阈值（默认为 8）（将链表转换成红黑树前会判断，如果当前数组的长度小于 64，那么会选择先进行数组扩容，而不是转换为红黑树）时，将链表转化为红黑树，以减少搜索时间。Hashtable 没有这样的机制。 5.2HashMap的底层实现 jdk1.8以前，HashMap底层使用数组加链表的方式实现，也称链表散列。HashMap通过key的hashCode经过hash()函数处理后得到hash值，然后通过（n - 1）\u0026amp; hash 判断当前元素存放的位置（这里的 n 指的是数组的长度），如果当前位置存在元素的话，就判断该元素与要存入的元素的 hash 值以及 key 是否相同，如果相同的话，直接覆盖，不相同就通过拉链法解决冲突。\nhash函数详解\n为了能让 HashMap 存取高效，尽量较少碰撞，也就是要尽量把数据分配均匀。我们上面也讲到了过了，Hash 值的范围值-2147483648 到 2147483647，前后加起来大概 40 亿的映射空间，只要哈希函数映射得比较均匀松散，一般应用是很难出现碰撞的。但问题是一个 40 亿长度的数组，内存是放不下的。所以这个散列值是不能直接拿来用的。用之前还要先做对数组的长度取模运算，得到的余数才能用来要存放的位置也就是对应的数组下标。这个数组下标的计算方法是“ (n - 1) \u0026amp; hash”。（n 代表数组长度）。这也就解释了 HashMap 的长度为什么是 2 的幂次方。\n这个算法应该如何设计呢？\n我们首先可能会想到采用%取余的操作来实现。但是，重点来了：“取余(%)操作中如果除数是 2 的幂次则等价于与其除数减一的与(\u0026amp;)操作（也就是说 hash%length==hash\u0026amp;(length-1)的前提是 length 是 2 的 n 次方；）。” 并且 采用二进制位操作 \u0026amp;，相对于%能够提高运算效率，这就解释了 HashMap 的长度为什么是 2 的幂次方。\n5.3ConcurrentHashMap 和 Hashtable 的区别 底层数据结构：ConcurrentHashMap在jdk1.7之前使用 分段数组+链表 实现，jdk1.8采用同HashMap一样的 数组 + 链表/红黑树。Hashtable 和 JDK1.8 之前的 HashMap 的底层数据结构类似都是采用 数组+链表 的形式。 实现线程安全的方式：ConcurrentHashMap在jdk1.7之前使用分段数组segment 加锁实现，每一把锁只锁容器其中一部分数据，多线程访问容器里不同数据段的数据，就不会存在锁竞争，提高并发访问率。jdk1.8 采用了粒度更小的方式，直接对Node采用 volatile 关键字修饰，方法则用synchronized和CAS进行并发控制。 ","date":1614038400,"description":"","dir":"post\\Java Base\\","excerpt_html":null,"excerpt_text":null,"expirydate":-62135596800,"fuzzywordcount":4100,"html":null,"keywords":null,"kind":"page","lang":"en","lastmod":1614038400,"objectID":"ee77e75a82953ee241559658f1bbc1d7","permalink":"http://localhost:1313/post/java-base/java%E9%9B%86%E5%90%88%E5%B0%8F%E7%BB%93/","publishdate":"2021-02-23T00:00:00Z","readingtime":9,"relpermalink":"/post/java-base/java%E9%9B%86%E5%90%88%E5%B0%8F%E7%BB%93/","section":"post","summary":"这篇文章对Java集合相关类进行介绍，包括Collection、List、Set、Map、Queue这些常见得集合相关接口和类。 1.集合概述","tags":["Java集合"],"title":"Java集合小结","type":"post","url":"/post/java-base/java%E9%9B%86%E5%90%88%E5%B0%8F%E7%BB%93/","weight":0,"wordcount":4088},{"author":null,"categories":["Code"],"content":" \u0026ldquo;初步了解和使用Aviator\u0026rdquo;\n1.Aviator简介 Aviator 是一个高性能，轻量级的java语言实现的表达式求值引擎，主要用于各种表达式的动态求值。\n官方文档\ngithub地址\n支持数字、字符串、正则表达式、布尔值、正则表达式等基本类型，完整支持所有 Java 运算符及优先级等。 函数是一等公民，支持闭包和函数式编程。 内置 bigint/decmal 类型用于大整数和高精度运算，支持运算符重载得以让这些类型使用普通的算术运算符 +-*/ 参与运算。 完整的脚本语法支持，包括多行数据、条件语句、循环语句、词法作用域和异常处理等。 函数式编程结合 Sequence 抽象，便捷处理任何集合。 轻量化的模块系统。 多种方式，方便地调用 Java 方法，完整支持 Java 脚本 API（方便从 Java 调用脚本）。 丰富的定制选项，可作为安全的语言沙箱和全功能语言使用。 轻量化，高性能，通过直接将脚本翻译成 JVM 字节码，AviatorScript 的基础性能较好。\n使用场景包括：\n规则判断及规则引擎 公式计算 动态脚本控制 集合数据 ELT 等 …… 2.Aviator入门常用 这里使用版本为4.2.5\n2.1数据类型 Number类型: 数字类型,支持四种类型,分别是long,double,java.math.BigInteger(简称 big int)和java.math.BigDecimal(简 称 decimal),规则如下: 任何以大写字母 N 结尾的整数都被认为是 big int 任何以大写字母 M 结尾的数字都被认为是 decimal 其他的任何整数都将被转换为 Long 其他任何浮点数都将被转换为 Double 超过 long 范围的整数字面量都将自动转换为 big int 类型 String类型: 字符串类型,单引号或者双引号括起来的文本串,如\u0026rsquo;hello world\u0026rsquo;, 变量如果传入的是String或者Character也将转为String类型 Bool类型: 常量true和false,表示真值和假值,与 java 的Boolean.TRUE和Boolean.False对应 Pattern类型: 正则表达式, 以//括起来的字符串,如/\\d+/,内部 实现为java.util.Pattern 变量类型: 与 Java 的变量命名规则相同,变量的值由用户传入 nil类型: 常量nil,类似 java 中的null,但是nil比较特殊,nil不仅可以参与==、!=的比较, 也可以参与\u0026gt;、\u0026gt;=、\u0026lt;、\u0026lt;=的比较,Aviator 规定任何类型都大于nil除了nil本身,nil==nil返回true。 用户传入的变量值如果为null,那么也将作为nil处理,nil打印为null 2.2操作符 算术运算符 Aviator 支持常见的算术运算符,包括+ - * / %五个二元运算符,和一元运算符-(负)。其中- * / %和一元的-仅能作用于Number类型。 +不仅能用于Number类型,还可以用于String的相加,或者字符串与其他对象的相加。 Aviator 规定,任何类型与String相加,结果为String。\n逻辑运算符 Avaitor 的支持的逻辑运算符包括,一元否定运算符!,以及逻辑与的\u0026amp;\u0026amp;,逻辑或的||。逻辑运算符的操作数只能为Boolean。 \u0026amp;\u0026amp;和||都执行短路规则。\n关系运算符 Aviator 支持的关系运算符包括\u0026lt;, \u0026lt;=, \u0026gt;, \u0026gt;=以及==和!= 。 关系运算符可以作用于Number之间、String之间、Pattern之间、Boolean之间、变量之间以及其他类型与nil之间的关系比较, 不同类型除了nil之外不能相互比较。\n3.一些简单特性 package aviator; import com.googlecode.aviator.AviatorEvaluator; import com.googlecode.aviator.Expression; import com.googlecode.aviator.runtime.function.AbstractFunction; import com.googlecode.aviator.runtime.function.AbstractVariadicFunction; import com.googlecode.aviator.runtime.function.FunctionUtils; import com.googlecode.aviator.runtime.type.AviatorDouble; import com.googlecode.aviator.runtime.type.AviatorObject; import com.googlecode.aviator.runtime.type.AviatorString; import java.util.HashMap; import java.util.Map; /** * @author zousy * @version v1.0 * @Description * @date 2021-01-16 15:39 */ public class App { public static void main(String[] args) { //执行表达式 Aviator的数值类型仅支持Long和Double, 任何整数都将转换成Long, // 任何浮点数都将转换为Double, 包括用户传入的变量数值 Object execute = AviatorEvaluator.execute(\u0026#34;1+1+2\u0026#34;); System.out.println(execute.toString() + execute.getClass().getSimpleName()); Object execute1 = AviatorEvaluator.execute(\u0026#34;2.3+3.0\u0026#34;); System.out.println(execute1.toString() + execute1.getClass().getSimpleName()); //内置函数和 多行表达式 使用; 隔开 Expression compile = AviatorEvaluator.getInstance().compile(\u0026#34;println(\u0026#39;Hello, AviatorScript!\u0026#39;); 1+2\u0026#34;); Object res = compile.execute(); System.out.println(res.toString()); //使用变量 Map\u0026lt;String, Object\u0026gt; env = new HashMap\u0026lt;String, Object\u0026gt;(); env.put(\u0026#34;name\u0026#34;,\u0026#34;zsad\u0026#34;); String execute2 = (String) AviatorEvaluator.execute(\u0026#34; \u0026#39;hello\u0026#39; + name\u0026#34;, env); System.out.println(execute2); //支持函数调用 Object execute3 = AviatorEvaluator.execute(\u0026#34;string.length(\u0026#39;hello\u0026#39;)\u0026#34;); System.out.println(execute3.toString()); //支持通过 lambda 关键字定义一个匿名函数，并且支持闭包捕获 //lambda (参数1,参数2...) -\u0026gt; 参数体表达式 end env.put(\u0026#34;x\u0026#34;,1); env.put(\u0026#34;y\u0026#34;,10); Object execute4 = AviatorEvaluator.execute(\u0026#34;(lambda (x,y) -\u0026gt; x + y end)(x,y)\u0026#34;, env); System.out.println(execute4.toString()); //支持自定义函数 AviatorEvaluator.addFunction(new AddFunction()); AviatorEvaluator.addFunction(new GetFirstNonNullFunction()); env.put(\u0026#34;a\u0026#34;,12); env.put(\u0026#34;v\u0026#34;,13); //参数确定 自定义函数 System.out.println(AviatorEvaluator.execute(\u0026#34;add(1,33)\u0026#34;).toString()); //参数不确定自定义函数 System.out.println(AviatorEvaluator.execute(\u0026#34;getFirstNonNull(a,v,bas,asf)\u0026#34;,env)); //lambda表达式 自定义函数 AviatorEvaluator.defineFunction(\u0026#34;sub\u0026#34;,\u0026#34;lambda (a,b) -\u0026gt; a - b end\u0026#34;); System.out.println(AviatorEvaluator.execute(\u0026#34;sub(12,13)\u0026#34;).toString()); //lambda表达式支持闭包 自定义函数 ? 按照官方写 会报错 存疑 AviatorEvaluator.defineFunction(\u0026#34;sub2\u0026#34;,\u0026#34;lambda (a) -\u0026gt; lambda (b) -\u0026gt; a - b end end\u0026#34;); System.out.println(AviatorEvaluator.execute(\u0026#34;sub2(12)(14)\u0026#34;).toString()); } //参数确定 继承 AbstractFunction static class AddFunction extends AbstractFunction { @Override public AviatorObject call(Map\u0026lt;String, Object\u0026gt; env, AviatorObject arg1, AviatorObject arg2) { Number left = FunctionUtils.getNumberValue(arg1, env); Number right = FunctionUtils.getNumberValue(arg2, env); return new AviatorDouble(left.doubleValue() + right.doubleValue()); } //函数名 add @Override public String getName() { return \u0026#34;add\u0026#34;; } } //参数不确定 继承 AbstractVariadicFunction static class GetFirstNonNullFunction extends AbstractVariadicFunction { @Override public AviatorObject variadicCall(Map\u0026lt;String, Object\u0026gt; env, AviatorObject... args) { if (args != null) { for (AviatorObject arg : args) { if (arg.getValue(env) != null) { return arg; } } } return new AviatorString(null); } @Override public String getName() { return \u0026#34;getFirstNonNull\u0026#34;; } } } 运行结果 4.官方函数例子(官方代码) package aviator.example; import com.googlecode.aviator.AviatorEvaluator; import java.util.ArrayList; import java.util.Arrays; import java.util.HashMap; import java.util.Map; /** * @author zousy * @version v1.0 * @Description * @date 2021-01-18 15:59 */ public class FunctionExample { public static void main(String[] args) { System.out.println(AviatorEvaluator.execute(\u0026#34;sysdate()\u0026#34;)); System.out.println(AviatorEvaluator.execute(\u0026#34;rand()\u0026#34;)); System.out.println(AviatorEvaluator.execute(\u0026#34;now()\u0026#34;)); System.out.println(AviatorEvaluator.execute(\u0026#34;date_to_string(sysdate(),\u0026#39;yyyy-MM-dd\u0026#39;)\u0026#34;)); System.out.println(AviatorEvaluator .execute(\u0026#34;string_to_date(date_to_string(sysdate(),\u0026#39;yyyy-MM-dd\u0026#39;),\u0026#39;yyyy-MM-dd\u0026#39;)\u0026#34;)); // string function System.out.println(\u0026#34;test string function...\u0026#34;); System.out.println(AviatorEvaluator.execute(\u0026#34;string.length(\u0026#39;hello\u0026#39;)\u0026#34;)); System.out.println(AviatorEvaluator.execute(\u0026#34;string.contains(\u0026#39;hello\u0026#39;,\u0026#39;h\u0026#39;)\u0026#34;)); System.out.println(AviatorEvaluator.execute(\u0026#34;string.startsWith(\u0026#39;hello\u0026#39;,\u0026#39;h\u0026#39;)\u0026#34;)); System.out.println(AviatorEvaluator.execute(\u0026#34;string.endsWith(\u0026#39;hello\u0026#39;,\u0026#39;llo\u0026#39;)\u0026#34;)); System.out.println( AviatorEvaluator.execute(\u0026#34;string.contains(\\\u0026#34;test\\\u0026#34;,string.substring(\u0026#39;hello\u0026#39;,1,2))\u0026#34;)); System.out.println(Arrays .toString((String[]) AviatorEvaluator.execute(\u0026#34;string.split(\u0026#39;hello world,aviator\u0026#39;,\u0026#39; \u0026#39;)\u0026#34;))); // math function System.out.println(\u0026#34;test math function...\u0026#34;); System.out.println(AviatorEvaluator.execute(\u0026#34;math.abs(-3)\u0026#34;)); System.out.println(AviatorEvaluator.execute(\u0026#34;math.pow(-3,2)\u0026#34;)); System.out.println(AviatorEvaluator.execute(\u0026#34;math.sqrt(14.0)\u0026#34;)); System.out.println(AviatorEvaluator.execute(\u0026#34;math.log(100)\u0026#34;)); System.out.println(AviatorEvaluator.execute(\u0026#34;math.log10(1000)\u0026#34;)); System.out.println(AviatorEvaluator.execute(\u0026#34;math.sin(20)\u0026#34;)); System.out.println(AviatorEvaluator.execute(\u0026#34;math.cos(99.23)\u0026#34;)); System.out.println(AviatorEvaluator.execute(\u0026#34;math.tan(19.9)\u0026#34;)); // seq lib Map\u0026lt;String, Object\u0026gt; env = new HashMap\u0026lt;String, Object\u0026gt;(); ArrayList\u0026lt;Integer\u0026gt; list = new ArrayList\u0026lt;Integer\u0026gt;(); list.add(3); list.add(100); list.add(-100); env.put(\u0026#34;list\u0026#34;, list); System.out.println(AviatorEvaluator.execute(\u0026#34;reduce(list,+,0)\u0026#34;, env)); System.out.println(AviatorEvaluator.execute(\u0026#34;filter(list,seq.exists())\u0026#34;, env)); System.out.println(AviatorEvaluator.execute(\u0026#34;count(list)\u0026#34;, env)); System.out.println(AviatorEvaluator.execute(\u0026#34;include(list,100)\u0026#34;, env)); System.out.println(AviatorEvaluator.execute(\u0026#34;sort(list)\u0026#34;, env)); System.out.println(AviatorEvaluator.execute(\u0026#34;map(list,println)\u0026#34;, env)); System.out.println(list); } } 运行结果\n5.支持大数据操作 package aviator.example; import com.googlecode.aviator.AviatorEvaluator; import com.googlecode.aviator.Options; import java.math.BigDecimal; import java.math.BigInteger; import java.math.MathContext; import java.util.HashMap; import java.util.Map; /** * @author zousy * @version v1.0 * @Description * @date 2021-01-18 16:10 */ public class BigNumberExample { //以大写字母N为后缀的整数都被认为是big int,如1N,2N,9999999999999999999999N等, 都是big int类型。 //超过long范围的整数字面量都将自动转换为big int类型。 //以大写字母M为后缀的数字都被认为是decimal, 如1M,2.222M, 100000.9999M等, 都是decimal类型。 //big int和decimal的运算,跟其他数字类型long,double没有什么区别,操作符仍然是一样的。 // aviator重载了基本算术操作符来支持这两种新类型: public static void main(String[] args) { Object rt = AviatorEvaluator.execute(\u0026#34;9223372036854775807100.356M * 2\u0026#34;); System.out.println(rt + \u0026#34; \u0026#34; + rt.getClass()); rt = AviatorEvaluator.execute(\u0026#34;92233720368547758074+1000\u0026#34;); System.out.println(rt + \u0026#34; \u0026#34; + rt.getClass()); BigInteger a = new BigInteger(String.valueOf(Long.MAX_VALUE) + String.valueOf(Long.MAX_VALUE)); BigDecimal b = new BigDecimal(\u0026#34;3.2\u0026#34;); BigDecimal c = new BigDecimal(\u0026#34;9999.99999\u0026#34;); Map\u0026lt;String, Object\u0026gt; env = new HashMap\u0026lt;String, Object\u0026gt;(); env.put(\u0026#34;a\u0026#34;, a); env.put(\u0026#34;b\u0026#34;, b); env.put(\u0026#34;c\u0026#34;, c); rt = AviatorEvaluator.execute(\u0026#34;a+10000000000000000000\u0026#34;, env); System.out.println(rt + \u0026#34; \u0026#34; + rt.getClass()); rt = AviatorEvaluator.execute(\u0026#34;b+c*2\u0026#34;, env); System.out.println(rt + \u0026#34; \u0026#34; + rt.getClass()); rt = AviatorEvaluator.execute(\u0026#34;a*b/c\u0026#34;, env); System.out.println(rt + \u0026#34; \u0026#34; + rt.getClass()); // set math context AviatorEvaluator.setOption(Options.MATH_CONTEXT, MathContext.DECIMAL64); rt = AviatorEvaluator.execute(\u0026#34;a*b/c\u0026#34;, env); System.out.println(rt + \u0026#34; \u0026#34; + rt.getClass()); } } 运行结果 6.正则表达式匹配 package aviator.example; import com.googlecode.aviator.AviatorEvaluator; import java.util.HashMap; import java.util.Map; /** * @author zousy * @version v1.0 * @Description * @date 2021-01-18 16:40 */ public class RegularExpressionExample { //email与正则表达式/([\\\\w0-8]+@\\\\w+[\\\\.\\\\w+]+)/通过=~操作符来匹配,结果为一个 Boolean 类 型, // 因此可以用于三元表达式判断,匹配成功的时候返回$1,指代正则表达式的分组 1,也就是用户名,否则返回unknown。 //Aviator 在表达式级别支持正则表达式,通过//括起来的字符序列构成一个正则表达式, // 正则表达式可以用于匹配(作为=~的右操作数)、比较大小。但是匹配仅能与字符串进行匹配。 // 匹配成功后, Aviator 会自动将匹配成功的捕获分组(capturing groups) 放入 env ${num}的变量中, // 其中$0 指代整个匹配的字符串,而$1表示第一个分组，$2表示第二个分组以此类推。 //请注意，分组捕获放入 env 是默认开启的，因此如果传入的 env 不是线程安全并且被并发使用，可能存在线程安全的隐患。 // 关闭分组匹配，可以通过 AviatorEvaluator.setOption(Options.PUT_CAPTURING_GROUPS_INTO_ENV, false);来关闭，对性能有稍许好处。 //Aviator 的正则表达式规则跟 Java 完全一样,因为内部其实就是使用java.util.regex.Pattern做编译的。 public static void main(final String[] args) { String email = \u0026#34;killme2008@gmail.com\u0026#34;; Map\u0026lt;String, Object\u0026gt; env = new HashMap\u0026lt;\u0026gt;(); env.put(\u0026#34;email\u0026#34;, email); String username = (String) AviatorEvaluator.execute(\u0026#34;email=~/([\\\\w0-8]+)@\\\\w+[\\\\.\\\\w+]+/ ? $1:\u0026#39;unknow\u0026#39;\u0026#34;, env); System.out.println(username); } } 运行结果 ","date":1610928000,"description":"","dir":"post\\Aviator\\","excerpt_html":null,"excerpt_text":null,"expirydate":-62135596800,"fuzzywordcount":3000,"html":null,"keywords":null,"kind":"page","lang":"en","lastmod":1610928000,"objectID":"79564a5b84bfb1f949aa9907acfbf66f","permalink":"http://localhost:1313/post/aviator/aviator%E4%BD%BF%E7%94%A8/","publishdate":"2021-01-18T00:00:00Z","readingtime":6,"relpermalink":"/post/aviator/aviator%E4%BD%BF%E7%94%A8/","section":"post","summary":"\u0026ldquo;初步了解和使用Aviator\u0026rdquo; 1.Aviator简介 Aviator 是一个高性能，轻量级的java语言实现的表达式求值引擎，主要","tags":null,"title":"Aviator的初步了解和使用","type":"post","url":"/post/aviator/aviator%E4%BD%BF%E7%94%A8/","weight":0,"wordcount":2903},{"author":null,"categories":["Code"],"content":" \u0026ldquo;初步了解和使用SPRING AOP\u0026rdquo;\n一、JDK 动态代理的使用 1.Food 目标接口 package proxy; /** * @author zsy * @version v1.0 * @Description * @date 2020-09-23 15:50 */ public interface Food { /** * @Author Zousy * @Description 测试静态代理 * @Date 15:52 2020/9/23 * @Param [] * @return void */ void priName(); } 2.DynamicProxy 动态代理类 要实现 InvocationHandler接口 package proxy; import java.lang.reflect.InvocationHandler; import java.lang.reflect.Method; /** * @author zsy * @version v1.0 * @Description * @date 2020-09-23 15:57 */ public class DynamicProxy implements InvocationHandler { private Object target; public DynamicProxy(Object target){ this.target = target; } @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { System.out.println(\u0026#34;动态代理前------------\u0026#34;); Object result = method.invoke(target,args); System.out.println(\u0026#34;动态代理后------------\u0026#34;); return result; } } 3.Test 测试类 package proxy; import java.lang.reflect.InvocationHandler; import java.lang.reflect.Proxy; /** * @author zousy * @version v1.0 * @Description * @date 2021-01-09 16:32 */ public class App { public static void main(String[] args) { Food food = new Food() { @Override public void priName() { System.out.println(\u0026#34;--------执行记录---------\u0026#34;); } }; InvocationHandler invocationHandler = new DynamicProxy(food); Food proxy = (Food) Proxy.newProxyInstance(food.getClass().getClassLoader(), food.getClass().getInterfaces(),invocationHandler); proxy.priName(); } } 4.输出 二、CGLIB的使用 1.Food 目标类 package com.proxy; /** * @author zousy * @version v1.0 * @Description * @date 2021-01-09 17:14 */ public class Food { public void priName(){ System.out.println(\u0026#34;----------执行-----------\u0026#34;); } } 1.App 测试类 package com.proxy; import org.springframework.cglib.proxy.Enhancer; import org.springframework.cglib.proxy.MethodInterceptor; import org.springframework.cglib.proxy.MethodProxy; import java.lang.reflect.Method; /** * @author zousy * @version v1.0 * @Description * @date 2021-01-09 17:15 */ public class App { public static void main(String[] args) { Enhancer enhancer = new Enhancer(); Food food = new Food(); enhancer.setSuperclass(food.getClass()); enhancer.setCallback(new MethodInterceptor() { @Override public Object intercept(Object o, Method method, Object[] objects, MethodProxy methodProxy) throws Throwable { System.out.println(\u0026#34;记录执行日志\u0026#34;); return methodProxy.invokeSuper(o,objects); } }); Food proxy = (Food) enhancer.create(); proxy.priName(); } } 3. 输出 3. Spring Aop的使用 1.LogAop 切面类 package com.page.aop; import lombok.extern.slf4j.Slf4j; import org.aspectj.lang.ProceedingJoinPoint; import org.aspectj.lang.annotation.*; import org.springframework.stereotype.Component; /** * @author zousy * @version v1.0 * @Description 切面必须配置到IOC容器中 * @date 2021-01-11 14:06 */ @Aspect @Slf4j @Component public class LogAop { //切入点 需要写 切点表达式 @Pointcut(\u0026#34;execution (* com.page.service.impl.UserServiceImpl.query(..))\u0026#34;) public void pointcut(){ } @Before(\u0026#34;pointcut()\u0026#34;) public void before(){ log.info(\u0026#34;---------执行之前----------\u0026#34;); } @After(\u0026#34;pointcut()\u0026#34;) public void after(){ log.info(\u0026#34;---------执行之后----------\u0026#34;); } @Around(\u0026#34;pointcut()\u0026#34;) public void around(ProceedingJoinPoint point) throws Throwable { log.info(\u0026#34;---------环绕执行之前----------\u0026#34;); point.proceed(); System.out.println(point.getClass().getName()); System.out.println(point.getTarget().getClass().getSimpleName()); log.info(\u0026#34;---------环绕执行之后----------\u0026#34;); } @AfterReturning(\u0026#34;pointcut()\u0026#34;) public void afterReturning(){ log.info(\u0026#34;---------------方法返回之后执行--------------\u0026#34;); } } 2.UserServiceImpl 测试类 package com.page.service.impl; import com.page.service.UserService; import org.springframework.stereotype.Service; /** * @author zousy * @version v1.0 * @Description * @date 2021-01-09 18:02 */ @Service public class UserServiceImpl implements UserService { public void query(){ System.out.println(\u0026#34;查询用户值\u0026#34;); } } 3.主类 package com.page; import com.page.service.impl.UserServiceImpl; import org.mybatis.spring.annotation.MapperScan; import org.springframework.boot.SpringApplication; import org.springframework.boot.autoconfigure.SpringBootApplication; import org.springframework.context.ConfigurableApplicationContext; import org.springframework.context.annotation.EnableAspectJAutoProxy; import org.springframework.stereotype.Repository; /** * @author zousy * @version v1.0 * @Description * @date 2020-12-09 17:27 */ @SpringBootApplication @MapperScan( basePackages = \u0026#34;com.zsy.page.*\u0026#34;, annotationClass = Repository.class) //开启Spring AOP功能 @EnableAspectJAutoProxy public class AopPageApplication { public static void main(String[] args) { //SpringApplication.run(AopPageApplication.class,args); ConfigurableApplicationContext context = SpringApplication.run(AopPageApplication.class, args); UserServiceImpl bean = context.getBean(UserServiceImpl.class); bean.query(); } } 4.结果 ","date":1610323200,"description":"","dir":"post\\Spring\\Spring AOP\\","excerpt_html":null,"excerpt_text":null,"expirydate":-62135596800,"fuzzywordcount":1100,"html":null,"keywords":null,"kind":"page","lang":"en","lastmod":1610323200,"objectID":"1e1c5ea23e2bb04fedb82fd952542825","permalink":"http://localhost:1313/post/spring/spring-aop/spring-aop%E4%BD%BF%E7%94%A8/","publishdate":"2021-01-11T00:00:00Z","readingtime":3,"relpermalink":"/post/spring/spring-aop/spring-aop%E4%BD%BF%E7%94%A8/","section":"post","summary":"\u0026ldquo;初步了解和使用SPRING AOP\u0026rdquo; 一、JDK 动态代理的使用 1.Food 目标接口 package proxy; /** * @author zsy * @version v1.0 * @Description * @date 2020-09-23 15:50 */ public interface Food { /** * @Author Zousy * @Description 测试静态代理","tags":["Spring"],"title":"Spring AOP的初步使用","type":"post","url":"/post/spring/spring-aop/spring-aop%E4%BD%BF%E7%94%A8/","weight":0,"wordcount":1091},{"author":null,"categories":["Code"],"content":" \u0026ldquo;初步了解和使用SPRING AOP\u0026rdquo;\n一、AOP 1.1 什么是AOP AOP(Aspect Orient Programming)，面向切面编程。AOP是一种编程思想，是对面向对象编程（OOP）的一种补充。\n1.2 AOP实现分类 AOP的本质是由AOP框架修改业务组件的字节码，是代理模式的一种应用。按照修改的字节码的时机可以分为两类:\n静态AOP: AOP框架在编译阶段进行修改，生成了静态的AOP代理类(生成的.class文件已经被改动)，比如AspecJ框架。 动态AOP: AOP框架在运行阶段动态生成代理对象(在内存中动态生成程序需要的.class文件)，比如SpringAop。 常用AOP实现比较\n二、AOP术语 Aspect（切面）：通常是一个类，里面定义切入点和通知。 JointPoint（连接点）：程序执行过程中可以插入的点，可以是方法的调用、异常的抛出，在Spring AOP中通常是方法的调用。 Advice（通知）：AOP框架中的增强处理，有before,after,afterReturning,afterThrowing,around PoinCut（切入点）：带有通知的连接点， 引入（Introduction）：引入允许我们向现有的类添加新的方法或者属性。 织入（Weaving）: 将增强处理添加到目标对象中，并创建一个被增强的对象，这个过程就是织入。 三、初步认识Spring AOP Spring AOP 与ApectJ 的目的一致，都是为了统一处理横切业务，但与AspectJ不同的是，Spring AOP 并不尝试提供完整的AOP功能(即使它完全可以实现)，Spring AOP 更注重的是与Spring IOC容器的结合，并结合该优势来解决横切业务的问题，因此在AOP的功能完善方面，相对来说AspectJ具有更大的优势，同时,Spring注意到AspectJ在AOP的实现方式上依赖于特殊编译器(ajc编译器)，因此Spring很机智回避了这点，转向采用动态代理技术的实现原理来构建Spring AOP的内部机制（动态织入），这是与AspectJ（静态织入）最根本的区别。在AspectJ 1.5后，引入@Aspect形式的注解风格的开发，Spring也非常快地跟进了这种方式，因此Spring 2.0后便使用了与AspectJ一样的注解。请注意，Spring 只是使用了与 AspectJ 5 一样的注解，但仍然没有使用 AspectJ 的编译器，底层依是动态代理技术的实现，因此并不依赖于 AspectJ 的编译器。\n","date":1610150400,"description":"","dir":"post\\Spring\\Spring AOP\\","excerpt_html":null,"excerpt_text":null,"expirydate":-62135596800,"fuzzywordcount":1000,"html":null,"keywords":null,"kind":"page","lang":"en","lastmod":1610150400,"objectID":"2a9071c1d5a1adcf52ab484b880f5b6d","permalink":"http://localhost:1313/post/spring/spring-aop/spring-aop%E4%BB%8B%E7%BB%8D/","publishdate":"2021-01-09T00:00:00Z","readingtime":2,"relpermalink":"/post/spring/spring-aop/spring-aop%E4%BB%8B%E7%BB%8D/","section":"post","summary":"\u0026ldquo;初步了解和使用SPRING AOP\u0026rdquo; 一、AOP 1.1 什么是AOP AOP(Aspect Orient Programming)，面向切面编程。AOP是一种编程思想，是对面向","tags":["Spring"],"title":"Spring AOP的初步了解","type":"post","url":"/post/spring/spring-aop/spring-aop%E4%BB%8B%E7%BB%8D/","weight":0,"wordcount":996},{"author":null,"categories":["Life"],"content":" “Yeah It\u0026rsquo;s on. ”\n前言 这是使用Hexo搭建的博客，使用了Hux的主题。 用腾讯云做的服务器，nginx静态资源访问，图片加载很慢，之后再优化下吧。 刚开始是打算用来记录Java技术之类的学习，现在想了下这个博客还是用来记录自己平时的想法和生活吧。 具体的技术什么的就发表在平台之类的网站上吧。\n计划 2020年毕业的，今年也比较特殊，新冠嘛。 年初谈了女朋友，当时已经签了三方，可是也没有去想毁三方，现在还是有点后悔的，早知道就留在武汉了。 打算呆到2022年3月份回去吧，所以可能2022年1月份会辞职，回去过年，找工作。 所以2021年要准备很多事情，学习很多东西。\n希望 希望以后的自己会越来越好吧，生活和感情上都是。\n","date":1606608000,"description":"","dir":"post\\Shuyou\\","excerpt_html":null,"excerpt_text":null,"expirydate":-62135596800,"fuzzywordcount":400,"html":null,"keywords":null,"kind":"page","lang":"en","lastmod":1606608000,"objectID":"7373279681095aa1f35ef6faaa228d37","permalink":"http://localhost:1313/post/shuyou/hello-world/","publishdate":"2020-11-29T00:00:00Z","readingtime":1,"relpermalink":"/post/shuyou/hello-world/","section":"post","summary":"“Yeah It\u0026rsquo;s on. ” 前言 这是使用Hexo搭建的博客，使用了Hux的主题。 用腾讯云做的服务器，nginx静态资源访问，图片加载很慢，之后再优化下吧","tags":["记录"],"title":"Welcome to ShuYou Blog","type":"post","url":"/post/shuyou/hello-world/","weight":0,"wordcount":301},{"author":null,"categories":null,"content":"关于我 98年，湖北武汉人。\n有时积极，偶尔消极，总是平常。\n大体是乐观的一个人\n不太擅长社交，只是和熟悉起来的朋友会很来劲。\n编程方面 主要是Java后台开发，磨练技术中\u0026hellip;\u0026hellip;\n会写一点点前端，对Golang语言有兴趣，最近在入门GO语言。\n喜欢编程，会有成就感，但是新技术太多，很多学不过来，也会有挫败感。\n慢慢积累，想当初自己也是啥都不会，后面意识到凡事都会有一个过程的。\n兴趣爱好 游戏、电影、小说\n我比较喜欢玩竞技类的游戏、网络游戏，不太喜欢一个人玩单机游戏。\n个人喜欢的电影很多，也看过很多电影，很喜欢剧情片。\n小说也看过比较多吧，当然大部分都是网络小说，现在看的很少了，可能是没有那种第一次看时异世界的感觉了，主要是现在的小说都大同小异的，没有新鲜感了。\n一直想发展一些其他兴趣爱好，因为自己现在的兴趣爱好主要是消费娱乐为主，想发展一些自己有产出的那些兴趣爱好。我觉得自己有产出的兴趣爱好才是能给自己带来更多快乐的。所以现在电影和小说都很少看了，只有游戏偶尔和朋友一起玩一下。\n网站 主要是自己记录下编程学习的网站，偶尔发点胡思乱想的东西。\n尽管自己的水平还有待提升，但记录本身就是一种提升的过程。\n感觉评论交流没有很大的必要性，所以没有弄博客评论功能，只是挂了下邮箱，虽然我邮箱不太常看，但是看到了相关交流邮件会尽量回复的。\n网站所有内容遵循GPL协议。\nEND 最后希望积累更多的编程知识。\n过去无法挽回，未来可以改变。\n","date":-62135596800,"description":"","dir":"about\\","excerpt_html":null,"excerpt_text":null,"expirydate":-62135596800,"fuzzywordcount":700,"html":null,"keywords":null,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8576ec274c98b3831668a172fa632d80","permalink":"http://localhost:1313/about/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/about/","section":"","summary":"关于我 98年，湖北武汉人。 有时积极，偶尔消极，总是平常。 大体是乐观的一个人 不太擅长社交，只是和熟悉起来的朋友会很来劲。 编程方面 主要是Java","tags":null,"title":"","type":"page","url":"/about/","weight":0,"wordcount":607},{"author":null,"categories":null,"content":"","date":-62135596800,"description":"","dir":"search\\","excerpt_html":null,"excerpt_text":null,"expirydate":-62135596800,"fuzzywordcount":100,"html":null,"keywords":null,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8946788897930c0c0c39fbfcd30ff2e4","permalink":"http://localhost:1313/search/placeholder/","publishdate":"0001-01-01T00:00:00Z","readingtime":0,"relpermalink":"/search/placeholder/","section":"search","summary":"","tags":null,"title":"","type":"search","url":"/search/placeholder/","weight":0,"wordcount":0},{"author":null,"categories":null,"content":"","date":-62135596800,"description":"Archive of historical posts.","dir":"archive\\","excerpt_html":"Archive of historical posts.","excerpt_text":"Archive of historical posts.","expirydate":-62135596800,"fuzzywordcount":100,"html":"Archive of historical posts.","keywords":null,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"a06e5ce9eca4c3260843078104889780","permalink":"http://localhost:1313/archive/","publishdate":"0001-01-01T00:00:00Z","readingtime":0,"relpermalink":"/archive/","section":"","summary":"","tags":null,"title":"Posts Archive","type":"archive","url":"/archive/","weight":0,"wordcount":0}]